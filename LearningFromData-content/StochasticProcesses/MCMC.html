
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>16.2. Markov chain Monte Carlo sampling &#8212; Combined Learning from Data materials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/coloredpages.css?v=0a037ad7" />
    <link rel="stylesheet" type="text/css" href="../../_static/myadmonitions.css?v=89ac28d1" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"loader": {"load": ["[tex]/textmacros"]}, "chtml": {"mtextInheritFont": true}, "tex": {"packages": {"[+]": ["textmacros"]}, "macros": {"data": "\\mathcal{D}", "pars": "\\boldsymbol{\\theta}", "para": "\\theta", "optpars": "\\pars^*", "optpara": "\\para^*", "prob": "\\mathbb{P}", "cprob": ["\\prob\\left( #1 \\, \\left\\vert \\, #2 \\right. \\right)", 2], "cprobsub": ["\\prob_{#1}\\left( #2 \\, \\left\\vert \\, #3 \\right. \\right)", 3], "pdf": ["p \\left( #1 \\, \\left\\vert \\, #2 \\right. \\right)", 2], "pdfsub": ["p_{#1} \\left( #2 \\, \\left\\vert \\, #3 \\right. \\right)", 3], "p": ["p \\left( #1 \\right)", 1], "psub": ["p_{#1} \\left( #2 \\right)", 2], "futuredata": "\\mathcal{F}", "expect": ["\\mathbb{E} \\left[ #1 \\right]", 1], "var": ["\\text{Var} \\left( #1 \\right)", 1], "std": ["\\text{Std} \\left( #1 \\right)", 1], "cov": ["\\text{Cov} \\left( #1, #2 \\right)", 2], "dmat": "\\boldsymbol{X}", "models": ["\\boldsymbol{M}\\left( #1 \\, ; \\, #2 \\right)", 2], "model": ["M\\left( #1 \\, ; \\, #2 \\right)", 2], "modeloutputs": "\\boldsymbol{M}", "modeloutput": "M", "MLmodel": ["\\boldsymbol{\\hat{y}}\\left( #1 \\right)", 1], "MLoutputs": "\\boldsymbol{\\hat{y}}", "MLoutput": "\\hat{y}", "outputs": "\\boldsymbol{y}", "inputs": "\\boldsymbol{x}", "targets": "\\boldsymbol{t}", "weights": "\\boldsymbol{w}", "testoutputs": "\\boldsymbol{y}^\\odot", "testinputs": "\\boldsymbol{x}^\\odot", "output": "y", "inputt": "x", "target": "t", "weight": "w", "testoutput": "y^\\odot", "MLtestoutput": "\\hat{y}^\\odot", "testinput": "x^\\odot", "trainingdata": "\\mathcal{T}", "LaTeX": "\\text{LaTeX}", "residual": "\\epsilon", "residuals": "\\boldsymbol{\\epsilon}", "zeros": "\\boldsymbol{0}", "covres": "\\boldsymbol{\\Sigma_{\\epsilon}}", "covpars": "\\boldsymbol{\\Sigma_{\\pars}}", "tildecovpars": "\\boldsymbol{\\widetilde{\\Sigma}_{\\pars}}", "sigmas": "\\boldsymbol{\\sigma}", "sigmai": "\\sigma_i", "sigmares": "\\sigma_{\\epsilon}", "cbar": "\\bar c", "Lra": "\\Longrightarrow", "yth": "y_{\\text{th}}", "yexp": "y_{\\text{exp}}", "ym": "y_{\\text{m}}", "thetavec": "\\boldsymbol{\\theta}", "parsLR": "\\boldsymbol{\\beta}", "paraLR": "\\beta", "covparsLR": "\\boldsymbol{\\Sigma_{\\parsLR}}", "optparsLR": "\\parsLR^*", "optparaLR": "\\paraLR^*", "tildecovparsLR": "\\boldsymbol{\\widetilde{\\Sigma}_{\\parsLR}}", "alphavec": "\\boldsymbol{\\alpha}", "muvec": "\\boldsymbol{\\mu}", "phivec": "\\boldsymbol{\\phi}", "betavec": "\\boldsymbol{\\beta}", "sigmavec": "\\boldsymbol{\\sigma}", "Sigmavec": "\\boldsymbol{\\Sigma}", "thetavechat": "\\widehat\\thetavec", "avec": "\\boldsymbol{a}", "Bvec": "\\boldsymbol{B}", "fvec": "\\boldsymbol{f}", "mvec": "\\boldsymbol{m}", "qvec": "\\boldsymbol{q}", "rvec": "\\boldsymbol{r}", "uvec": "\\boldsymbol{u}", "wvec": "\\boldsymbol{w}", "xvec": "\\boldsymbol{x}", "yvec": "\\boldsymbol{y}", "wt": "\\widetilde", "nb": "n_b", "mel": ["\\langle #1 | #2 | #3 \\rangle", 3], "qoi": "\\mathbf{Q}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'LearningFromData-content/StochasticProcesses/MCMC';</script>
    <script src="../../_static/custom.js?v=33f35b7a"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="16.3. MCMC Intro from BUQEYE" href="MCMC_intro_BUQ.html" />
    <link rel="prev" title="16.1. Markov chains" href="MarkovChains.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../Intro/About.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo-copilot.png" class="logo__image only-light" alt="Combined Learning from Data materials - Home"/>
    <script>document.write(`<img src="../../_static/logo-copilot.png" class="logo__image only-dark" alt="Combined Learning from Data materials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../Intro/About.html">
                    About this Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Intro/Invitation.html">1. Invitation to inductive inference</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Intro/Introduction.html">2. Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Intro/Introduction/sec-01-physicist-s-perspective.html">2.1. Physicistâ€™s perspective</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Intro/Introduction/sec-02-bayesian-workflow.html">2.2. Bayesian workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Intro/Introduction/sec-03-machine-learning.html">2.3. Machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Intro/Introduction/sec-04-virtues.html">2.4. Virtues</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I: Bayesian methods for scientific modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/RootBayesianBasics.html">3. Overview of Part I</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/Inferenceandpdfs.html">4. Inference and PDFs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/Inferenceandpdfs/sec-01-statements.html">4.1. Statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/Inferenceandpdfs/sec-02-manipulating-probabilities-bayesian-rules-of-probability-as.html">4.2. Manipulating probabilities: Bayesian rules of probability as principles of logic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/Inferenceandpdfs/sec-03-probability-density-functions.html">4.3. Probability density functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/Inferenceandpdfs/sec-04-summary.html">4.4. Looking ahead</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/exercise_sum_product_rule.html">4.5. Exercise: Checking the sum and product rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/exercise_medical_example_by_Bayes.html">4.6. Exercise: Standard medical example using Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/MoreBayesTheorem.html">4.7. More on Bayesâ€™ theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/Bayesian_epistemology.html">4.8. *Aside: Bayesian epistemology</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/DataModelsPredictions.html">4.9. Data, models, and predictions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../BayesianStatistics/BayesianParameterEstimation/parameter_estimation_Gaussian_noise.html">ðŸ“¥ Parameter estimation I: Gaussian mean and variance</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/Posteriors.html">5. Bayesian posteriors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/Exploring_pdfs.html">5.1. ðŸ“¥ Exploring PDFs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/Exploring_pdfs_followups.html">Follow-ups to Exploring PDFs</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/Gaussians.html">5.2. Gaussians: A couple of frequentist connections</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/visualization_of_CLT.html">ðŸ“¥ Visualization of the Central Limit Theorem</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianParameterEstimation/Interpreting2Dposteriors.html">5.3. Interpreting 2D posteriors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/chi_squared_tests.html">5.4. ðŸ“¥ Demonstration: Sum of normal variables squared</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/CoinTossing.html">6. Updating via Bayes' rule</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/CoinTossing/sec-01-coin-tossing-frequentists-and-bayesaians.html">6.1. Coin tossing: Frequentists and Bayesaians</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/CoinTossing/sec-02-when-do-priors-matter-when-don-t-they-matter.html">6.2. When do priors matter? When donâ€™t they matter?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/CoinTossing/sec-03-computing-the-posterior-analytically.html">6.3. Computing the posterior analytically</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/CoinTossing/sec-04-degree-of-belief-credibility-intervals-vs-frequentist-1-sigm.html">6.4. Degree of belief/credibility intervals vs frequentist 1-sigma intervals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/CoinTossing/sec-05-take-aways-and-follow-up-questions-from-coin-flipping.html">6.5. Take-aways and follow-up questions from coin flipping:</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/demo-BayesianBasics.html">6.6. ðŸ“¥ Demonstration:  Bayesian Coin Tossing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/Bayesian_updating_coinflip_interactive.html">6.7. ðŸ“¥ Demonstration: Coin tossing (with widget)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/UsingBayes.html">7. Bayes in practice</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianBasics/BayesianAdvantages.html">7.1. Advantages of the Bayesian approach</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianWorkflow/BayesianWorkflow.html">7.2. Bayesian research workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianLinearRegression/BayesianLinearRegression_rjf.html">7.3. Bayesian Linear Regression (BLR)</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II: Advanced Bayesian methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../BayesianStatistics/RootAdvancedMethods.html">8. Overview of Part II</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianStatistics/BayesianParameterEstimation/ParameterEstimation.html">9. More Bayesian parameter estimation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianParameterEstimation/radioactive_lighthouse_exercise.html">9.1. ðŸ“¥ Radioactive lighthouse problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianParameterEstimation/amplitude_in_presence_of_background.html">9.2. ðŸ“¥ Amplitude of a signal in the presence of background</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianParameterEstimation/parameter_estimation_fitting_straight_line_I.html">9.3. Parameter estimation example: fitting a straight line</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianParameterEstimation/parameter_estimation_fitting_straight_line_II.html">9.4. ðŸ“¥ Parameter estimation example: fitting a straight line II</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/BayesianParameterEstimation/dealing_with_outliers.html">9.5. ðŸ“¥ Dealing with outliers</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianStatistics/AssigningProbabilities/Assigning.html">10. Assigning probabilities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../BayesianStatistics/AssigningProbabilities/IgnorancePDF.html">10.1. Assigning probabilities (I): Indifferences and translation groups</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../BayesianStatistics/AssigningProbabilities/demo-straight_lines.html">Alternative notebook with MCMC sampling</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/AssigningProbabilities/MaxEnt2.html">10.2. Assigning probabilities (II): The principle of maximum entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/AssigningProbabilities/MaxEnt_Function_Reconstruction.html">10.3. ðŸ“¥ Maximum Entropy for reconstructing a function from its moments</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../BayesianStatistics/ComputationalBayes/BayesLinear.html">11. Bayes goes linear: History matching</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianStatistics/Multimodel_inference.html">12. Multi-model inference with Bayes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../BayesianStatistics/ModelSelection/ModelSelection.html">12.1. Model Selection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../BayesianStatistics/ModelSelection/BUQ/Evidence_for_model_EFT_coefficients.html">Evidence calculation for EFT expansions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../BayesianStatistics/ModelSelection/BUQ/two_model_evidence.html">Follow-up to EFT evidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../BayesianStatistics/ModelSelection/BUQ/computing_evidence.html">Computing the evidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../BayesianStatistics/ModelSelection/BUQ/MCMC-parallel-tempering_ptemcee_vs_zeus.html">Demo: Multimodal distributions with two samplers</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/ModelMixing/model_mixing.html">12.2. Model averaging and mixing </a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../OtherTopics/DiscrepancyModels.html">13. Discrepancy Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../OtherTopics/MD_balldrop_v1.html">13.4. ðŸ“¥ Ball-drop experiment</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III: MCMC sampling</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="RootMCMC.html">14. Overview of Part III</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="StochasticProcesses.html">15. Stochastic processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="BUQ/Metropolis_Poisson_example.html">15.7. Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="demo-MCMC.html">15.8. Demonstration: Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="Recap_BUQ.html">15.9. Recap of Poisson and more about MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="BUQ/parameter_estimation_Gaussian_noise-2.html">15.10. Parameter estimation example: Gaussian noise and averages II</a></li>
<li class="toctree-l2"><a class="reference internal" href="BUQ/MCMC-random-walk-and-sampling.html">15.11. Exercise: Random walk</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="MCMC_overview.html">16. Overview of Markov Chain Monte Carlo</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="MarkovChains.html">16.1. Markov chains</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">16.2. Markov chain Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="MCMC_intro_BUQ.html">16.3. Alternative MCMC introduction (Gregory)</a></li>
<li class="toctree-l2"><a class="reference internal" href="BUQ/Assignment_extending_radioactive_lighthouse.html">16.4. Assignment: 2D radioactive lighthouse location using MCMC</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Advanced_MCMC.html">17. Advanced MCMC</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/ComputationalBayes/AdvancedMCMC.html">17.1. Advanced Markov chain Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="BUQ/MCMC-diagnostics.html">17.2. Overview: MCMC Diagnostics</a></li>

<li class="toctree-l2"><a class="reference internal" href="BUQ/intuition_sampling.html">17.4. Intuition on sampling and best practices</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Other_samplers.html">18. HMC and other samplers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="BUQ2/HMC_intro_BUQ.html">18.1. Hamiltonian Monte Carlo (HMC) overview and visualization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="BUQ2/Liouville_theorem_visualization.html">Liouville Theorem Visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="BUQ2/Orbital_eqs_with_different_algorithms.html">Solving orbital equations with different algorithms</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="zeus.html">18.2. The Zeus Ensemble Slice Sampler</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="BUQ2/PyMC_intro_updated.html">18.3. PyMC Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="OverviewIntroPyMC.html">Overview of Intro to PyMC notebook</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="BUQ2/parameter_estimation_Gaussian_noise_compare_samplers.html">18.4. Comparing samplers for a simple problem</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV: Machine learning: A Bayesian perspective</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../MachineLearning/RootML.html">19. Overview of Part IV</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../MachineLearning/GP/RootGP.html">20. Overview of Gaussian processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../MachineLearning/GP/GaussianProcesses.html">20.4. Introduction to Gaussian processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../MachineLearning/GP/CF/demo-GaussianProcesses.html">ðŸ“¥ demo-GaussianProcesses notebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../MachineLearning/GP/BUQ/lecture_20.html">GP recap; GP applications; (old lecture 20)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../MachineLearning/GP/Sklearn_demos.html">20.5. scikit-learn demo notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../MachineLearning/GP/BUQ/plot_gpr_noisy_targets.html">ðŸ“¥ One-dimension regression example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../MachineLearning/GP/BUQ/plot_gpr_prior_posterior.html">ðŸ“¥ Prior and posterior with different kernels</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../MachineLearning/GP/GPy_demos.html">20.6. GPy demo notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../MachineLearning/GP/BUQ/demo-GaussianProcesses.html">Gaussian processes demonstration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../MachineLearning/GP/CF/exercise_GP_GPy.html">Exercise: Gaussian processes using <code class="docutils literal notranslate"><span class="pre">GPy</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../MachineLearning/GP/BUQ/Gaussian_processes_exercises.html">Exercise: Gaussian Process models with GPy</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../MachineLearning/LogReg/LogReg.html">21. Logistic Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/ANN/MachineLearningExamples.html">21.5. Machine Learning: First Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/ANN/NeuralNet/exercises_LogReg_NeuralNet.html">21.6. Exercise: Logistic Regression and neural networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../MachineLearning/ANN/MachineLearning.html">22. Machine learning: Overview and notation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/ANN/NeuralNet.html">22.5. Artifical neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/ANN/NeuralNet/demo-NeuralNet.html">22.6. Demonstration: Neural network classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/ANN/Neural_Network_for_simple_function_in_PyTorch.html">22.7. ðŸ“¥ ANN from ChatGPT using PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/ANN/ModelValidation.html">22.8. Model validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/ANN/DataBiasFairness.html">22.9. Data bias and fairness in machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/ANN/NeuralNet/NeuralNetBackProp.html">22.10. *Neural networks: Backpropagation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../OtherTopics/ANNFT.html">23. ANNs in the large-width limit (ANNFT)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../OtherTopics/random_initialized_ANN_vs_width.html">23.3. ðŸ“¥ Distributions of Randomly-Initialized ANNs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../MachineLearning/BNN/bnn.html">24. Bayesian neural nets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/BNN/demo-bnn.html">24.4. Demonstration: Variational Inference and Bayesian Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/BNN/exercises_BNN.html">24.5. Exercise: Bayesian neural networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../MachineLearning/CNN/cnn.html">25. *Convolutional neural nets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../MachineLearning/CNN/demo-cnn.html">25.6. Demonstration: Image recognition with Convolutional Neural Networks</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V: Other topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../OtherTopics/RootOtherTopics.html">26. Overview of Part V </a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../OtherTopics/Emulators.html">27. Emulators</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/ComputationalBayes/BayesFast.html">27.1. Bayes goes fast: Emulators (from CF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianStatistics/ComputationalBayes/extra_RBM_emulators.html">27.2. RBM emulators (BUQ)</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../OtherTopics/Student_t_distribution_from_Gaussians.html">28. ðŸ“¥ Student t distribution from Gaussians</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../OtherTopics/SVD.html">29. PCA, SVD, and all that</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../OtherTopics/linear_algebra_games_including_SVD.html">29.5. ðŸ“¥ demo-SVD notebook</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../OtherTopics/qbism.html">30. QBism: Bayesian quantum mechanics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Backmatter</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Backmatter/bibliography.html">31. Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Backmatter/JB_tests.html">32. Guide to Jupyter Book markdown</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix A: Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Reference/Statistics.html">33. Statistics concepts and notation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ModelingOptimization/GradientDescent.html">34. Gradient-descent optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix B: Scientific modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../ModelingOptimization/RootScientificModeling.html">35. Overview of scientific modeling material</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ModelingOptimization/OverviewModeling.html">36. Overview of modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ModelingOptimization/LinearModels.html">37. Linear models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ModelingOptimization/MathematicalOptimization.html">38. Mathematical optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix C: Getting started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Setup/RootGettingStarted.html">39. Overview of Getting started material</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Setup/exercise_Intro_01_Jupyter_Python.html">40. ðŸ“¥ Exercise: Jupyter notebooks and Python</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Setup/more_python_and_jupyter.html">41. More about Python and Jupyter notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Setup/exercise_Intro_02_Jupyter_Python.html">41.4. ðŸ“¥ Python lists and iterations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Setup/exercise_Intro_03_Numpy.html">41.5. ðŸ“¥ Linear algebra operations with NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Setup/demo-Intro.html">41.6. ðŸ“¥ Reading data and fitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Setup/Simple_widgets_v1.html">41.7. ðŸ“¥ Making a simple widget-based UI</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Setup/setting_up.html">42. Setting up for using this Jupyter Book</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Setup/installing_anaconda.html">42.1. Using Anaconda</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Setup/using_github.html">42.2. Using GitHub</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">TALENT mini-projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Mini-projects/RootMiniProjects.html">Overview of mini-projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Mini-projects/mini-project_I_toy_model_of_EFT.html">ðŸ“¥ MP I: Parameter estimation for a toy model of an EFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Mini-projects/model-selection_mini-project-IIa.html">ðŸ“¥ MP IIa: Model selection basics</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Mini-projects/model-selection_mini-project-IIb_How_many_lines_ptemcee.html">ðŸ“¥ MP IIb: How many lines?</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../Mini-projects/Mini-project_IIb_overview.html">Overview of Mini-project IIb: How many lines?</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../Mini-projects/mini-project_IIIa_bayesian_optimization.html">ðŸ“¥ MP IIIa: Bayesian optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Mini-projects/mini-project_IIIb_Bayesian_neural_networks_from_demo.html">ðŸ“¥ MP IIIb: Bayesian Neural Networks</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/NuclearTalent/LFD_for_Physicists/main?urlpath=tree/./LearningFromData-content/StochasticProcesses/MCMC.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/NuclearTalent/LFD_for_Physicists" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/NuclearTalent/LFD_for_Physicists/issues/new?title=Issue%20on%20page%20%2FLearningFromData-content/StochasticProcesses/MCMC.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/LearningFromData-content/StochasticProcesses/MCMC.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../../_sources/LearningFromData-content/StochasticProcesses/MCMC.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Markov chain Monte Carlo sampling</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-problems-in-bayesian-inference">General problems in Bayesian inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-integration">Numerical integration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quadrature-methods">Quadrature methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-integration">Monte Carlo integration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-integration-in-statistics">Monte Carlo integration in statistics</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-from-a-pdf">Sampling from a PDF</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-metropolis-hastings-algorithm">The Metropolis-Hastings algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#state-of-the-art-mcmc-implementations">State-of-the-art MCMC implementations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizations-of-mcmc">Visualizations of MCMC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-in-mcmc-sampling">Challenges in MCMC sampling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions">Solutions</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="markov-chain-monte-carlo-sampling">
<span id="sec-mcmc"></span><h1><span class="section-number">16.2. </span>Markov chain Monte Carlo sampling<a class="headerlink" href="#markov-chain-monte-carlo-sampling" title="Link to this heading">#</a></h1>
<blockquote class="epigraph">
<div><blockquote>
<div><p>â€œI decry the current tendency to seek patents on algorithms. There are better ways to earn a living than to prevent other people from making use of oneâ€™s contributions to computer science.â€</p>
</div></blockquote>
<p class="attribution">â€”Donald Knuth</p>
</div></blockquote>
<p>In this chapter we will return to Bayesian inference and show how custom design of Markov chains provides an enabling technology to address also high-dimensional problems. The posterior PDF <span class="math notranslate nohighlight">\(\pdf{\pars}{\data, I}\)</span> is a key quantity in the Bayesian approach. The approach that will be outlined here is general to any PDF <span class="math notranslate nohighlight">\(p(\pars)\)</span> so we will just use this simple notation.</p>
<section id="general-problems-in-bayesian-inference">
<h2>General problems in Bayesian inference<a class="headerlink" href="#general-problems-in-bayesian-inference" title="Link to this heading">#</a></h2>
<p>We might be interested in:</p>
<ol class="arabic">
<li><p>The exploration of the PDF;</p>
<ul class="simple">
<li><p>Where is the probability mass concentrated?</p></li>
<li><p>Are there correlations between parameters?</p></li>
<li><p>Are there multiple modes?</p></li>
<li><p>How do the marginal PDFs look like?</p></li>
</ul>
</li>
<li><p>The exploration of posterior predictions with a function <span class="math notranslate nohighlight">\(g(\pars)\)</span>;</p>
<ul>
<li><p>Finding the <strong>posterior predictive distribution</strong></p>
<div class="amsmath math notranslate nohighlight" id="equation-300b67ca-9f99-41ee-9f84-efa426783074">
<span class="eqno">(16.19)<a class="headerlink" href="#equation-300b67ca-9f99-41ee-9f84-efa426783074" title="Permalink to this equation">#</a></span>\[\begin{equation}
\{ g(\pars) \; : \; \pars \sim p(\pars) \}
\end{equation}\]</div>
</li>
<li><p>The evaluation of <strong>expectation values</strong></p>
<div class="math notranslate nohighlight" id="equation-eq-mcmc-expectation-integral">
<span class="eqno">(16.20)<a class="headerlink" href="#equation-eq-mcmc-expectation-integral" title="Link to this equation">#</a></span>\[
        \expect{g(\pars)} = \int d\pars g(\pars) p(\pars)
      \]</div>
</li>
</ul>
</li>
</ol>
<p>The first set of tasks can be addressed with a method to collect a set of samples <span class="math notranslate nohighlight">\(\{ \pars_i\}_{i=1}^N\)</span>, where the samples are distributed according to <span class="math notranslate nohighlight">\(p(\pars)\)</span>. The process of generating a random sample according to a probability distribution is commonly called a â€œdrawâ€â€. Such samples can then be histogramed and plotted to provide an approximate representation of <span class="math notranslate nohighlight">\(p(\pars)\)</span>.</p>
<p>In particular, it should be noted that marginal distributions can easily be obtained from these samples by ignoring the values of parameters that we are integrating over. For example, the marginal distribution <span class="math notranslate nohighlight">\(p(\para_1,\para_2)\)</span> of <span class="math notranslate nohighlight">\(p(\pars)\)</span> with <span class="math notranslate nohighlight">\(\pars\)</span> a <span class="math notranslate nohighlight">\(D\)</span>-dimensional vector is represented by the samples</p>
<div class="math notranslate nohighlight">
\[
\{ (\para_1,\para_2)_i \}_{i=1}^N,
\]</div>
<p>where <span class="math notranslate nohighlight">\((\para_1,\para_2)_i\)</span> are the first two elements of the <span class="math notranslate nohighlight">\(\pars_i\)</span> sample. This procedure of ignoring the other parameter values can be understood since the process of marginalization is basically a projection of the multi-dimensional PDF onto the chosen axes.</p>
<p>The computation of high-dimensional integrals such as in Eq. <a class="reference internal" href="#equation-eq-mcmc-expectation-integral">(16.20)</a> is very challenging in high dimensions. In fact, high-dimensional integrals appear frequently in science and applied mathematics. It is only anaytically solvable in some special cases and is therefore an important and general problem. It turns out that the ability to draw samples from <span class="math notranslate nohighlight">\(p(\pars)\)</span> will make it possible to compute very efficient, numerical approximations of this integral also in very high dimensions.</p>
</section>
<section id="numerical-integration">
<h2>Numerical integration<a class="headerlink" href="#numerical-integration" title="Link to this heading">#</a></h2>
<p>There are several methods to perform numerical integration. Quadrature is the method of choice for small numbers of dimensions, while Monte Carlo integration becomes very powerful in high dimensions.</p>
<section id="quadrature-methods">
<h3>Quadrature methods<a class="headerlink" href="#quadrature-methods" title="Link to this heading">#</a></h3>
<p>For small dimensions, classical numerical integration using quadrature is often the method of choice. For example, the one-dimensional integral</p>
<div class="amsmath math notranslate nohighlight" id="equation-40445196-dd30-432e-bfd6-138a582cc49c">
<span class="eqno">(16.21)<a class="headerlink" href="#equation-40445196-dd30-432e-bfd6-138a582cc49c" title="Permalink to this equation">#</a></span>\[\begin{equation}
I = \int d\para f(\para),
\end{equation}\]</div>
<p>is approximated by a weighted average of the function <span class="math notranslate nohighlight">\(f\)</span> evaluated at a number of quadrature points <span class="math notranslate nohighlight">\(\{ \para_i \}_{i=1}^N\)</span></p>
<div class="amsmath math notranslate nohighlight" id="equation-4db285cc-fdf9-4bdc-818c-ca6b90ed98b5">
<span class="eqno">(16.22)<a class="headerlink" href="#equation-4db285cc-fdf9-4bdc-818c-ca6b90ed98b5" title="Permalink to this equation">#</a></span>\[\begin{equation}
I \approx \sum_{i=1}^N w_i f(\para_i),
\end{equation}\]</div>
<p>where the different quadrature schemes are distinguished by using different sets of design points and weights. We note, however, that the number of design points that is required for a fixed precision grows exponentially with dimension. Assuming an integration scheme with <span class="math notranslate nohighlight">\(N\)</span> design points in one dimension, the same coverage in <span class="math notranslate nohighlight">\(D\)</span> dimensions requires <span class="math notranslate nohighlight">\(N^D\)</span> design points, which will be impractically large unless <span class="math notranslate nohighlight">\(D\)</span> is sufficiently small. This exponential increase in the number of required function evaluations with the dimensionality of the problem is often called the curse of dimensionality and is the main motivation for Monte Carlo integration.</p>
</section>
<section id="monte-carlo-integration">
<h3>Monte Carlo integration<a class="headerlink" href="#monte-carlo-integration" title="Link to this heading">#</a></h3>
<p>The most straightforward implementation of Monte Carlo integration provides an estimate of the <span class="math notranslate nohighlight">\(D\)</span>-dimensional integral</p>
<div class="math notranslate nohighlight" id="equation-eq-mcmc-mc-integration">
<span class="eqno">(16.23)<a class="headerlink" href="#equation-eq-mcmc-mc-integration" title="Link to this equation">#</a></span>\[
I = \int_{V_D} d\pars f(\pars) \approx I_N \equiv \frac{V_D}{N} \sum_{i=1}^N f(\pars_i),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\{ \pars_i \}_{i=1}^N\)</span> are <span class="math notranslate nohighlight">\(N\)</span> uniformly distributed points in the <span class="math notranslate nohighlight">\(D\)</span> dimensional volume <span class="math notranslate nohighlight">\(V_D\)</span> of the parameter space. The convergence of this expression will not be proven here, but is a consequence of the law of large numbers. Furthermore, one can show that the error in this approximation (usually) decreases as <span class="math notranslate nohighlight">\(1/\sqrt{N}\)</span> regardless of the dimensionality <span class="math notranslate nohighlight">\(D\)</span>, which is the main reason for its usefulness when facing complicated, high-dimensional integrals. To be specific, the variance of <span class="math notranslate nohighlight">\(I_N\)</span> can be estimated by the sample variance</p>
<div class="amsmath math notranslate nohighlight" id="equation-b19398fd-5ffe-48b7-bb7c-e223e054b4d4">
<span class="eqno">(16.24)<a class="headerlink" href="#equation-b19398fd-5ffe-48b7-bb7c-e223e054b4d4" title="Permalink to this equation">#</a></span>\[\begin{equation}
\sigma_N^2 = \frac{1}{N-1} \sum_{i=1}^N \left( f(\pars_i) - \langle f \rangle \right)^2,
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\langle f \rangle\)</span> is the sample mean, and becomes</p>
<div class="amsmath math notranslate nohighlight" id="equation-d4960d0f-c7f3-49f5-b0ec-5788fac1e054">
<span class="eqno">(16.25)<a class="headerlink" href="#equation-d4960d0f-c7f3-49f5-b0ec-5788fac1e054" title="Permalink to this equation">#</a></span>\[\begin{equation}
\text{Var}(I_N)) = V_D^2 \frac{\sigma_N^2}{N}.
\end{equation}\]</div>
<div class="proof example admonition" id="example-0">
<p class="admonition-title"><span class="caption-number">Example 16.4 </span> (Monte Carlo estimation of <span class="math notranslate nohighlight">\(\pi\)</span>)</p>
<section class="example-content" id="proof-content">
<p>A standard example of Monte Carlo integration is the estimation of <span class="math notranslate nohighlight">\(\pi\)</span> using the (circular) step function</p>
<div class="amsmath math notranslate nohighlight" id="equation-096bafeb-4854-42d2-a0e7-a5a40b7ac51a">
<span class="eqno">(16.26)<a class="headerlink" href="#equation-096bafeb-4854-42d2-a0e7-a5a40b7ac51a" title="Permalink to this equation">#</a></span>\[\begin{equation}
f(x,y) = \left\{ 
\begin{array}{ll}
1 &amp; \text{if $x^2+y^2 \leq 1$} \\
0 &amp; \text{otherwise},
\end{array}
\right.
\end{equation}\]</div>
<p>such that the integral</p>
<div class="amsmath math notranslate nohighlight" id="equation-5cbc18f2-a2ab-43a7-bae7-7742e2d6c902">
<span class="eqno">(16.27)<a class="headerlink" href="#equation-5cbc18f2-a2ab-43a7-bae7-7742e2d6c902" title="Permalink to this equation">#</a></span>\[\begin{equation}
I = \int_{-1}^1 dx \int_{-1}^1 dy f(x,y) = \pi.
\end{equation}\]</div>
<p>One can estimate <span class="math notranslate nohighlight">\(I\)</span> using <span class="math notranslate nohighlight">\(N\)</span> samples <span class="math notranslate nohighlight">\(\{ (x_i,y_i) \}_{i=1}^N \)</span> from the bivariate, uniform distribution <span class="math notranslate nohighlight">\(\mathcal{U} \left( [-1,1]^2 \right)\)</span> such that</p>
<div class="math notranslate nohighlight" id="equation-eq-mcmc-mc-integration-pi">
<span class="eqno">(16.28)<a class="headerlink" href="#equation-eq-mcmc-mc-integration-pi" title="Link to this equation">#</a></span>\[
I \approx I_N = \frac{4}{N} \sum_{i=1}^N f(x_i,y_i),
\]</div>
<p>according to Eq. <a class="reference internal" href="#equation-eq-mcmc-mc-integration">(16.23)</a>. In the code cell below we implement this Monte Carlo integration using python and show that the (relative) error of this numerical simulation of <span class="math notranslate nohighlight">\(\pi\)</span> indeed decreases as <span class="math notranslate nohighlight">\(1/\sqrt{N}\)</span>.</p>
</section>
</div><div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">stats</span>

<span class="n">num_samples</span><span class="o">=</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">50</span><span class="p">)]</span>
<span class="n">I_num</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">I_rel_error</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">num_samples</span><span class="p">:</span>
    <span class="n">max_I_num</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="c1"># We will do ten simulations and find the largest error that is made</span>
    <span class="k">for</span> <span class="n">iexp</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="c1"># `num` bivariate samples from a uniform distribution [-1,1]**2</span>
        <span class="n">samples</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">loc</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span><span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">num</span><span class="p">))</span>
        <span class="c1"># Boolean array with elements that are true if the sample is inside the unit circle</span>
        <span class="c1"># This corresponds to the unit circle step function</span>
        <span class="n">inside</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">samples</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span><span class="o">**</span><span class="mi">2</span> <span class="o">&lt;=</span> <span class="mi">1</span>
        <span class="c1"># We can evaluate the Monte Carlo sum</span>
        <span class="n">iexp_I_num</span> <span class="o">=</span> <span class="mf">4.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">inside</span><span class="p">)</span> <span class="o">/</span> <span class="n">num</span>
        <span class="c1"># We keep the largest error that we find in all experiments</span>
        <span class="n">max_I_num</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_I_num</span><span class="p">,</span><span class="n">iexp_I_num</span><span class="p">)</span>
        
    <span class="n">I_num</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">max_I_num</span><span class="p">)</span>
    <span class="c1"># And the (absolute value) of the relative error</span>
    <span class="n">I_rel_error</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">I_num</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>
    
<span class="n">num_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)</span>
<span class="n">I_rel_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">I_rel_error</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="p">{</span><span class="s2">&quot;figsize&quot;</span><span class="p">:(</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">)})</span>
<span class="n">ax</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">I_rel_error</span><span class="p">,</span> <span class="s1">&#39;bo&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$(I_N-\pi) / \pi$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">loglog</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">num_samples</span><span class="p">),</span><span class="s1">&#39;r--&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;$1/\sqrt</span><span class="si">{N}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$N$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;rel. error&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">);</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">myst_nb</span><span class="w"> </span><span class="kn">import</span> <span class="n">glue</span>
<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;MC-integration_fig&quot;</span><span class="p">,</span> <span class="n">fig</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<details class="admonition hide below-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell output</p>
<p class="expanded admonition-title">Hide code cell output</p>
</summary>
<div class="cell_output docutils container">
<img alt="../../_images/81fbb081f34c5413afac6982f824273e99d41b29047c3424eb86e6cdcc5d1e5f.png" src="../../_images/81fbb081f34c5413afac6982f824273e99d41b29047c3424eb86e6cdcc5d1e5f.png" />
</div>
</details>
</div>
<figure class="align-default" id="fig-mc-integration">
<img alt="../../_images/81fbb081f34c5413afac6982f824273e99d41b29047c3424eb86e6cdcc5d1e5f.png" src="../../_images/81fbb081f34c5413afac6982f824273e99d41b29047c3424eb86e6cdcc5d1e5f.png" />
<figcaption>
<p><span class="caption-number">Fig. 16.7 </span><span class="caption-text">Relative error of the Monte Carlo integration <a class="reference internal" href="#equation-eq-mcmc-mc-integration-pi">(16.28)</a> as a function of the number of samples, demonstrating the <span class="math notranslate nohighlight">\(1/\sqrt{N}\)</span> scaling.</span><a class="headerlink" href="#fig-mc-integration" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Despite the favorable <span class="math notranslate nohighlight">\(1/\sqrt{N}\)</span>-scaling, the problem of straight-forward Monte Carlo integration is that you will often evaluate the integrand at locations where it is small. This affects the precision of the approximation due to the volume factor and since the sample variance is large. It would be more beneficial to focus the sampling to regions that contribute significantly to the integral.</p>
</section>
<section id="monte-carlo-integration-in-statistics">
<h3>Monte Carlo integration in statistics<a class="headerlink" href="#monte-carlo-integration-in-statistics" title="Link to this heading">#</a></h3>
<p>Indeed, it turns out that significantly better precision can be achieved for integrals such as <a class="reference internal" href="#equation-eq-mcmc-expectation-integral">(16.20)</a>, that appear in statistical computations. Given that we have a method to collect samples <span class="math notranslate nohighlight">\(\{ \pars_i \}_{i=1}^N\)</span> from <span class="math notranslate nohighlight">\(p(\pars)\)</span>, we find that</p>
<div class="math notranslate nohighlight" id="equation-eq-mcmc-mc-pdf-integration">
<span class="eqno">(16.29)<a class="headerlink" href="#equation-eq-mcmc-mc-pdf-integration" title="Link to this equation">#</a></span>\[
I = \int_{V_D} d\pars g(\pars) p(\pars) \approx I_N \equiv \frac{1}{N} \sum_{i=1}^N g(\pars_i),
\]</div>
<p>where you should note that the volume factor has disappeared and that you only evaluate <span class="math notranslate nohighlight">\(g(\pars)\)</span> at the sampled positions where the PDF is usually large.</p>
</section>
</section>
<section id="sampling-from-a-pdf">
<h2>Sampling from a PDF<a class="headerlink" href="#sampling-from-a-pdf" title="Link to this heading">#</a></h2>
<p>We have seen that the ability to draw samples from a probability distribution is very useful. If <span class="math notranslate nohighlight">\(p(\pars)\)</span> is of a standard form then it is straightforward to sample from it using available algorithms, most of which are based on nonlinear transformations of uniformly distributed random numbers. For example, a set of samples <span class="math notranslate nohighlight">\(\{ u_i\}_{i=1}^N\)</span> from the one-dimensional standard uniform distribution <span class="math notranslate nohighlight">\(\mathcal{U}([0,1])\)</span>
can be tuned to a a set of samples <span class="math notranslate nohighlight">\(\{ \para_i \}_{i=1}^N\)</span> from the standard Gaussian distribution <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span> via the transformation</p>
<div class="amsmath math notranslate nohighlight" id="equation-8380515c-1aa8-4aca-adab-152a3ee0814b">
<span class="eqno">(16.30)<a class="headerlink" href="#equation-8380515c-1aa8-4aca-adab-152a3ee0814b" title="Permalink to this equation">#</a></span>\[\begin{equation}
\para_i = F^{-1}_{\mathcal{N}}(u_i),
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(F^{-1}_{\mathcal{N}}\)</span> is the <em>inverse</em> distribution function of the standard normal distribution. That is, we interpret <span class="math notranslate nohighlight">\(u_i\)</span> as a probability sample and use the above transformation to find the solution to <span class="math notranslate nohighlight">\(u_i = F(\para_i)\)</span>.</p>
<p>This works well for many standard distributions. However, for the generation of random samples from nonstandard, arbitrary distributions (such as the ones that we might encounter in a realistic Bayesian analysis), no algorithms are available. This is where custom-built Markov chains enter.</p>
</section>
<section id="the-metropolis-hastings-algorithm">
<h2>The Metropolis-Hastings algorithm<a class="headerlink" href="#the-metropolis-hastings-algorithm" title="Link to this heading">#</a></h2>
<p>How can we collect random samples from an arbitrary probability distribution <span class="math notranslate nohighlight">\(p(\pars)\)</span>? In particular, how can we perform this task when we might not even have a closed form expression for the PDF? It is common in Bayesian inference that we find ourselves in the situation that we can evaluate <span class="math notranslate nohighlight">\(p(\pars)\)</span> at any position <span class="math notranslate nohighlight">\(\pars\)</span>, but we donâ€™t know beforehand if it will be large or small.</p>
<p>We will now show that it is possible to construct a Markov chain that has <span class="math notranslate nohighlight">\(p(\pars)\)</span> as its stationary distribution. Once this chain has converged it will provide us with samples from <span class="math notranslate nohighlight">\(p(\pars)\)</span>.</p>
<p>As we have learned, the evolution of positions is described by the transition density <span class="math notranslate nohighlight">\(T\)</span>. The generalization to continuous variables of the stationary-chain equilibrium described in Eq. <a class="reference internal" href="MarkovChains.html#equation-eq-markovchains-discrete-equlibrium">(16.11)</a> is</p>
<div class="amsmath math notranslate nohighlight" id="equation-49bd43eb-ae59-4782-872f-b871f1d46145">
<span class="eqno">(16.31)<a class="headerlink" href="#equation-49bd43eb-ae59-4782-872f-b871f1d46145" title="Permalink to this equation">#</a></span>\[\begin{equation}
p(\pars) = \int_{\Omega'} p(\pars')T(\pars',\pars) d\pars'.
\end{equation}\]</div>
<p>This equilibrium condition can be replaced with the more stringent <strong>detailed-balance</strong> condition</p>
<div class="math notranslate nohighlight" id="equation-eq-mcmc-detailed-balance">
<span class="eqno">(16.32)<a class="headerlink" href="#equation-eq-mcmc-detailed-balance" title="Link to this equation">#</a></span>\[
p(\pars)T(\pars,\pars') = p(\pars') T(\pars',\pars),
\]</div>
<p>which can be understood from the integral relation</p>
<div class="amsmath math notranslate nohighlight" id="equation-e3f85843-1674-499e-a3a2-7e2f9703825b">
<span class="eqno">(16.33)<a class="headerlink" href="#equation-e3f85843-1674-499e-a3a2-7e2f9703825b" title="Permalink to this equation">#</a></span>\[\begin{equation}
\int d\pars' p(\pars)T(\pars,\pars') = \int d\pars' p(\pars') T(\pars',\pars),
\end{equation}\]</div>
<p>that describes the balance between the transitions of probability density out of <span class="math notranslate nohighlight">\(\pars\)</span> and into <span class="math notranslate nohighlight">\(\pars\)</span>. At equilibrium, these integrals must be equal.</p>
<p>The trick to achieve this for a certain <span class="math notranslate nohighlight">\(p(\pars)\)</span> is to write <span class="math notranslate nohighlight">\(T\)</span> as a product of a simple step proposal function <span class="math notranslate nohighlight">\(S(\pars,\pars')\)</span>, that provides probability densities for steps in <span class="math notranslate nohighlight">\(\pars\)</span>-space, and a function <span class="math notranslate nohighlight">\(A(\pars,\pars')\)</span> that we will call the <strong>acceptance probability</strong></p>
<div class="amsmath math notranslate nohighlight" id="equation-0b8d0ec3-0b75-4cbe-9edc-fb108fff39c5">
<span class="eqno">(16.34)<a class="headerlink" href="#equation-0b8d0ec3-0b75-4cbe-9edc-fb108fff39c5" title="Permalink to this equation">#</a></span>\[\begin{equation}
T(\pars,\pars') = A(\pars,\pars') S(\pars,\pars').
\end{equation}\]</div>
<p>The step proposal function can be almost anything, but a key property is that it should be easy to draw a sample from it. The probability for proposing a new position <span class="math notranslate nohighlight">\(\pars'\)</span> will depend on the current position <span class="math notranslate nohighlight">\(\pars\)</span></p>
<div class="amsmath math notranslate nohighlight" id="equation-b116b192-99c8-48c1-9045-bad468aeed64">
<span class="eqno">(16.35)<a class="headerlink" href="#equation-b116b192-99c8-48c1-9045-bad468aeed64" title="Permalink to this equation">#</a></span>\[\begin{equation}
\pdf{\text{proposing $\pars'$}}{\text{current position}=\pars} = S(\pars,\pars')
\end{equation}\]</div>
<p>For example, assuming that <span class="math notranslate nohighlight">\(\pars\)</span> is a vector in a <span class="math notranslate nohighlight">\(D\)</span>-dimensional parameter space we can take <span class="math notranslate nohighlight">\(S(\pars,\pars')\)</span> to be a uniform distribution within a <span class="math notranslate nohighlight">\(D\)</span>-dimensional hypercube with side length <span class="math notranslate nohighlight">\(\Delta\)</span></p>
<div class="amsmath math notranslate nohighlight" id="equation-721d0fd2-a571-4122-be54-a76c6fc053dc">
<span class="eqno">(16.36)<a class="headerlink" href="#equation-721d0fd2-a571-4122-be54-a76c6fc053dc" title="Permalink to this equation">#</a></span>\[\begin{equation}
S(\pars,\pars') = \left\{ 
\begin{array}{ll}
\frac{1}{\Delta^D} &amp; \text{if $\left| \pars'_i - \pars_i \right| \leq \frac{\Delta}{2}$ for all $i \in \{1, \ldots, D\}$} \\
0 &amp; \text{otherwise}
\end{array}
\right.
\end{equation}\]</div>
<p>Alternatively we can use a multivariate Gaussian distribution with a mean that corresponds to the current position <span class="math notranslate nohighlight">\(\pars\)</span> and a covariance matrix that we can specify.</p>
<p>The probability of actually accepting the proposed step from <span class="math notranslate nohighlight">\(\pars\)</span> to <span class="math notranslate nohighlight">\(\pars'\)</span> will then be given by <span class="math notranslate nohighlight">\(0 \leq A(\pars,\pars') \leq 1\)</span>.</p>
<p>The detailed balance condition <a class="reference internal" href="#equation-eq-mcmc-detailed-balance">(16.32)</a> can be fulfilled by placing the following condition on the acceptance probability</p>
<div class="amsmath math notranslate nohighlight" id="equation-fea5bb6f-1da8-48b6-a468-01cf8260bda8">
<span class="eqno">(16.37)<a class="headerlink" href="#equation-fea5bb6f-1da8-48b6-a468-01cf8260bda8" title="Permalink to this equation">#</a></span>\[\begin{equation}
\frac{A(\pars,\pars')}{A(\pars',\pars)} = \frac{p(\pars')S(\pars',\pars)}{p(\pars)S(\pars,\pars')}.
\end{equation}\]</div>
<p>The expression for <span class="math notranslate nohighlight">\(A\)</span> that fulfils this condition is</p>
<div class="amsmath math notranslate nohighlight" id="equation-54f4e1a7-9af8-43a0-a8de-10ce93a820e7">
<span class="eqno">(16.38)<a class="headerlink" href="#equation-54f4e1a7-9af8-43a0-a8de-10ce93a820e7" title="Permalink to this equation">#</a></span>\[\begin{equation}
A(\pars,\pars') = \text{min} \left( \frac{p(\pars')S(\pars',\pars)}{p(\pars)S(\pars,\pars')},1\right),
\end{equation}\]</div>
<p>and holds the probability by which we decide whether to accept the proposed position <span class="math notranslate nohighlight">\(\pars'\)</span>, or remain in the old one <span class="math notranslate nohighlight">\(\pars\)</span>. The ratio that appears in the acceptance probability is called the Metropolis-Hastings ratio and will here be denoted <span class="math notranslate nohighlight">\(r\)</span>. We note that it gets further simplified for symmetric proposal functions (for which it is sometimes referred to as the Metropolis ratio)</p>
<div class="math notranslate nohighlight" id="equation-eq-mcmc-metropolis-ratio">
<span class="eqno">(16.39)<a class="headerlink" href="#equation-eq-mcmc-metropolis-ratio" title="Link to this equation">#</a></span>\[\begin{split}
r = \left\{
\begin{array}{ll}
\frac{p(\pars')S(\pars',\pars)}{p(\pars)S(\pars,\pars')} 
&amp; \text{for general proposal functions}, \\
\frac{p(\pars')}{p(\pars)}
&amp; \text{for symmetric proposal functions}.
\end{array}
\right.
\end{split}\]</div>
<p>Note how the proposal function <span class="math notranslate nohighlight">\(S(\pars,\pars')\)</span> in combination with the acceptance probability <span class="math notranslate nohighlight">\(A(\pars,\pars') = \text{min} (r,1)\)</span> provides a simple process for generating samples in a Markov chain that has the required equilibrium distribution. It only requires simple draws of proposal steps, and the evaluation of <span class="math notranslate nohighlight">\(p\)</span> at these positions. It is stochastic due to the random proposal of steps and since the decision to accept a proposed step contains randomness via the acceptance probability.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The proposal function must allow the chain to be irreducible, positively recurrent and aperiodic.</p>
</div>
<p>The basic structure of the Metropolis (and Metropolis-Hastings) algorithm is the following:</p>
<div class="proof algorithm admonition" id="algorithm-1">
<p class="admonition-title"><span class="caption-number">Algorithm 16.1 </span> (The Metropolis-Hastings algorithm)</p>
<section class="algorithm-content" id="proof-content">
<ol class="arabic">
<li><p>Initialize the sampling by choosing a starting point <span class="math notranslate nohighlight">\(\boldsymbol{\pars}_0\)</span>.</p></li>
<li><p>Collect samples by repeating the following:</p>
<ol class="arabic">
<li><p>Given <span class="math notranslate nohighlight">\(\boldsymbol{\pars}_i\)</span>, <em>propose</em> a new point <span class="math notranslate nohighlight">\(\boldsymbol{\phi}\)</span>, sampled from the proposal distribution <span class="math notranslate nohighlight">\(S( \boldsymbol{\pars}_i, \boldsymbol{\phi} )\)</span>. This proposal distribution could take many forms. However, for concreteness you can imagine it as a multivariate normal with mean vector given by <span class="math notranslate nohighlight">\(\boldsymbol{\pars}_i\)</span> and (position-independent) variances <span class="math notranslate nohighlight">\(\boldsymbol{\sigma}^2\)</span> specified by the user.</p>
<ul class="simple">
<li><p>The propsal function will (usually) give a smaller probability for visiting positions that are far from the current position.</p></li>
<li><p>The width <span class="math notranslate nohighlight">\(\boldsymbol{\sigma}\)</span> determines the average step size and is known as the proposal width.</p></li>
</ul>
</li>
<li><p>Compute the Metropolis(-Hastings) ratio <span class="math notranslate nohighlight">\(r\)</span> given by Eq. <a class="reference internal" href="#equation-eq-mcmc-metropolis-ratio">(16.39)</a>. For symmetric proposal distributions, with the simpler expression for the acceptance probability, this algorithm is known as the Metropolis algorithm.</p></li>
<li><p>Decide whether or not to accept candidate <span class="math notranslate nohighlight">\(\boldsymbol{\phi}\)</span> for <span class="math notranslate nohighlight">\(\boldsymbol{\pars}_{i+1}\)</span>.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(r \geq 1\)</span>: accept the proposal position and set <span class="math notranslate nohighlight">\(\boldsymbol{\pars}_{i+1} = \boldsymbol{\phi}\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(r &lt; 1\)</span>: accept the position with probability <span class="math notranslate nohighlight">\(r\)</span> by sampling a uniform <span class="math notranslate nohighlight">\(\mathcal{U}([0,1])\)</span> distribution (note that now we have <span class="math notranslate nohighlight">\(0 \leq r &lt; 1\)</span>).</p>
<ul>
<li><p>If <span class="math notranslate nohighlight">\(u \sim \mathcal{U}([0,1]) \leq r\)</span>, then <span class="math notranslate nohighlight">\(\boldsymbol{\pars}_{i+1} = \boldsymbol{\phi}\)</span> (accept);</p></li>
<li><p>else <span class="math notranslate nohighlight">\(\boldsymbol{\pars}_{i+1} = \boldsymbol{\pars}_i\)</span> (reject).</p></li>
</ul>
</li>
</ul>
<p>Note that the chain always grows since you add the current position again if the proposed step is rejected.</p>
</li>
<li><p>Iterate until the chain has reached a predetermined length or passes some convergence tests.</p></li>
</ol>
</li>
</ol>
</section>
</div><ul class="simple">
<li><p>The Metropolis algorithm dates back to the 1950s in physics, but didnâ€™t become widespread in statistics until almost 1980.</p></li>
<li><p>It enabled Bayesian methods to become feasible.</p></li>
<li><p>Note, however, that nowadays there are much more sophisticated samplers than the original Metropolis one.</p></li>
</ul>
</section>
<section id="state-of-the-art-mcmc-implementations">
<h2>State-of-the-art MCMC implementations<a class="headerlink" href="#state-of-the-art-mcmc-implementations" title="Link to this heading">#</a></h2>
<p>Here is an (incomplete) list of state-of-the-art MCMC implementations and packages that are available in Python (and often other languages)</p>
<div class="admonition-emcee admonition">
<p class="admonition-title">emcee:</p>
<p><a class="reference external" href="https://emcee.readthedocs.io/en/latest/">emcee</a> <span id="id1">[<a class="reference internal" href="../Backmatter/bibliography.html#id23" title="Daniel Foreman-Mackey, David W. Hogg, Dustin Lang, and Jonathan Goodman. Emcee: the mcmc hammer. Publications of the Astronomical Society of the Pacific, 125(925):306â€“312, Mar 2013. doi:10.1086/670067.">FMHLG13</a>]</span> is an MIT licensed pure-Python implementation of Goodman &amp; Weareâ€™s <a class="reference external" href="http://msp.berkeley.edu/camcos/2010/5-1/p04.xhtml">Affine Invariant Markov chain Monte Carlo (MCMC) Ensemble sampler</a> <span id="id2">[<a class="reference internal" href="../Backmatter/bibliography.html#id24" title="Jonathan Goodman and Jonathan Weare. Ensemble samplers with affine invariance. Comm. App. Math. and Comp. Sci., 5(1):65â€“80, 2010. doi:10.2140/camcos.2010.5.65.">GW10</a>]</span></p>
</div>
<div class="admonition-pymc3 admonition">
<p class="admonition-title">PyMC3:</p>
<p><a class="reference external" href="https://docs.pymc.io/">PyMC3</a> is a Python package for Bayesian statistical modeling and probabilistic machine learning which focuses on advanced Markov chain Monte Carlo and variational fitting algorithms.</p>
</div>
<div class="admonition-pystan admonition">
<p class="admonition-title">PyStan:</p>
<p><a class="reference external" href="https://pystan.readthedocs.io/en/latest/">PyStan</a> provides an interface to <a class="reference external" href="http://mc-stan.org/">Stan</a>, a package for Bayesian inference using the No-U-Turn sampler, a variant of Hamiltonian Monte Carlo.</p>
</div>
<div class="admonition-pymultinest admonition">
<p class="admonition-title">PyMultiNest:</p>
<p><a class="reference external" href="https://johannesbuchner.github.io/PyMultiNest/">PyMultiNest</a> interacts with <a class="reference external" href="https://github.com/farhanferoz/MultiNest">MultiNest</a> <span id="id3">[<a class="reference internal" href="../Backmatter/bibliography.html#id25" title="F Feroz, M P Hobson, and M Bridges. MultiNest: an efficient and robust Bayesian inference tool for cosmology and particle physics. Monthly Notices of the Royal Astronomical Society, 398(4):1601â€“1614, September 2009. doi:10.1111/j.1365-2966.2009.14548.x.">FHB09</a>]</span>, a Nested Sampling Monte Carlo library.</p>
</div>
</section>
<section id="visualizations-of-mcmc">
<h2>Visualizations of MCMC<a class="headerlink" href="#visualizations-of-mcmc" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>There are excellent javascript visualizations of MCMC sampling on the internet.</p></li>
<li><p>A particularly useful set of interactive demos was created by Chi Feng, and is available on the github page: <a class="reference external" href="https://chi-feng.github.io/mcmc-demo/">The Markov-chain Monte Carlo Interactive Gallery</a></p></li>
<li><p>An accessible introduction to MCMC, with simplified versions of Fengâ€™s visualizations, was created by Richard McElreath. It promotes Hamiltonian Monte Carlo and is available in a blog entry called <a class="reference external" href="http://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/">Markov Chains: Why Walk When You Can Flow?</a></p></li>
</ul>
<p>Assume that we have <span class="math notranslate nohighlight">\(N\)</span> samples from the joint pdf. This might be the Markov Chain from an MCMC sampler: <span class="math notranslate nohighlight">\(\left\{ (\theta_0, \theta_1)_i \right\}_{i=0}^{N-1}\)</span>. Then the marginal distribution of <span class="math notranslate nohighlight">\(\theta_1\)</span> will be given by the same chain by simply ignoring the <span class="math notranslate nohighlight">\(\theta_0\)</span> column, i.e., <span class="math notranslate nohighlight">\(\left\{ \theta_{1,i} \right\}_{i=0}^{N-1}\)</span>.</p>
<!-- !split -->
</section>
<section id="challenges-in-mcmc-sampling">
<h2>Challenges in MCMC sampling<a class="headerlink" href="#challenges-in-mcmc-sampling" title="Link to this heading">#</a></h2>
<p>There is much to be written about challenges in performing MCMC sampling and diagnostics that should be made to ascertain that your Markov chain has converged (although it is not really possible to be 100% certain except in special cases.)</p>
<p>We will not focus on these issues here, but just list a few problematic pdfs:</p>
<ul class="simple">
<li><p>Correlated distributions that are very narrow in certain directions. (scaled parameters needed)</p></li>
<li><p>Donut or banana shapes. (very low acceptance ratios)</p></li>
<li><p>Multimodal distributions. (might easily get stuck in local region of high probability and completely miss other regions.)</p></li>
</ul>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<div class="exercise admonition" id="exercise:metropolis-sampling-uniform">

<p class="admonition-title"><span class="caption-number">Exercise 16.16 </span> (Metropolis sampling of a uniform distribution)</p>
<section id="exercise-content">
<p>Show that the update rule implemented in <a class="reference internal" href="MarkovChains.html#example:reversible-markov-process">Example 16.3</a> has the uniform distribution <span class="math notranslate nohighlight">\(\mathcal{U}(0,1)\)</span> as its limiting distribution.</p>
</section>
</div>
<div class="exercise admonition" id="exercise:power-law-distribution-sampling">

<p class="admonition-title"><span class="caption-number">Exercise 16.17 </span> (Power-law distributions)</p>
<section id="exercise-content">
<p>Discrete power-law distributions have the general form <span class="math notranslate nohighlight">\(p(i) \propto i^\alpha\)</span>. for some constant <span class="math notranslate nohighlight">\(\alpha\)</span>. Unlike exponentially decaying distributions (such as the normal distribution and many others), power-law distributions have <em>fat tails</em>. They are therefore often used to model skewed data sets. Let</p>
<div class="math notranslate nohighlight">
\[
\pi_i = \frac{i^{-3/2}}{\sum_{k=1}^\infty k^{-3/2}}, \quad \text{for } i = 1, 2, \ldots
\]</div>
<p>Implement a Metropolis algorithm to sample for <span class="math notranslate nohighlight">\(\boldsymbol{\pi}\)</span>.</p>
</section>
</div>
<div class="exercise admonition" id="exercise:MCMC:discrete-metropolis">

<p class="admonition-title"><span class="caption-number">Exercise 16.18 </span> (The Metropolis algorithm for a discrete distribution)</p>
<section id="exercise-content">
<p>Use the Metropolis algorithm as outlined in <a class="reference internal" href="MarkovChains.html#remark:MCMC:Metropolis-discrete">Remark 16.1</a> to construct a transition matrix <span class="math notranslate nohighlight">\(T\)</span> for a Markov chain that gives the discrete limiting distribution</p>
<div class="math notranslate nohighlight">
\[
\pi = \left(\frac{1}{9},\frac{3}{4},\frac{5}{36}\right).
\]</div>
<p>Assume that the step proposal matrix has <span class="math notranslate nohighlight">\(S(i,j)=1/3\)</span> for all <span class="math notranslate nohighlight">\(i,j\)</span>.</p>
<ul class="simple">
<li><p>Compute <span class="math notranslate nohighlight">\(T\)</span> by hand, or numerically using Python. Verify that it is a valid transition matrix and that the above distribution is stationary.</p></li>
<li><p>Verify that <span class="math notranslate nohighlight">\(\pi\)</span> and <span class="math notranslate nohighlight">\(T\)</span> fulfills detailed balance.</p></li>
<li><p>Note that this distribution is the limiting distribution found in <a class="reference internal" href="MarkovChains.html#exercise:stationary-gothenburg-winter-weather"><span class="std std-numref">Exercise 16.12</span></a>. However, the transition matrices are not the same. Try to explain why. Can you construct other transition matrices with the same limiting distribution?</p></li>
<li><p>Construct the Markov chain outcomes of random variable <span class="math notranslate nohighlight">\(\{X_n\}\)</span> numerically. Plot the convergence of the discrete distribution for different
starting distributions and see if the correct limiting distribution is obtained.</p></li>
</ul>
</section>
</div>
</section>
<section id="solutions">
<h2>Solutions<a class="headerlink" href="#solutions" title="Link to this heading">#</a></h2>
<div class="solution dropdown admonition" id="solution:metropolis-sampling-uniform">

<p class="admonition-title">Solution to<a class="reference internal" href="#exercise:metropolis-sampling-uniform"> Exercise 16.16 (Metropolis sampling of a uniform distribution)</a></p>
<section id="solution-content">
<p>Check the acceptance criterion.</p>
</section>
</div>
<div class="solution dropdown admonition" id="solution:power-law-distribution-sampling">

<p class="admonition-title">Solution to<a class="reference internal" href="#exercise:power-law-distribution-sampling"> Exercise 16.17 (Power-law distributions)</a></p>
<section id="solution-content">
<p>It is suggested to use <span class="math notranslate nohighlight">\(\{-1,1\}\)</span> as the proposal step (with <span class="math notranslate nohighlight">\(p=0.5\)</span> a step to the left). The acceptence fraction involves a ratio of probabilities, which removes the infinite sum in the denominator. Note also that the acceptance criterion must include the reflective boundary at <span class="math notranslate nohighlight">\(i=1\)</span>.</p>
</section>
</div>
<div class="solution dropdown admonition" id="solution:MCMC:discrete-metropolis">

<p class="admonition-title">Solution to<a class="reference internal" href="#exercise:MCMC:discrete-metropolis"> Exercise 16.18 (The Metropolis algorithm for a discrete distribution)</a></p>
<section id="solution-content">
<ul class="simple">
<li><p>Construct the <span class="math notranslate nohighlight">\(T\)</span>-matrix</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
T =
\begin{pmatrix}
0.33333333 &amp; 0.33333333 &amp; 0.33333333 \\
0.04938272 &amp; 0.88888889 &amp; 0.0617284 \\
0.26666667 &amp; 0.33333333 &amp; 0.4       
\end{pmatrix}
\end{split}\]</div>
<p>and note that all elements are positive and that the row sums equal one. You can verify by hand that <span class="math notranslate nohighlight">\(\pi = \pi T\)</span> with the given distribution.</p>
<ul class="simple">
<li><p>Eq. <a class="reference internal" href="MarkovChains.html#equation-eq-markovchains-detailed-balance">(16.13)</a> gives nine equalities that should be checked.</p></li>
<li><p>The stationary distribution <span class="math notranslate nohighlight">\(\pi\)</span> is a left eigenvector of <span class="math notranslate nohighlight">\(T\)</span> with eigenvalue <span class="math notranslate nohighlight">\(+1\)</span>. But there is no uniqueness theorem that dictates that there can only be one matrix with this eigenvalue/vector. It is easy to create other transition matrices with the same limiting distribution by just changing the step proposal matrix.</p></li>
<li><p>Generate the Markov chain and see that it converges</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">t</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.33333333</span><span class="p">,</span> <span class="mf">0.33333333</span><span class="p">,</span> <span class="mf">0.33333333</span><span class="p">],</span>
 <span class="p">[</span><span class="mf">0.04938272</span><span class="p">,</span> <span class="mf">0.88888889</span><span class="p">,</span> <span class="mf">0.0617284</span> <span class="p">],</span>
 <span class="p">[</span><span class="mf">0.26666667</span><span class="p">,</span> <span class="mf">0.33333333</span><span class="p">,</span> <span class="mf">0.4</span>       <span class="p">]])</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">40</span> <span class="c1"># Number of steps</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># Start distribution</span>

<span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">N</span><span class="p">):</span>
    <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">t</span><span class="p">))</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</section>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./LearningFromData-content/StochasticProcesses"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="MarkovChains.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">16.1. </span>Markov chains</p>
      </div>
    </a>
    <a class="right-next"
       href="MCMC_intro_BUQ.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">16.3. </span>MCMC Intro from BUQEYE</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#general-problems-in-bayesian-inference">General problems in Bayesian inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-integration">Numerical integration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#quadrature-methods">Quadrature methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-integration">Monte Carlo integration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#monte-carlo-integration-in-statistics">Monte Carlo integration in statistics</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-from-a-pdf">Sampling from a PDF</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-metropolis-hastings-algorithm">The Metropolis-Hastings algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#state-of-the-art-mcmc-implementations">State-of-the-art MCMC implementations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizations-of-mcmc">Visualizations of MCMC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-in-mcmc-sampling">Challenges in MCMC sampling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions">Solutions</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christian ForssÃ©n, Dick Furnstahl, and Daniel Phillips
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>