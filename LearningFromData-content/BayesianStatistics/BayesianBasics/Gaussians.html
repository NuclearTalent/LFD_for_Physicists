
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>5.3. Gaussians: A couple of frequentist connections &#8212; Combined Learning from Data materials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/coloredpages.css?v=0a037ad7" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=6bd7df4c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/myadmonitions.css?v=89ac28d1" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"loader": {"load": ["[tex]/textmacros"]}, "chtml": {"mtextInheritFont": true}, "tex": {"packages": {"[+]": ["textmacros"]}, "macros": {"data": "\\mathcal{D}", "pars": "\\boldsymbol{\\theta}", "para": "\\theta", "optpars": "\\pars^*", "optpara": "\\para^*", "prob": "\\mathbb{P}", "cprob": ["\\prob\\left( #1 \\, \\left\\vert \\, #2 \\right. \\right)", 2], "cprobsub": ["\\prob_{#1}\\left( #2 \\, \\left\\vert \\, #3 \\right. \\right)", 3], "pdf": ["p \\left( #1 \\, \\left\\vert \\, #2 \\right. \\right)", 2], "pdfsub": ["p_{#1} \\left( #2 \\, \\left\\vert \\, #3 \\right. \\right)", 3], "p": ["p \\left( #1 \\right)", 1], "psub": ["p_{#1} \\left( #2 \\right)", 2], "futuredata": "\\mathcal{F}", "expect": ["\\mathbb{E} \\left[ #1 \\right]", 1], "var": ["\\text{Var} \\left( #1 \\right)", 1], "std": ["\\text{Std} \\left( #1 \\right)", 1], "cov": ["\\text{Cov} \\left( #1, #2 \\right)", 2], "dmat": "\\boldsymbol{X}", "models": ["\\boldsymbol{M}\\left( #1 \\, ; \\, #2 \\right)", 2], "model": ["M\\left( #1 \\, ; \\, #2 \\right)", 2], "modeloutputs": "\\boldsymbol{M}", "modeloutput": "M", "MLmodel": ["\\boldsymbol{\\hat{y}}\\left( #1 \\right)", 1], "MLoutputs": "\\boldsymbol{\\hat{y}}", "MLoutput": "\\hat{y}", "outputs": "\\boldsymbol{y}", "inputs": "\\boldsymbol{x}", "targets": "\\boldsymbol{t}", "weights": "\\boldsymbol{w}", "testoutputs": "\\boldsymbol{y}^\\odot", "testinputs": "\\boldsymbol{x}^\\odot", "output": "y", "inputt": "x", "target": "t", "weight": "w", "testoutput": "y^\\odot", "MLtestoutput": "\\hat{y}^\\odot", "testinput": "x^\\odot", "trainingdata": "\\mathcal{T}", "LaTeX": "\\text{LaTeX}", "residual": "\\epsilon", "residuals": "\\boldsymbol{\\epsilon}", "zeros": "\\boldsymbol{0}", "covres": "\\boldsymbol{\\Sigma_{\\epsilon}}", "covpars": "\\boldsymbol{\\Sigma_{\\pars}}", "tildecovpars": "\\boldsymbol{\\widetilde{\\Sigma}_{\\pars}}", "sigmas": "\\boldsymbol{\\sigma}", "sigmai": "\\sigma_i", "sigmares": "\\sigma_{\\epsilon}", "cbar": "\\bar c", "Lra": "\\Longrightarrow", "yth": "y_{\\text{th}}", "yexp": "y_{\\text{exp}}", "ym": "y_{\\text{m}}", "thetavec": "\\boldsymbol{\\theta}", "parsLR": "\\boldsymbol{\\beta}", "paraLR": "\\beta", "covparsLR": "\\boldsymbol{\\Sigma_{\\parsLR}}", "optparsLR": "\\parsLR^*", "optparaLR": "\\paraLR^*", "tildecovparsLR": "\\boldsymbol{\\widetilde{\\Sigma}_{\\parsLR}}", "alphavec": "\\boldsymbol{\\alpha}", "muvec": "\\boldsymbol{\\mu}", "phivec": "\\boldsymbol{\\phi}", "betavec": "\\boldsymbol{\\beta}", "sigmavec": "\\boldsymbol{\\sigma}", "Sigmavec": "\\boldsymbol{\\Sigma}", "thetavechat": "\\widehat\\thetavec", "avec": "\\boldsymbol{a}", "Bvec": "\\boldsymbol{B}", "fvec": "\\boldsymbol{f}", "mvec": "\\boldsymbol{m}", "qvec": "\\boldsymbol{q}", "rvec": "\\boldsymbol{r}", "uvec": "\\boldsymbol{u}", "wvec": "\\boldsymbol{w}", "xvec": "\\boldsymbol{x}", "yvec": "\\boldsymbol{y}", "wt": "\\widetilde", "nb": "n_b", "mel": ["\\langle #1 | #2 | #3 \\rangle", 3], "qoi": "\\mathbf{Q}", "ytrue": "y_{\\text{true}}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'LearningFromData-content/BayesianStatistics/BayesianBasics/Gaussians';</script>
    <script src="../../../_static/custom.js?v=33f35b7a"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="ðŸ“¥ Visualization of the Central Limit Theorem" href="visualization_of_CLT.html" />
    <link rel="prev" title="5.2. ðŸ“¥ Visualizing correlated Gaussian distributions" href="Visualizing_correlated_gaussians.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../Intro/About.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/logo-copilot.png" class="logo__image only-light" alt="Combined Learning from Data materials - Home"/>
    <script>document.write(`<img src="../../../_static/logo-copilot.png" class="logo__image only-dark" alt="Combined Learning from Data materials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../Intro/About.html">
                    Learning from data for physicists:
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Intro/Invitation.html">1. Invitation to inductive inference</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Intro/Introduction.html">2. Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Intro/Introduction/sec-01-physicist-s-perspective.html">2.1. Physicistâ€™s perspective</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Intro/Introduction/sec-02-bayesian-workflow.html">2.2. Bayesian workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Intro/Introduction/sec-03-machine-learning.html">2.3. Machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Intro/Introduction/sec-04-virtues.html">2.4. Virtues</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I: Bayesian methods for scientific modeling</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="RootBayesianBasics.html">3. Overview of Part I</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Inferenceandpdfs.html">4. Inference and PDFs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Inferenceandpdfs/sec-01-statements.html">4.1. Statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="Inferenceandpdfs/sec-02-manipulating-probabilities-bayesian-rules-of-probability-as.html">4.2. Manipulating probabilities: Bayesian rules of probability as principles of logic</a></li>
<li class="toctree-l2"><a class="reference internal" href="Inferenceandpdfs/sec-03-probability-density-functions.html">4.3. Probability density functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="Inferenceandpdfs/sec-04-summary.html">4.4. Looking ahead</a></li>
<li class="toctree-l2"><a class="reference internal" href="MoreBayesTheorem.html">4.5. Review of Bayesâ€™ theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="DataModelsPredictions.html">4.6. Data, models, and predictions</a></li>
<li class="toctree-l2"><a class="reference internal" href="Bayesian_epistemology.html">4.7. *Aside: Bayesian epistemology</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="Posteriors.html">5. Bayesian posteriors</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 has-children"><a class="reference internal" href="Exploring_pdfs.html">5.1. ðŸ“¥ Exploring PDFs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="Exploring_pdfs_followups.html">Follow-ups to Exploring PDFs</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="Visualizing_correlated_gaussians.html">5.2. ðŸ“¥ Visualizing correlated Gaussian distributions</a></li>
<li class="toctree-l2 current active has-children"><a class="current reference internal" href="#">5.3. Gaussians: A couple of frequentist connections</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="visualization_of_CLT.html">ðŸ“¥ Visualization of the Central Limit Theorem</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/Interpreting2Dposteriors.html">5.4. Interpreting 2D posteriors</a></li>
<li class="toctree-l2"><a class="reference internal" href="chi_squared_tests.html">5.5. ðŸ“¥ Demonstration: Sum of normal variables squared</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="CoinTossing.html">6. Updating via Bayes' rule</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="CoinTossing/sec-01-coin-tossing-frequentists-and-bayesaians.html">6.1. Coin tossing: Frequentists and Bayesaians</a></li>
<li class="toctree-l2"><a class="reference internal" href="CoinTossing/sec-02-when-do-priors-matter-when-don-t-they-matter.html">6.2. When do priors matter? When donâ€™t they matter?</a></li>
<li class="toctree-l2"><a class="reference internal" href="CoinTossing/sec-03-computing-the-posterior-analytically.html">6.3. Computing the posterior analytically</a></li>
<li class="toctree-l2"><a class="reference internal" href="CoinTossing/sec-04-degree-of-belief-credibility-intervals-vs-frequentist-1-sigm.html">6.4. Degree of belief/credibility intervals vs frequentist 1-sigma intervals</a></li>
<li class="toctree-l2"><a class="reference internal" href="CoinTossing/sec-05-take-aways-and-follow-up-questions-from-coin-flipping.html">6.5. Take-aways and follow-up questions from coin flipping</a></li>
<li class="toctree-l2"><a class="reference internal" href="demo-BayesianBasics.html">6.6. ðŸ“¥ Demonstration:  Bayesian Coin Tossing</a></li>
<li class="toctree-l2"><a class="reference internal" href="Bayesian_updating_coinflip_interactive.html">6.7. ðŸ“¥ Demonstration: Coin tossing (with widget)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="ErrorPropagation.html">7. Error propagation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="ErrorPropagation/sec-01-error-propagation-i-nuisance-parameters-and-marginalization.html">7.1. Error propagation (I): Nuisance parameters and marginalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="ErrorPropagation/sec-02-error-propagation-ii-changing-variables.html">7.2. Error propagation (II): Changing variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="ErrorPropagation/sec-03-error-propagation-iii-a-useful-approximation.html">7.3. Error propagation (III): A useful approximation</a></li>
<li class="toctree-l2"><a class="reference internal" href="ErrorPropagation/sec-04-solutions.html">7.4. Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="UsingBayes.html">8. Bayes in practice</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="BayesianAdvantages.html">8.1. Advantages of the Bayesian approach</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianWorkflow/BayesianWorkflow.html">8.2. Bayesian research workflow</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../BayesianLinearRegression/BayesianLinearRegression_rjf.html">8.3. Bayesian Linear Regression (BLR)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ModelingOptimization/demo-ModelValidation.html">ðŸ“¥ Demonstration: Linear Regression and Model Validation</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianParameterEstimation/Exercises_parameter_estimation.html">9. Exercises for Part I</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="exercise_sum_product_rule.html">9.1. Exercise: Checking the sum and product rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="exercise_medical_example_by_Bayes.html">9.2. Exercise: Standard medical example using Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/parameter_estimation_Gaussian_noise.html">9.3. ðŸ“¥ Parameter estimation I: Gaussian mean and variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/radioactive_lighthouse_exercise.html">9.4. ðŸ“¥ Radioactive lighthouse problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/amplitude_in_presence_of_background.html">9.5. ðŸ“¥ Amplitude of a signal in the presence of background</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/parameter_estimation_fitting_straight_line_I.html">9.6. ðŸ“¥ Parameter estimation example: fitting a straight line</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/parameter_estimation_fitting_straight_line_II.html">9.7. ðŸ“¥ Parameter estimation example: fitting a straight line II</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II: Advanced Bayesian methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../RootAdvancedMethods.html">10. Overview of Part II</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/DiscrepancyModels.html">11. Discrepancy Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/DiscrepancyModels/sec-01-koh-and-boh-discrepancy-models.html">11.1. KOH and BOH discrepancy models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/DiscrepancyModels/sec-02-framework.html">11.2. Framework</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/DiscrepancyModels/sec-03-the-ball-drop-model.html">11.3. The ball-drop model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/MD_balldrop_v1.html">11.4. ðŸ“¥ Ball-drop experiment notebook</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../AssigningProbabilities/Assigning.html">12. Assigning probabilities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../AssigningProbabilities/IgnorancePDF.html">12.1. Assigning probabilities (I): Indifferences and translation groups</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../AssigningProbabilities/demo-straight_lines.html">Alternative notebook with MCMC sampling</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../AssigningProbabilities/MaxEnt2.html">12.2. Assigning probabilities (II): The principle of maximum entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../AssigningProbabilities/MaxEnt_Function_Reconstruction.html">12.3. ðŸ“¥ Maximum Entropy for reconstructing a function from its moments</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../BayesianParameterEstimation/dealing_with_outliers.html">13. ðŸ“¥ Dealing with outliers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ComputationalBayes/BayesLinear.html">14. Bayes goes linear: History matching</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Multimodel_inference.html">15. Multi-model inference with Bayes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../ModelSelection/ModelSelection.html">15.1. Model Selection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../ModelSelection/BUQ/Evidence_for_model_EFT_coefficients.html">Evidence calculation for EFT expansions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ModelSelection/BUQ/two_model_evidence.html">Follow-up to EFT evidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ModelSelection/BUQ/computing_evidence.html">Computing the evidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ModelSelection/BUQ/MCMC-parallel-tempering_ptemcee_vs_zeus.html">Demo: Multimodal distributions with two samplers</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../ModelMixing/model_mixing.html">15.2. Model averaging and mixing </a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III: MCMC sampling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../StochasticProcesses/RootMCMC.html">16. Overview of Part III</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/StochasticProcesses.html">17. Stochastic processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/Metropolis_Poisson_example.html">17.7. Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/demo-MCMC.html">17.8. Demonstration: Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/Recap_BUQ.html">17.9. Recap of Poisson and more about MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/parameter_estimation_Gaussian_noise-2.html">17.10. Parameter estimation example: Gaussian noise and averages II</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/MCMC-random-walk-and-sampling.html">17.11. Exercise: Random walk</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/MCMC_overview.html">18. Overview of Markov Chain Monte Carlo</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/MarkovChains.html">18.1. Markov chains</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/MCMC.html">18.2. Markov chain Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/MCMC_intro_BUQ.html">18.3. Alternative MCMC introduction (Gregory)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/Assignment_extending_radioactive_lighthouse.html">18.4. Assignment: 2D radioactive lighthouse location using MCMC</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/Advanced_MCMC.html">19. Advanced MCMC</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ComputationalBayes/AdvancedMCMC.html">19.1. Advanced Markov chain Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/MCMC-diagnostics.html">19.2. Overview: MCMC Diagnostics</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/intuition_sampling.html">19.4. Intuition on sampling and best practices</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/Other_samplers.html">20. HMC and other samplers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../StochasticProcesses/BUQ2/HMC_intro_BUQ.html">20.1. Hamiltonian Monte Carlo (HMC) overview and visualization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../StochasticProcesses/BUQ2/Liouville_theorem_visualization.html">Liouville Theorem Visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../StochasticProcesses/BUQ2/Orbital_eqs_with_different_algorithms.html">Solving orbital equations with different algorithms</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/zeus.html">20.2. The Zeus Ensemble Slice Sampler</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../StochasticProcesses/BUQ2/PyMC_intro_updated.html">20.3. PyMC Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../StochasticProcesses/OverviewIntroPyMC.html">Overview of Intro to PyMC notebook</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ2/parameter_estimation_Gaussian_noise_compare_samplers.html">20.4. Comparing samplers for a simple problem</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV: Machine learning: A Bayesian perspective</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../MachineLearning/RootML.html">21. Overview of Part IV</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/GP/RootGP.html">22. Overview of Gaussian processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../MachineLearning/GP/GaussianProcesses.html">22.4. Introduction to Gaussian processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/CF/demo-GaussianProcesses.html">ðŸ“¥ demo-GaussianProcesses notebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/lecture_20.html">GP recap; GP applications; (old lecture 20)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../MachineLearning/GP/Sklearn_demos.html">22.5. scikit-learn demo notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/plot_gpr_noisy_targets.html">ðŸ“¥ One-dimension regression example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/plot_gpr_prior_posterior.html">ðŸ“¥ Prior and posterior with different kernels</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../MachineLearning/GP/GPy_demos.html">22.6. GPy demo notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/demo-GaussianProcesses.html">Gaussian processes demonstration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/CF/exercise_GP_GPy.html">Exercise: Gaussian processes using <code class="docutils literal notranslate"><span class="pre">GPy</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/Gaussian_processes_exercises.html">Exercise: Gaussian Process models with GPy</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/LogReg/LogReg.html">23. Logistic Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/MachineLearningExamples.html">23.5. Machine Learning: First Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/NeuralNet/exercises_LogReg_NeuralNet.html">23.6. Exercise: Logistic Regression and neural networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/ANN/MachineLearning.html">24. Machine learning: Overview and notation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/NeuralNet.html">24.5. Artifical neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/NeuralNet/demo-NeuralNet.html">24.6. Demonstration: Neural network classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/Neural_Network_for_simple_function_in_PyTorch.html">24.7. ðŸ“¥ ANN from ChatGPT using PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/ModelValidation.html">24.8. Model validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/DataBiasFairness.html">24.9. Data bias and fairness in machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/NeuralNet/NeuralNetBackProp.html">24.10. *Neural networks: Backpropagation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/ANNFT.html">25. ANNs in the large-width limit (ANNFT)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/random_initialized_ANN_vs_width.html">25.3. ðŸ“¥ Distributions of Randomly-Initialized ANNs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/BNN/bnn.html">26. Bayesian neural nets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/BNN/demo-bnn.html">26.4. Demonstration: Variational Inference and Bayesian Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/BNN/exercises_BNN.html">26.5. Exercise: Bayesian neural networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/CNN/cnn.html">27. *Convolutional neural nets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/CNN/demo-cnn.html">27.6. Demonstration: Image recognition with Convolutional Neural Networks</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V: Other topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../OtherTopics/RootOtherTopics.html">28. Overview of Part V </a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/Emulators.html">29. Emulators</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ComputationalBayes/BayesFast.html">29.1. Bayes goes fast: Emulators (from CF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ComputationalBayes/extra_RBM_emulators.html">29.2. RBM emulators (BUQ)</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../OtherTopics/Student_t_distribution_from_Gaussians.html">30. ðŸ“¥ Student t distribution from Gaussians</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/SVD.html">31. PCA, SVD, and all that</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/linear_algebra_games_including_SVD.html">31.5. ðŸ“¥ demo-SVD notebook</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../OtherTopics/qbism.html">32. QBism: Bayesian quantum mechanics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Backmatter</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Backmatter/bibliography.html">33. Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Backmatter/JB_tests.html">34. Guide to Jupyter Book markdown</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix A: Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Reference/Statistics.html">35. Statistics concepts and notation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/GradientDescent.html">36. Gradient-descent optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix B: Scientific modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/RootScientificModeling.html">37. Overview of scientific modeling material</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling.html">38. Overview of modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-01-notation.html">38.1. Notation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-02-models-in-science.html">38.2. Models in science</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-03-parametric-versus-non-parametric-models.html">38.3. Parametric versus non-parametric models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-04-linear-versus-non-linear-models.html">38.4. Linear versus non-linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-05-regression-analysis-optimization-versus-inference.html">38.5. Regression analysis: optimization versus inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-06-exercises.html">38.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-07-solutions.html">38.7. Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ModelingOptimization/LinearModels.html">39. Linear models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/LinearModels/sec-01-definition-of-linear-models.html">39.1. Definition of linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/LinearModels/sec-02-regression-analysis-with-linear-models.html">39.2. Regression analysis with linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/LinearModels/sec-03-ordinary-linear-regression-warmup.html">39.3. Ordinary linear regression: warmup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/LinearModels/sec-04-ordinary-linear-regression-in-practice.html">39.4. Ordinary linear regression in practice</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/LinearModels/sec-05-solutions.html">39.5. Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ModelingOptimization/MathematicalOptimization.html">40. Mathematical optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/MathematicalOptimization/sec-01-gradient-descent-optimization.html">40.1. Gradient-descent optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/MathematicalOptimization/sec-02-batch-stochastic-and-mini-batch-gradient-descent.html">40.2. Batch, stochastic and mini-batch gradient descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/MathematicalOptimization/sec-03-adaptive-gradient-descent-algorithms.html">40.3. Adaptive gradient descent algorithms</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix C: Getting started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Setup/RootGettingStarted.html">41. Overview of Getting started material</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Setup/exercise_Intro_01_Jupyter_Python.html">42. ðŸ“¥ Exercise: Jupyter notebooks and Python</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Setup/more_python_and_jupyter.html">43. More about Python and Jupyter notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/exercise_Intro_02_Jupyter_Python.html">43.4. ðŸ“¥ Python lists and iterations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/exercise_Intro_03_Numpy.html">43.5. ðŸ“¥ Linear algebra operations with NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/demo-Intro.html">43.6. ðŸ“¥ Reading data and fitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/Simple_widgets_v1.html">43.7. ðŸ“¥ Making a simple widget-based UI</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Setup/setting_up.html">44. Setting up for using this Jupyter Book</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/installing_anaconda.html">44.1. Using Anaconda</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/using_github.html">44.2. Using GitHub</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">TALENT mini-projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/RootMiniProjects.html">Overview of mini-projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/mini-project_I_toy_model_of_EFT.html">ðŸ“¥ MP I: Parameter estimation for a toy model of an EFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/model-selection_mini-project-IIa.html">ðŸ“¥ MP IIa: Model selection basics</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Mini-projects/model-selection_mini-project-IIb_How_many_lines_ptemcee.html">ðŸ“¥ MP IIb: How many lines?</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Mini-projects/Mini-project_IIb_overview.html">Overview of Mini-project IIb: How many lines?</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/mini-project_IIIa_bayesian_optimization.html">ðŸ“¥ MP IIIa: Bayesian optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/mini-project_IIIb_Bayesian_neural_networks_from_demo.html">ðŸ“¥ MP IIIb: Bayesian Neural Networks</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/NuclearTalent/LFD_for_Physicists" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/NuclearTalent/LFD_for_Physicists/issues/new?title=Issue%20on%20page%20%2FLearningFromData-content/BayesianStatistics/BayesianBasics/Gaussians.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/LearningFromData-content/BayesianStatistics/BayesianBasics/Gaussians.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Gaussians: A couple of frequentist connections</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-near-ubiquity-of-gaussians">The near ubiquity of Gaussians</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gaussian-is-to-statistics-what-the-harmonic-oscillator-is-to-mechanics">The Gaussian is to statistics what the harmonic oscillator is to mechanics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-central-limit-theorem">The Central Limit Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#consequences">Consequences:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#proof-of-the-clt-in-a-special-case">Proof of the CLT in a special case:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#notebook">Notebook:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#p-values-when-all-you-can-do-is-falsify">p-values: when all you can do is falsify</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contrast-bayesian-and-significance-analyses-for-coin-flipping">Contrast Bayesian and significance analyses for coin flipping</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-credible-intervals-and-frequentist-confidence-intervals">Bayesian credible intervals and frequentist confidence intervals</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="gaussians-a-couple-of-frequentist-connections">
<span id="sec-gaussians"></span><h1><span class="section-number">5.3. </span>Gaussians: A couple of frequentist connections<a class="headerlink" href="#gaussians-a-couple-of-frequentist-connections" title="Link to this heading">#</a></h1>
<section id="the-near-ubiquity-of-gaussians">
<h2>The near ubiquity of Gaussians<a class="headerlink" href="#the-near-ubiquity-of-gaussians" title="Link to this heading">#</a></h2>
<p>In <a class="reference internal" href="../BayesianParameterEstimation/parameter_estimation_Gaussian_noise.html"><span class="std std-doc">ðŸ“¥ Parameter estimation example: Gaussian noise and averages I</span></a> we sample a Gaussian distribution and
estimated its mean and variance. We are going to see a lot of Gaussian
distributions in this course. And indeed some people implicitly always
assume Gaussian distributions. So this seems like as good a place as
any to pause and consider why Gaussian distributions are a common
choice to describe noise, as well as thinking about the circumstances
in which that choice might be a poor one.</p>
<p>It turns out there are several reasons why one might choose a Gaussian
to describe a probability distribution. Here are two:</p>
<section id="the-gaussian-is-to-statistics-what-the-harmonic-oscillator-is-to-mechanics">
<h3>The Gaussian is to statistics what the harmonic oscillator is to mechanics<a class="headerlink" href="#the-gaussian-is-to-statistics-what-the-harmonic-oscillator-is-to-mechanics" title="Link to this heading">#</a></h3>
<p>Suppose we have a probability distribution <span class="math notranslate nohighlight">\(p(x | D,I)\)</span> that is
unimodal (has only one hump), then one way to form a â€œbest estimateâ€™â€™
for the variable <span class="math notranslate nohighlight">\(x\)</span> is to compute the maximum of the
distribution. (To save writing we denote the pdf of interest as <span class="math notranslate nohighlight">\(p(x)\)</span>
for a while hereafter.)</p>
<a class="bg-primary reference internal image-reference" href="../../../_images/point_estimate_cartoon.png"><img alt="point estimate" class="bg-primary align-right" src="../../../_images/point_estimate_cartoon.png" style="width: 250px;" />
</a>
<p>We find this point, which weâ€™ll denote by <span class="math notranslate nohighlight">\(x_0\)</span>, using calculus:</p>
<div class="math notranslate nohighlight">
\[
  \left.\frac{dp}{dx}\right|_{x_0} = 0
  \quad \mbox{with} \quad
    \left.\frac{d^2p}{dx^2}\right|_{x_0} &lt; 0 \ \text{(maximum)}.
\]</div>
<p>To characterize the posterior <span class="math notranslate nohighlight">\(p(x)\)</span>, we look nearby. We want to know
how sharp this maximum is: is <span class="math notranslate nohighlight">\(p(x)\)</span> sharply peaked around <span class="math notranslate nohighlight">\(x=x_0\)</span> or
is the maximum kind-of shallow? To work this out weâ€™ll do a Taylor
expansion around <span class="math notranslate nohighlight">\(x=x_0\)</span>.
<span class="math notranslate nohighlight">\(p(x)\)</span> itself
varies very rapidly, but since <span class="math notranslate nohighlight">\(p(x)\)</span> is positive definite we can
Taylor expand <span class="math notranslate nohighlight">\(\log p\)</span> instead. (See the box below for a strict mathematical
reason why itâ€™s problematic for our purposes to directly Taylor expand <span class="math notranslate nohighlight">\(p(x)\)</span> around its
maximum.)</p>
<div class="math notranslate nohighlight">
\[
 \Longrightarrow\ L(x) \equiv \log p(x|D,I) = 
   L(x_0) + \left.\frac{dL}{dx}\right|_{x_0 = 0}(x-x_0)
   + \frac{1}{2} \left.\frac{d^2L}{dx^2}\right|_{x_0 = 0}(x-x_0)^2 + \cdots
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\left.\frac{dL}{dx}\right|_{x_0 = 0}=0\)</span>, because <span class="math notranslate nohighlight">\(L(x_0)\)</span> is a maximum if <span class="math notranslate nohighlight">\(p(x_0)\)</span> is, and  <span class="math notranslate nohighlight">\(\left.\frac{d^2L}{dx^2}\right|_{x_0 = 0} &lt; 0\)</span>.
If we can neglect higher-order terms, then when we re-exponentiate,</p>
<div class="math notranslate nohighlight">
\[
  p(x| D,I) \approx A\, e^{\frac{1}{2}\left.\frac{d^2L}{dx^2}\right|_{x_0 = 0}(x-x_0)^2} ,
\]</div>
<p>with <span class="math notranslate nohighlight">\(A\)</span> a normalization factor. So in this general circumstance we get a Gaussian distribution. Comparing to</p>
<div class="math notranslate nohighlight">
\[
  p(x|D,I) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/\sigma^2}
  \quad\Longrightarrow\quad
  \mu = x_0, \ \sigma = \left(-\left.\frac{d^2L}{dx^2}\right|_{x_0}\right)^{-1/2},
\]</div>
<p>where we see the importance of the second derivative being negative.</p>
<p>We usually characterize the distribution by <span class="math notranslate nohighlight">\(x_0 \pm \sigma\)</span> (e.g., with a point and symmetric error bars if this is data noise), because <em>if</em> it is a Gaussian this is <em>sufficient</em> to tell us the entire distribution. E.g., <span class="math notranslate nohighlight">\(n\)</span> standard deviations is <span class="math notranslate nohighlight">\(n\times \sigma\)</span>.
But for a Bayesian, the full posterior <span class="math notranslate nohighlight">\(p(x|D,I)\)</span> for all <span class="math notranslate nohighlight">\(x\)</span> is the general result, and <span class="math notranslate nohighlight">\(x = x_0 \pm \sigma\)</span> may be only an approximate characterization.</p>
<div class="admonition-to-think-about admonition">
<p class="admonition-title">To think about â€¦</p>
<p>What if <span class="math notranslate nohighlight">\(p(x|D,I)\)</span> is asymmetric? What if it is multimodal?</p>
</div>
<div class="admonition-p-or-log-p admonition">
<p class="admonition-title"><span class="math notranslate nohighlight">\(p\)</span> or <span class="math notranslate nohighlight">\(\log p\)</span>?</p>
<p>We motivated Gaussian approximations from a Taylor expansion to quadratic order of the <em>logarithm</em> of a pdf.
What would go wrong if we directly expanded the pdf? Well, if we do
that we get:</p>
<div class="math notranslate nohighlight">
\[
  p(x) \approx p(x_0) + \frac{1}{2}\left.\frac{d^2p}{dx^2}\right|_{x_0}(x-x_0)^2
 \ \overset{x\pm\rightarrow\infty}{\longrightarrow} -\infty,
\]</div>
<p>i.e., we get something that diverges as <span class="math notranslate nohighlight">\(x\)</span> tends to either plus or
minus infinity.</p>
<a class="bg-primary reference internal image-reference" href="../../../_images/pdf_expansion_cartoon.png"><img alt="pdf expansion" class="bg-primary align-center" src="../../../_images/pdf_expansion_cartoon.png" style="width: 400px;" />
</a>
<p>A pdf must be normalizable and positive definite, so this approximation violates these conditions!</p>
</div>
</section>
<section id="the-central-limit-theorem">
<h3>The Central Limit Theorem<a class="headerlink" href="#the-central-limit-theorem" title="Link to this heading">#</a></h3>
<p>Another reason a Gaussian pdf emerges in many calculations is because
the Central Limit Theorem (CLT) states that the sum of random variables drawn
from all (or almost all) probability
distributions will eventually produce Gaussians if the number of samples in
each sum is large enough.</p>
<p><em>Central Limit Theorem</em>: The sum of <span class="math notranslate nohighlight">\(n\)</span> random values drawn from any
pdf of finite variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> tends as <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span> to
be Gaussian distributed about the expectation value of the sum, with
variance <span class="math notranslate nohighlight">\(n \sigma^2\)</span>.</p>
<section id="consequences">
<h4>Consequences:<a class="headerlink" href="#consequences" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>The mean of a large number of values becomes normally distributed
<em>regardless</em> of the probability distribution the values are drawn
from.</p></li>
<li><p>The binomial, Poisson, chi-squared, and Studentâ€™s t- distributions all approach
Gaussian distributions in the limit of a large number
of degrees of freedom (e.q., for large <span class="math notranslate nohighlight">\(n\)</span> for binomial). This consequence is probably not immediately obvious to you from the statement of the CLT! (See the <a class="reference internal" href="visualization_of_CLT.html"><span class="std std-doc">ðŸ“¥ Visualization of the Central Limit Theorem</span></a> notebook for an explanation of the Poisson distribution.)</p></li>
</ul>
<div class="my-checkpoint admonition">
<p class="admonition-title">Checkpoint Question</p>
<p>Why would we expect the CLT to work for a binomial distribution?</p>
<div class="dropdown my-hint admonition">
<p class="admonition-title">Hint</p>
<p>If we denote Bin(<span class="math notranslate nohighlight">\(n,p\)</span>) as the binomial distribution for <span class="math notranslate nohighlight">\(n\)</span> trials with probability <span class="math notranslate nohighlight">\(p\)</span> of success, how is Bin(<span class="math notranslate nohighlight">\(1,p\)</span>) related to Bin(<span class="math notranslate nohighlight">\(n,p\)</span>)?</p>
</div>
<div class="dropdown my-answer admonition">
<p class="admonition-title">Answer</p>
<p>If we add up <span class="math notranslate nohighlight">\(n\)</span> random variables from Bin(<span class="math notranslate nohighlight">\(1,p\)</span>), each with value 0 or 1, this is equivalent to a Bin(<span class="math notranslate nohighlight">\(n,p\)</span>) random variable with the same number of successes. So we already have a sum of random variables built in and the CLT will apply. (In more detail: getting <span class="math notranslate nohighlight">\(k\)</span> ones (and <span class="math notranslate nohighlight">\(n-k\)</span> zeros) from the <span class="math notranslate nohighlight">\(n\)</span> Bin(<span class="math notranslate nohighlight">\(1,p\)</span>) draws will have probability <span class="math notranslate nohighlight">\(p^k\)</span> times <span class="math notranslate nohighlight">\((1-p)^{n-k}\)</span> times the number of combinations <span class="math notranslate nohighlight">\(n\choose k\)</span>. This is the same as the binomial probability for <span class="math notranslate nohighlight">\(k\)</span> successes.)</p>
</div>
</div>
</section>
<section id="proof-of-the-clt-in-a-special-case">
<h4>Proof of the CLT in a special case:<a class="headerlink" href="#proof-of-the-clt-in-a-special-case" title="Link to this heading">#</a></h4>
<p>Start with <em>independent</em> random variables <span class="math notranslate nohighlight">\(x_1,\cdots,x_n\)</span> drawn from a distribution with mean <span class="math notranslate nohighlight">\(\langle x \rangle = 0\)</span> and variance <span class="math notranslate nohighlight">\(\langle x^2\rangle = \sigma^2\)</span>, where</p>
<div class="math notranslate nohighlight">
\[
  \langle x^n \rangle \equiv \int x^n p(x)\, dx 
\]</div>
<p>(generalize later to nonzero mean). Now let</p>
<div class="math notranslate nohighlight">
\[
  X = \frac{1}{\sqrt{n}}(x_1 + x_2 + \cdots + x_n)
   = \sum_{j=1}^n \frac{x_j}{\sqrt{n}} ,   
\]</div>
<p>scaling by <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span> so that the variance of <span class="math notranslate nohighlight">\(X\)</span> is constant in the <span class="math notranslate nohighlight">\(n\rightarrow\infty\)</span> limit.</p>
<p>What is the distribution of <span class="math notranslate nohighlight">\(X\)</span>?
<span class="math notranslate nohighlight">\(\Longrightarrow\)</span> call it <span class="math notranslate nohighlight">\(p(X|I)\)</span>, where <span class="math notranslate nohighlight">\(I\)</span> is the information about the probability distribution for <span class="math notranslate nohighlight">\(x_j\)</span>.</p>
<p><strong>Plan:</strong> Use the sum and product rules and their consequences to relate <span class="math notranslate nohighlight">\(p(X)\)</span> to what we know of <span class="math notranslate nohighlight">\(p(x_j)\)</span>. (Note: weâ€™ll suppress <span class="math notranslate nohighlight">\(I\)</span> to keep the formulas from getting too cluttered.)</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  p(X) &amp;= \int_{-\infty}^{\infty} dx_1 \cdots dx_n\,
            p(X,x_1,\cdots,x_n) \\
       &amp;= \int_{-\infty}^{\infty} dx_1 \cdots dx_n\,
            p(X|x_1,\cdots,x_n)\,p(x_1,\cdots,x_n) \\
       &amp;= \int_{-\infty}^{\infty} dx_1 \cdots dx_n\,
            p(X|x_1,\cdots,x_n)\,p(x_1)\cdots p(x_n) .     
\end{align}\end{split}\]</div>
<div class="dropdown admonition">
<p class="admonition-title">State the rule used to justify each step</p>
<ol class="arabic simple">
<li><p>marginalization</p></li>
<li><p>product rule</p></li>
<li><p>independence</p></li>
</ol>
</div>
<p>We might proceed by using a direct, normalized expression for <span class="math notranslate nohighlight">\(p(X|x_1,\cdots,x_n)\)</span>,</p>
<div class="my-checkpoint admonition">
<p class="admonition-title">Checkpoint question</p>
<p>What is <span class="math notranslate nohighlight">\(p(X|x_1,\cdots,x_n)\)</span>?</p>
<div class="dropdown my-answer admonition">
<p class="admonition-title">Answer</p>
<p><span class="math notranslate nohighlight">\(p(X|x_1,\cdots,x_n) = \delta\Bigl(X - \frac{1}{\sqrt{n}}(x_1 + \cdots + x_n)\Bigr)\)</span></p>
</div>
</div>
<p>and perform one of the integrations.
Instead we will use a Fourier representation:</p>
<div class="math notranslate nohighlight">
\[
p(X|x_1,\cdots,x_n) = \delta\Bigl(X - \frac{1}{\sqrt{n}}(x_1 + \cdots + x_n)\Bigr)
  = \frac{1}{2\pi} \int_{-\infty}^{\infty} d\omega
    \, e^{i\omega\Bigl(X - \frac{1}{\sqrt{n}}\sum_{j=1}^n x_j\Bigr)} .
\]</div>
<p>Substituting into <span class="math notranslate nohighlight">\(p(X)\)</span> and gathering together all pieces with <span class="math notranslate nohighlight">\(x_j\)</span> dependence while exchanging the order of integrations:</p>
<div class="math notranslate nohighlight">
\[ 
 p(X) = \frac{1}{2\pi} \int_{-\infty}^{\infty} d\omega
    \, e^{i\omega X} \prod_{j=1}^n \left[\int_{-\infty}^{\infty} dx_j\, e^{i\omega x_j / \sqrt{n}} p(x_j) \right] . 
\]</div>
<ul class="simple">
<li><p>Observe that the terms in []s have factorized into a product of independent integrals and they are all the same (just different labels for the integration variables).</p></li>
<li><p>Now we Taylor expand <span class="math notranslate nohighlight">\(e^{i\omega x_j/\sqrt{n}}\)</span>, arguing that the Fourier integral is dominated by small <span class="math notranslate nohighlight">\(x\)</span> as <span class="math notranslate nohighlight">\(n\rightarrow\infty\)</span>. (<em>When does this fail?</em>)</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
  e^{i\omega x/\sqrt{n}} = 1 + \frac{i\omega x}{\sqrt{n}}
    + \frac{(i\omega)^2 x^2}{2 n} + \mathcal{O}\left(\frac{\omega^3 x^3}{n^{3/2}}\right) .
\]</div>
<p>Then, using that <span class="math notranslate nohighlight">\(p(x)\)</span> is normalized (i.e., <span class="math notranslate nohighlight">\(\int_{-\infty}^{\infty} dx\, p(x) = 1\)</span>),</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
\int_{-\infty}^{\infty} dx\, e^{i\omega x / \sqrt{n}}
p(x)&amp;=
\int_{-\infty}^\infty dx p(x)
\left[1 +\frac{ i \omega x}{\sqrt{n}} + \frac{(i \omega)^2 x^2}{2 n} + \ldots\right]\\
&amp;=1  + \frac{i \omega}{\sqrt{n}} \langle x \rangle  - \frac{\omega^2}{2
n} \langle x^2 \rangle + \langle x^3 \rangle
\frac{\omega^3}{n^{3/2}}\\
&amp;=1 - \frac{\omega^2 \sigma^2}{2 n} +
\mathcal{O}\left(\frac{\omega^3}{n^{3/2}}\right) .
\end{align}
\end{split}\]</div>
<p>Now we can substitute into the posterior for <span class="math notranslate nohighlight">\(X\)</span> and take the large
<span class="math notranslate nohighlight">\(n\)</span> limit:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
p(X)&amp;=\frac{1}{2 \pi} \int_{-\infty}^\infty d \omega e^{i \omega X}
\left[1 - \frac{\omega^2 \sigma^2}{2 n} + \mathcal{O} \left(\frac{\omega^3}{n^{3/2}}\right)\right]^n\\
&amp;\stackrel{n\rightarrow \infty}{\longrightarrow} \frac{1}{2 \pi} \int
d\omega e^{i \omega X} e^{- \omega^2 \sigma^2/2}=\frac{1}{\sqrt{2
\pi}} e^{-X^2/(2 \sigma^2)}.
\end{align}
\end{split}\]</div>
<div class="my-checkpoint admonition">
<p class="admonition-title">Checkpoint question</p>
<p>How did we use the large <span class="math notranslate nohighlight">\(n\)</span> limit to get the first equality on the last line?</p>
<div class="dropdown my-answer admonition">
<p class="admonition-title">Answer</p>
<p>We used</p>
<div class="math notranslate nohighlight">
\[
  \lim_{n\rightarrow\infty} (1 + \frac{z}{n})^n = e^z \quad \text{for any $z$, here applied to }
  z = -\frac{\omega^2 \sigma^2}{2}.
\]</div>
<p>We can understand this intuitively by multiplying out</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{align}
    \underbrace{(1 + \frac{z}{n})(1 + \frac{z}{n})\cdots(1 + \frac{z}{n})}_{n\ \text{terms}}
    &amp;= 1 + n\cdot\frac{z}{n} + \binom{n}{2}\frac{z^2}{n^2} + \binom{n}{3}\frac{z^3}{n^3} + \ldots
    \\
    &amp;\overset{n\rightarrow\infty}{\longrightarrow} 1 + z + \frac{z^2}{2!} + \frac{z^3}{3!} + \ldots 
\end{align} \end{split}\]</div>
<p>In getting this result we have dropped terms that are of order <span class="math notranslate nohighlight">\(1/n\)</span> and <span class="math notranslate nohighlight">\(1/n^2\)</span>.
All terms that we dropped in truncating the Taylor expansions for each Fourier integral will also be suppressed by inverse powers (or half powers) of <span class="math notranslate nohighlight">\(n\)</span>.</p>
</div>
</div>
</section>
<section id="notebook">
<h4>Notebook:<a class="headerlink" href="#notebook" title="Link to this heading">#</a></h4>
<p>Look at <a class="reference internal" href="visualization_of_CLT.html"><span class="std std-doc">ðŸ“¥ Visualization of the Central Limit Theorem</span></a>.</p>
<p>Things to think about:</p>
<ul class="simple">
<li><p><em>What does â€œlargeâ€ number of degrees of freedom actually mean? Does
it matter where we look?</em></p></li>
<li><p><em>If we have a large number of draws from a uniform distribution, does the CLT imply that the histogrammed distribution should look like a Gaussian?</em></p></li>
<li><p><em>Can you identify a case where the CLT will fail?</em></p></li>
</ul>
</section>
</section>
</section>
<section id="p-values-when-all-you-can-do-is-falsify">
<h2>p-values: when all you can do is falsify<a class="headerlink" href="#p-values-when-all-you-can-do-is-falsify" title="Link to this heading">#</a></h2>
<p>A common way for a frequentist to discuss a theory/model, or put a
bound on a parameter value, is to quote a p-value.
This is set up using something called the null hypothesis. Somewhat
perversely you should pick the null hypothesis to be the opposite of
what you want to prove. So if you want to discover the Higgs boson,
the null hypothesis is that the Higgs boson does not exist.</p>
<p>Then you pick a level of proof you are comfortable with. For the
Higgs boson (and for many other particle physics experiments) it is
â€œ5 sigmaâ€™â€™. <em>How do you think we convert this statement to a
probability?</em> [Hint: refer to a Gaussian distribution.]
One minus the resulting probability is called the <span class="math notranslate nohighlight">\(p\)</span>-value. We will
denote it <span class="math notranslate nohighlight">\(p_{\rm crit}\)</span>. There is nothing
God-given about it. It is a standard (like â€œbeyond a reasonable
doubtâ€) that has been agreed upon in a research community for
determining that something is (likely) going on.</p>
<p>You then take data and compute <span class="math notranslate nohighlight">\(p(D|{\rm null~hypothesis})\)</span>. If
<span class="math notranslate nohighlight">\(p(D|{\rm null~ hypothesis}) &lt; p_{\rm crit}\)</span> then you conclude that the â€œthe null
hypothesis is rejected at the <span class="math notranslate nohighlight">\(p_{\rm crit}\)</span> significance levelâ€™â€™.
Note that if <span class="math notranslate nohighlight">\(p(D|{\rm null~hypothesis}) &gt; p_{\rm crit}\)</span> you cannot
conclude that the null hypothesis is true. It just means â€œno effect
was observedâ€.</p>
<div class="admonition-exercise admonition">
<p class="admonition-title">Exercise</p>
<p>Look at
<a class="reference internal" href="Bayesian_updating_coinflip_interactive.html"><span class="std std-doc">ðŸ“¥ Demonstration: Coin tossing (with widget)</span></a>. Pick
a <span class="math notranslate nohighlight">\(p_{\rm crit}\)</span>-value. If  <span class="math notranslate nohighlight">\(p_h=0.4\)</span>, work out how many coin tosses it would take
to reject the null hypothesis that itâ€™s a fair coin (<span class="math notranslate nohighlight">\(p_h=0.5\)</span>) at
this significance level.</p>
</div>
</section>
<section id="contrast-bayesian-and-significance-analyses-for-coin-flipping">
<h2>Contrast Bayesian and significance analyses for coin flipping<a class="headerlink" href="#contrast-bayesian-and-significance-analyses-for-coin-flipping" title="Link to this heading">#</a></h2>
<p>Suppose we do a coin flipping experiment where we toss a coin 20 times and it comes up heads 14 times. Letâ€™s compare how we might analyze this with Bayesian methods to how we might do a significance analysis with p-values.</p>
<p><strong>Bayesian analysis.</strong>
Following our study in <a class="reference internal" href="Bayesian_updating_coinflip_interactive.html"><span class="std std-doc">ðŸ“¥ Demonstration: Coin tossing (with widget)</span></a>, letâ€™s calculate the probability of heads, which we call <span class="math notranslate nohighlight">\(p_h\)</span>, given data <span class="math notranslate nohighlight">\(D = \{R \text{ heads}, N \text{ tosses}\}\)</span>.</p>
<ul class="simple">
<li><p>Letâ€™s assume a uniform prior (encoded as a beta function with <span class="math notranslate nohighlight">\(\alpha=1\)</span>, <span class="math notranslate nohighlight">\(\beta=1\)</span>).</p></li>
<li><p>We found that this can be expressed as a beta function, which we can calculate in Python using
<span class="math notranslate nohighlight">\(p(p_h|D) = p(D|p_h)p(p_h)\)</span> <span class="math notranslate nohighlight">\(\longrightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">scipy.stats.beta.pdf(p_h,1+R,1+N-R)</span></code>. For <span class="math notranslate nohighlight">\(N=20\)</span>, <span class="math notranslate nohighlight">\(R=14\)</span>, this is shown on the left below.</p></li>
<li><p>Now we can answer many questions, such as: what is the probability that <span class="math notranslate nohighlight">\(p_h\)</span> is between <span class="math notranslate nohighlight">\(0.49\)</span> and <span class="math notranslate nohighlight">\(0.51\)</span>? The answer comes from integrating our pdf over this interval (i.e., the area under the curve).</p></li>
<li><p>Note that in the Bayesian analysis, the data is given while the probability of heads is a random value.</p></li>
</ul>
<a class="bg-primary reference internal image-reference" href="../../../_images/beta_distribution_14heads_in_20tosses.png"><img alt="bootstrapping" class="bg-primary align-center" src="../../../_images/beta_distribution_14heads_in_20tosses.png" style="width: 700px;" />
</a>
<p><strong>Significance test.</strong> Here we will try to answer the question: Is the coin fair? Weâ€™ll follow the discussion in Wikipedia under <a class="reference external" href="https://en.wikipedia.org/wiki/P-value">p-value</a> titled â€œTesting the fairness of a coinâ€.</p>
<ul class="simple">
<li><p>We interpret â€œfairâ€ as meaning <span class="math notranslate nohighlight">\(p_h = 0.5\)</span>. Our null hypothesis is that the coin is fair and <span class="math notranslate nohighlight">\(p_h = 0.5\)</span>.</p></li>
<li><p>We plot the probabilities for getting <span class="math notranslate nohighlight">\(R\)</span> heads in <span class="math notranslate nohighlight">\(N=20\)</span> tosses using the binomial probability mass distribution on the right above.</p></li>
<li><p>We decide on the significance <span class="math notranslate nohighlight">\(p_{\rm crit} = 0.05\)</span>.</p></li>
<li><p>(Important conceptual question:) <span class="math notranslate nohighlight">\(p_{\rm crit}\)</span> and <span class="math notranslate nohighlight">\(p_h\)</span> are both probabilities. Turn to your neighbor and explain to them what they are the probabilities of and how they are different.</p></li>
<li><p>We need to find the probability of getting data <em>at least as extreme</em> as <span class="math notranslate nohighlight">\(D = \{R \text{ heads}, N \text{ tosses}\}\)</span>. For our example, that means adding up the binomial probabilities for <span class="math notranslate nohighlight">\(R = 14, 15, \ldots, 20\)</span>, which is 0.058. (This is called a â€œone-tailed testâ€. If we wanted to consider deviations favoring tails as well, we would would add as well the probabilities for <span class="math notranslate nohighlight">\(R = 0, 1, \ldots, 6\)</span>, so <span class="math notranslate nohighlight">\(0.115\)</span> in total. This would be a â€œtwo-tailed testâ€.)</p></li>
<li><p>Comparing to <span class="math notranslate nohighlight">\(p_{\rm crit} = 0.05\)</span>, we find the p-value is greater than <span class="math notranslate nohighlight">\(p_{\rm crit}\)</span>, so the null hypothesis is not rejected at the 95% level. (Note that we say â€œnot rejectedâ€ as opposed to â€œacceptedâ€.) If we had gotten 15 heads instead we would have rejected the null hypothesis.</p></li>
<li><p>Note that in this frequentist analysis, the <em>data</em> is random while <span class="math notranslate nohighlight">\(p_h\)</span> is fixed (although unknown).</p></li>
</ul>
<div class="admonition-exercise admonition">
<p class="admonition-title">Exercise</p>
<p>Verify that if we had gotten 15 heads in <span class="math notranslate nohighlight">\(N=20\)</span> tosses that we would have rejected the null hypothesis.</p>
</div>
</section>
<section id="bayesian-credible-intervals-and-frequentist-confidence-intervals">
<h2>Bayesian credible intervals and frequentist confidence intervals<a class="headerlink" href="#bayesian-credible-intervals-and-frequentist-confidence-intervals" title="Link to this heading">#</a></h2>
<p>We already commented on the difference between the
68% degree of belief interval for the most likely value (in that case
the bias weighting of the coin) and a frequentist <span class="math notranslate nohighlight">\(1 \sigma\)</span> confidence
interval.</p>
<ul class="simple">
<li><p>First point is that <span class="math notranslate nohighlight">\(1 \sigma=68\)</span>% assumes a Gaussian distribution
around the maximum of the posterior (cf. above). While this will
often work out okay, it may not. And, as we seek to translate,
<span class="math notranslate nohighlight">\(n \sigma\)</span> intervals into DoB statements, assuming a Gaussian
becomes more and more questionable the higher <span class="math notranslate nohighlight">\(n\)</span> is. (Why?)</p></li>
<li><p>But the second point is more philosophical (meta-statistical?). One
interval is a statement about <span class="math notranslate nohighlight">\(p(x|D,I)\)</span>, while the other is a
statement about <span class="math notranslate nohighlight">\(p(D|x,I)\)</span>.</p></li>
<li><p>(Note that because the conversion between the two pdfs requires the
use of Bayesâ€™ theorem the Bayesian interval may be affected by the
choice of the prior.)</p></li>
<li><p>Bayesian version is easy; a 68% credible interval or Bayesian
confidence interval or degree-of-belief (DoB) interval is: given
some data and some information <span class="math notranslate nohighlight">\(I\)</span>, there is a 68% chance (probability) that the interval contains the true parameter.</p></li>
<li><p>Frequentist 68% confidence interval</p>
<ul>
<li><p>Assuming the model (contained in <span class="math notranslate nohighlight">\(I\)</span>) and the value of the
parameter <span class="math notranslate nohighlight">\(x\)</span> then if we do the experiment a large number of
times then 68% of them will produce data in that interval.</p></li>
<li><p>So the <em>parameter</em> is fixed (no pdf) and the confidence
interval is a statement about data</p></li>
<li><p>Frequentists will try to make statements about parameters, but
they end up a bit tangled, e.g., â€œThere is a 68% probability
that when I compute a confidence interval from data of this sort
that the true value of <span class="math notranslate nohighlight">\(\theta\)</span> will fall within the
(hypothetical) space of observations.â€</p></li>
</ul>
</li>
<li><p>For a one-dimensional posterior that is symmetric, it is clear how to define the <span class="math notranslate nohighlight">\(d\%\)</span> confidence interval.</p>
<ul>
<li><p>Algorithm: start from the center, step outward on both sides, stop when <span class="math notranslate nohighlight">\(d\%\)</span> is enclosed.</p></li>
<li><p>For a two-dimensional posterior, need a way to integrate from the top. (Could lower a plane, as desribed below for HPD.)</p></li>
</ul>
</li>
<li><p>What if asymmetic or multimodal? Two of the possible choices:</p>
<ul>
<li><p>Equal-tailed interval (central interval): the area above and below the interval are equal.</p></li>
<li><p>Highest posterior density (HPD) region: posterior density for
every point is higher than the posterior density for any point
outside the
interval. [E.g., lower a horizontal line over the distribution until the desired interval percentage is covered by regions above the line.]</p></li>
</ul>
</li>
</ul>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./LearningFromData-content/BayesianStatistics/BayesianBasics"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Visualizing_correlated_gaussians.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">5.2. </span>ðŸ“¥ Visualizing correlated Gaussian distributions</p>
      </div>
    </a>
    <a class="right-next"
       href="visualization_of_CLT.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">ðŸ“¥ Visualization of the Central Limit Theorem</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-near-ubiquity-of-gaussians">The near ubiquity of Gaussians</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gaussian-is-to-statistics-what-the-harmonic-oscillator-is-to-mechanics">The Gaussian is to statistics what the harmonic oscillator is to mechanics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-central-limit-theorem">The Central Limit Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#consequences">Consequences:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#proof-of-the-clt-in-a-special-case">Proof of the CLT in a special case:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#notebook">Notebook:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#p-values-when-all-you-can-do-is-falsify">p-values: when all you can do is falsify</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contrast-bayesian-and-significance-analyses-for-coin-flipping">Contrast Bayesian and significance analyses for coin flipping</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-credible-intervals-and-frequentist-confidence-intervals">Bayesian credible intervals and frequentist confidence intervals</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christian ForssÃ©n, Dick Furnstahl, and Daniel Phillips
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>