
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4. Inference and PDFs &#8212; Combined Learning from Data materials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/coloredpages.css?v=0a037ad7" />
    <link rel="stylesheet" type="text/css" href="../../../_static/myadmonitions.css?v=89ac28d1" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"loader": {"load": ["[tex]/textmacros"]}, "chtml": {"mtextInheritFont": true}, "tex": {"packages": {"[+]": ["textmacros"]}, "macros": {"data": "\\mathcal{D}", "pars": "\\boldsymbol{\\theta}", "para": "\\theta", "optpars": "\\pars^*", "optpara": "\\para^*", "prob": "\\mathbb{P}", "cprob": ["\\prob\\left( #1 \\, \\left\\vert \\, #2 \\right. \\right)", 2], "cprobsub": ["\\prob_{#1}\\left( #2 \\, \\left\\vert \\, #3 \\right. \\right)", 3], "pdf": ["p \\left( #1 \\, \\left\\vert \\, #2 \\right. \\right)", 2], "pdfsub": ["p_{#1} \\left( #2 \\, \\left\\vert \\, #3 \\right. \\right)", 3], "p": ["p \\left( #1 \\right)", 1], "psub": ["p_{#1} \\left( #2 \\right)", 2], "futuredata": "\\mathcal{F}", "expect": ["\\mathbb{E} \\left[ #1 \\right]", 1], "var": ["\\text{Var} \\left( #1 \\right)", 1], "std": ["\\text{Std} \\left( #1 \\right)", 1], "cov": ["\\text{Cov} \\left( #1, #2 \\right)", 2], "dmat": "\\boldsymbol{X}", "models": ["\\boldsymbol{M}\\left( #1 \\, ; \\, #2 \\right)", 2], "model": ["M\\left( #1 \\, ; \\, #2 \\right)", 2], "modeloutputs": "\\boldsymbol{M}", "modeloutput": "M", "MLmodel": ["\\boldsymbol{\\hat{y}}\\left( #1 \\right)", 1], "MLoutputs": "\\boldsymbol{\\hat{y}}", "MLoutput": "\\hat{y}", "outputs": "\\boldsymbol{y}", "inputs": "\\boldsymbol{x}", "targets": "\\boldsymbol{t}", "weights": "\\boldsymbol{w}", "testoutputs": "\\boldsymbol{y}^\\odot", "testinputs": "\\boldsymbol{x}^\\odot", "output": "y", "inputt": "x", "target": "t", "weight": "w", "testoutput": "y^\\odot", "MLtestoutput": "\\hat{y}^\\odot", "testinput": "x^\\odot", "trainingdata": "\\mathcal{T}", "LaTeX": "\\text{LaTeX}", "residual": "\\epsilon", "residuals": "\\boldsymbol{\\epsilon}", "zeros": "\\boldsymbol{0}", "covres": "\\boldsymbol{\\Sigma_{\\epsilon}}", "covpars": "\\boldsymbol{\\Sigma_{\\pars}}", "tildecovpars": "\\boldsymbol{\\widetilde{\\Sigma}_{\\pars}}", "sigmas": "\\boldsymbol{\\sigma}", "sigmai": "\\sigma_i", "sigmares": "\\sigma_{\\epsilon}", "cbar": "\\bar c", "Lra": "\\Longrightarrow", "yth": "y_{\\text{th}}", "yexp": "y_{\\text{exp}}", "ym": "y_{\\text{m}}", "thetavec": "\\boldsymbol{\\theta}", "parsLR": "\\boldsymbol{\\beta}", "paraLR": "\\beta", "covparsLR": "\\boldsymbol{\\Sigma_{\\parsLR}}", "optparsLR": "\\parsLR^*", "optparaLR": "\\paraLR^*", "tildecovparsLR": "\\boldsymbol{\\widetilde{\\Sigma}_{\\parsLR}}", "alphavec": "\\boldsymbol{\\alpha}", "muvec": "\\boldsymbol{\\mu}", "phivec": "\\boldsymbol{\\phi}", "betavec": "\\boldsymbol{\\beta}", "sigmavec": "\\boldsymbol{\\sigma}", "Sigmavec": "\\boldsymbol{\\Sigma}", "thetavechat": "\\widehat\\thetavec", "avec": "\\boldsymbol{a}", "Bvec": "\\boldsymbol{B}", "fvec": "\\boldsymbol{f}", "mvec": "\\boldsymbol{m}", "qvec": "\\boldsymbol{q}", "rvec": "\\boldsymbol{r}", "uvec": "\\boldsymbol{u}", "wvec": "\\boldsymbol{w}", "xvec": "\\boldsymbol{x}", "yvec": "\\boldsymbol{y}", "wt": "\\widetilde", "nb": "n_b", "mel": ["\\langle #1 | #2 | #3 \\rangle", 3], "qoi": "\\mathbf{Q}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'LearningFromData-content/BayesianStatistics/BayesianBasics/Inferenceandpdfs';</script>
    <script src="../../../_static/custom.js?v=33f35b7a"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="4.5. Exercise: Checking the sum and product rules" href="exercise_sum_product_rule.html" />
    <link rel="prev" title="3. Bayesian methods for scientific modeling" href="RootBayesianBasics.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../Intro/About.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/logo-copilot.png" class="logo__image only-light" alt="Combined Learning from Data materials - Home"/>
    <script>document.write(`<img src="../../../_static/logo-copilot.png" class="logo__image only-dark" alt="Combined Learning from Data materials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../Intro/About.html">
                    About this Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Intro/Overview.html">1. Invitation to inductive inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Intro/Introduction.html">2. Introduction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I: Bayesian methods for scientific modeling</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="RootBayesianBasics.html">3. Overview of Part I</a></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">4. Inference and PDFs</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="exercise_sum_product_rule.html">4.5. Exercise: Checking the sum and product rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="exercise_medical_example_by_Bayes.html">4.6. Exercise: Standard medical example using Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="MoreBayesTheorem.html">4.7. More on Bayesâ€™ theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="Bayesian_epistemology.html">4.8. *Aside: Bayesian epistemology</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="DataModelsPredictions.html">4.9. Data, models, and predictions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../BayesianParameterEstimation/parameter_estimation_Gaussian_noise.html">ðŸ“¥ Parameter estimation I: Gaussian mean and variance</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Posteriors.html">5. Bayesian posteriors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="Exploring_pdfs.html">5.1. ðŸ“¥ Exploring PDFs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="Exploring_pdfs_followups.html">Follow-ups to Exploring PDFs</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="Gaussians.html">5.2. Gaussians: A couple of frequentist connections</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="visualization_of_CLT.html">ðŸ“¥ Visualization of the Central Limit Theorem</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/Interpreting2Dposteriors.html">5.3. Interpreting 2D posteriors</a></li>
<li class="toctree-l2"><a class="reference internal" href="chi_squared_tests.html">5.4. ðŸ“¥ Demonstration: Sum of normal variables squared</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="CoinTossing.html">6. Updating via Bayes' rule</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="CoinTossing/sec-01-coin-tossing-frequentists-and-bayesaians.html">6.1. Coin tossing: Frequentists and Bayesaians</a></li>
<li class="toctree-l2"><a class="reference internal" href="CoinTossing/sec-02-when-do-priors-matter-when-don-t-they-matter.html">6.2. When do priors matter? When donâ€™t they matter?</a></li>
<li class="toctree-l2"><a class="reference internal" href="CoinTossing/sec-03-computing-the-posterior-analytically.html">6.3. Computing the posterior analytically</a></li>
<li class="toctree-l2"><a class="reference internal" href="CoinTossing/sec-04-degree-of-belief-credibility-intervals-vs-frequentist-1-sigm.html">6.4. Degree of belief/credibility intervals vs frequentist 1-sigma intervals</a></li>
<li class="toctree-l2"><a class="reference internal" href="CoinTossing/sec-05-take-aways-and-follow-up-questions-from-coin-flipping.html">6.5. Take-aways and follow-up questions from coin flipping:</a></li>
<li class="toctree-l2"><a class="reference internal" href="demo-BayesianBasics.html">6.6. ðŸ“¥ Demonstration:  Bayesian Coin Tossing</a></li>
<li class="toctree-l2"><a class="reference internal" href="Bayesian_updating_coinflip_interactive.html">6.7. ðŸ“¥ Demonstration: Coin tossing</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="UsingBayes.html">7. Bayes in practice</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="BayesianAdvantages.html">7.1. Advantages of the Bayesian approach</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianWorkflow/BayesianWorkflow.html">7.2. Bayesian research workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianLinearRegression/BayesianLinearRegression_rjf.html">7.3. Bayesian Linear Regression (BLR)</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II: Advanced Bayesian methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../RootAdvancedMethods.html">8. Overview of Part II</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianParameterEstimation/ParameterEstimation.html">9. More Bayesian parameter estimation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/radioactive_lighthouse_exercise.html">9.1. ðŸ“¥ Radioactive lighthouse problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/amplitude_in_presence_of_background.html">9.2. ðŸ“¥ Amplitude of a signal in the presence of background</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/parameter_estimation_fitting_straight_line_I.html">9.3. Parameter estimation example: fitting a straight line</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/parameter_estimation_fitting_straight_line_II.html">9.4. ðŸ“¥ Parameter estimation example: fitting a straight line II</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/dealing_with_outliers.html">9.5. ðŸ“¥ Dealing with outliers</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../AssigningProbabilities/Assigning.html">10. Assigning probabilities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../AssigningProbabilities/IgnorancePDF.html">10.1. Assigning probabilities (I): Indifferences and translation groups</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../AssigningProbabilities/demo-straight_lines.html">Alternative notebook with MCMC sampling</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../AssigningProbabilities/MaxEnt2.html">10.2. Assigning probabilities (II): The principle of maximum entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../AssigningProbabilities/MaxEnt_Function_Reconstruction.html">10.3. ðŸ“¥ Maximum Entropy for reconstructing a function from its moments</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../ComputationalBayes/BayesLinear.html">11. Bayes goes linear: History matching</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Multimodel_inference.html">12. Multi-model inference with Bayes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../ModelSelection/ModelSelection.html">12.1. Model Selection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../ModelSelection/BUQ/Evidence_for_model_EFT_coefficients.html">Evidence calculation for EFT expansions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ModelSelection/BUQ/two_model_evidence.html">Follow-up to EFT evidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ModelSelection/BUQ/computing_evidence.html">Computing the evidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../ModelSelection/BUQ/MCMC-parallel-tempering_ptemcee_vs_zeus.html">Demo: Multimodal distributions with two samplers</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../ModelMixing/model_mixing.html">12.2. Model averaging and mixing </a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/DiscrepancyModels.html">13. Discrepancy Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/MD_balldrop_v1.html">13.4. ðŸ“¥ Ball-drop experiment</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III: MCMC sampling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../StochasticProcesses/RootMCMC.html">14. Overview of Part III</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/StochasticProcesses.html">15. Stochastic processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/Metropolis_Poisson_example.html">15.7. Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/demo-MCMC.html">15.8. Demonstration: Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/Recap_BUQ.html">15.9. Recap of Poisson and more about MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/parameter_estimation_Gaussian_noise-2.html">15.10. Parameter estimation example: Gaussian noise and averages II</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/MCMC-random-walk-and-sampling.html">15.11. Exercise: Random walk</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/MCMC_overview.html">16. Overview of Markov Chain Monte Carlo</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/MarkovChains.html">16.1. Markov chains</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/MCMC.html">16.2. Markov chain Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/MCMC_intro_BUQ.html">16.3. Alternative MCMC introduction (Gregory)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/Assignment_extending_radioactive_lighthouse.html">16.4. Assignment: 2D radioactive lighthouse location using MCMC</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/Advanced_MCMC.html">17. Advanced MCMC</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ComputationalBayes/AdvancedMCMC.html">17.1. Advanced Markov chain Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/MCMC-diagnostics.html">17.2. Overview: MCMC Diagnostics</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/intuition_sampling.html">17.4. Intuition on sampling and best practices</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/Other_samplers.html">18. HMC and other samplers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../StochasticProcesses/BUQ2/HMC_intro_BUQ.html">18.1. Hamiltonian Monte Carlo (HMC) overview and visualization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../StochasticProcesses/BUQ2/Liouville_theorem_visualization.html">Liouville Theorem Visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../StochasticProcesses/BUQ2/Orbital_eqs_with_different_algorithms.html">Solving orbital equations with different algorithms</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/zeus.html">18.2. The Zeus Ensemble Slice Sampler</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../StochasticProcesses/BUQ2/PyMC_intro_updated.html">18.3. PyMC Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../StochasticProcesses/OverviewIntroPyMC.html">Overview of Intro to PyMC notebook</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ2/parameter_estimation_Gaussian_noise_compare_samplers.html">18.4. Comparing samplers for a simple problem</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV: Machine learning: A Bayesian perspective</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../MachineLearning/RootML.html">19. Overview of Part IV</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/GP/RootGP.html">20. Overview of Gaussian processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../MachineLearning/GP/GaussianProcesses.html">20.4. Introduction to Gaussian processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/CF/demo-GaussianProcesses.html">ðŸ“¥ demo-GaussianProcesses notebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/lecture_20.html">GP recap; GP applications; (old lecture 20)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../MachineLearning/GP/Sklearn_demos.html">20.5. scikit-learn demo notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/plot_gpr_noisy_targets.html">ðŸ“¥ One-dimension regression example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/plot_gpr_prior_posterior.html">ðŸ“¥ Prior and posterior with different kernels</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../MachineLearning/GP/GPy_demos.html">20.6. GPy demo notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/demo-GaussianProcesses.html">Gaussian processes demonstration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/CF/exercise_GP_GPy.html">Exercise: Gaussian processes using <code class="docutils literal notranslate"><span class="pre">GPy</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/Gaussian_processes_exercises.html">Exercise: Gaussian Process models with GPy</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/LogReg/LogReg.html">21. Logistic Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/MachineLearningExamples.html">21.5. Machine Learning: First Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/NeuralNet/exercises_LogReg_NeuralNet.html">21.6. Exercise: Logistic Regression and neural networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/ANN/MachineLearning.html">22. Machine learning: Overview and notation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/NeuralNet.html">22.5. Artifical neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/NeuralNet/demo-NeuralNet.html">22.6. Demonstration: Neural network classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/Neural_Network_for_simple_function_in_PyTorch.html">22.7. ðŸ“¥ ANN from ChatGPT using PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/ModelValidation.html">22.8. Model validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/DataBiasFairness.html">22.9. Data bias and fairness in machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/NeuralNet/NeuralNetBackProp.html">22.10. *Neural networks: Backpropagation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/ANNFT.html">23. ANNs in the large-width limit (ANNFT)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/random_initialized_ANN_vs_width.html">23.3. ðŸ“¥ Distributions of Randomly-Initialized ANNs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/BNN/bnn.html">24. Bayesian neural nets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/BNN/demo-bnn.html">24.4. Demonstration: Variational Inference and Bayesian Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/BNN/exercises_BNN.html">24.5. Exercise: Bayesian neural networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/CNN/cnn.html">25. *Convolutional neural nets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/CNN/demo-cnn.html">25.6. Demonstration: Image recognition with Convolutional Neural Networks</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V: Other topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../OtherTopics/RootOtherTopics.html">26. Overview of Part V </a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/Emulators.html">27. Emulators</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ComputationalBayes/BayesFast.html">27.1. Bayes goes fast: Emulators (from CF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ComputationalBayes/extra_RBM_emulators.html">27.2. RBM emulators (BUQ)</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../OtherTopics/Student_t_distribution_from_Gaussians.html">28. ðŸ“¥ Student t distribution from Gaussians</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/SVD.html">29. PCA, SVD, and all that</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/linear_algebra_games_including_SVD.html">29.5. ðŸ“¥ demo-SVD notebook</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../OtherTopics/qbism.html">30. QBism: Bayesian quantum mechanics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Backmatter</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Backmatter/bibliography.html">31. Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Backmatter/JB_tests.html">32. Guide to Jupyter Book markdown</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix A: Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Reference/Statistics.html">33. Statistics concepts and notation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/GradientDescent.html">34. Gradient-descent optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix B: Scientific modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/RootScientificModeling.html">35. Overview of scientific modeling material</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling.html">36. Overview of modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/LinearModels.html">37. Linear models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/MathematicalOptimization.html">38. Mathematical optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix C: Getting started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Setup/RootGettingStarted.html">39. Overview of Getting started material</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Setup/exercise_Intro_01_Jupyter_Python.html">40. ðŸ“¥ Exercise: Jupyter notebooks and Python</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Setup/more_python_and_jupyter.html">41. More about Python and Jupyter notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/exercise_Intro_02_Jupyter_Python.html">41.4. ðŸ“¥ Python lists and iterations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/exercise_Intro_03_Numpy.html">41.5. ðŸ“¥ Linear algebra operations with NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/demo-Intro.html">41.6. ðŸ“¥ Reading data and fitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/Simple_widgets_v1.html">41.7. ðŸ“¥ Making a simple widget-based UI</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Setup/setting_up.html">42. Setting up for using this Jupyter Book</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/installing_anaconda.html">42.1. Using Anaconda</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/using_github.html">42.2. Using GitHub</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">TALENT mini-projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/RootMiniProjects.html">Overview of mini-projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/mini-project_I_toy_model_of_EFT.html">ðŸ“¥ MP I: Parameter estimation for a toy model of an EFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/model-selection_mini-project-IIa.html">ðŸ“¥ MP IIa: Model selection basics</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Mini-projects/model-selection_mini-project-IIb_How_many_lines_ptemcee.html">ðŸ“¥ MP IIb: How many lines?</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Mini-projects/Mini-project_IIb_overview.html">Overview of Mini-project IIb: How many lines?</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/mini-project_IIIa_bayesian_optimization.html">ðŸ“¥ MP IIIa: Bayesian optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/mini-project_IIIb_Bayesian_neural_networks_from_demo.html">ðŸ“¥ MP IIIb: Bayesian Neural Networks</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/NuclearTalent/LFD_for_Physicists" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/NuclearTalent/LFD_for_Physicists/issues/new?title=Issue%20on%20page%20%2FLearningFromData-content/BayesianStatistics/BayesianBasics/Inferenceandpdfs.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/LearningFromData-content/BayesianStatistics/BayesianBasics/Inferenceandpdfs.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Inference and PDFs</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statements">4.1. Statements</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#manipulating-probabilities-bayesian-rules-of-probability-as-principles-of-logic">4.2. Manipulating probabilities: Bayesian rules of probability as principles of logic</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sum-rule">Sum rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#product-rule">Product rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem">Bayesâ€™ theorem</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-density-functions">4.3. Probability density functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-pdfs-marginal-pdfs-and-an-example-of-marginalization">Joint PDFs, marginal PDFs, and an example of marginalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-pdfs">Visualizing PDFs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">4.4. Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="inference-and-pdfs">
<span id="ch-inferenceandpdfs"></span><h1><span class="section-number">4. </span>Inference and PDFs<a class="headerlink" href="#inference-and-pdfs" title="Link to this heading">#</a></h1>
<section id="statements">
<h2><span class="section-number">4.1. </span>Statements<a class="headerlink" href="#statements" title="Link to this heading">#</a></h2>
<p>A Bayesian approach to the world assigns probabilities to truth
claims. It also recognizes that the probability of claim <span class="math notranslate nohighlight">\(A\)</span> is
contingent upon statement <span class="math notranslate nohighlight">\(B\)</span> being true. We therefore introduce
notation for conditional probability:</p>
<div class="math notranslate nohighlight">
\[ 
   \cprob{A}{B} \equiv \text{``probability of $A$ given $B$ is true''}.
\]</div>
<p>For a Bayesian <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> could stand for anything. But, whatever
they are, statements can now be categorized as definitively true
(probability 1), or definitively false (probability 0), or anything in
between.</p>
<p>Examples:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\cprob{\text{``Betty eats grass''}}{\text{``Betty is a cow'' and
  ``All cows eat grass''}}=1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\cprob{\text{``Lines A and B intersect''}}{\text{``A and B are
  parallel lines in flat space''}}=0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\cprob{\text{``Particle A is negatively charged''}}{\text{``A is an electron''}}=1\)</span></p></li>
</ul>
<p>On the other hand something like <span class="math notranslate nohighlight">\(\cprob{\text{``below 0 deg,. C''}}{\text{``it is January in Ohio''}}\)</span> should be assigned a probability
between 0 and 1. We might even estimate this probability based on our
past experience of Januarys in Ohio. But even statements for which we
cannot construct a <span class="math notranslate nohighlight">\(\text{``frequentist estimator''}\)</span> can be assigned
probabilities, e.g.</p>
<div class="math notranslate nohighlight">
\[\cprob{\text{``String theory is true''}}{\text{``Data
from all high-energy colliders and cosmology''}},\]</div>
<p>or, less frivolously</p>
<div class="math notranslate nohighlight">
\[\cprob{\text{``I'll come to the party tonight''}}{\text{``You
invited me''}}.\]</div>
<p>This emphasizes that, for a Bayesian, the probability is interpreted
as representative of the Bayesianâ€™s current state of knowledge, and
not necessarily as the long-term average of a set of
many trials. After all, what would it mean to conduct a set of trials,
in some of which string theory was true, and in some of which it was not?</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>It is easy to forget the meaning of the key word <em>given</em> and assume that <span class="math notranslate nohighlight">\(\cprob{A}{B}\)</span> means that we know <span class="math notranslate nohighlight">\(B\)</span> is actually true, which may or may not be the case.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Further discussion on the interpretation of probability can be found in <a class="reference internal" href="Bayesian_epistemology.html#sec-bayesianepistemology"><span class="std std-ref">*Aside: Bayesian epistemology</span></a>.</p>
</div>
</section>
<section id="manipulating-probabilities-bayesian-rules-of-probability-as-principles-of-logic">
<span id="sec-manipulatingpdfs"></span><h2><span class="section-number">4.2. </span>Manipulating probabilities: Bayesian rules of probability as principles of logic<a class="headerlink" href="#manipulating-probabilities-bayesian-rules-of-probability-as-principles-of-logic" title="Link to this heading">#</a></h2>
<p>In the school of subjective probability you can assign whatever
probability we want to the above statements. They represent, after
all, your degree of belief. But you should beware! If your probability
assignments are consistently capricious people may stop paying
attention to them.</p>
<p>A minimal set of rules for consistently keeping track of probabilities
is provided by two basic rules of probability arithmetic. These are the sum
and product rules. A proof that the Sum and Product rules follow in any consistent implementation of probabilistic reasoning is given by Cox <span id="id1">[<a class="reference internal" href="../../Backmatter/bibliography.html#id3" title="Richard Threlkeld Cox. The Algebra of Probable Inference. Johns Hopkins University Press, 1961.">Cox61</a>]</span>.</p>
<section id="sum-rule">
<h3>Sum rule<a class="headerlink" href="#sum-rule" title="Link to this heading">#</a></h3>
<p>If the set <span class="math notranslate nohighlight">\(\{x_i\}\)</span> is <em>exhaustive</em> and <em>exclusive</em></p>
<div class="math notranslate nohighlight" id="equation-eq-discrete-sum-rule">
<span class="eqno">(4.1)<a class="headerlink" href="#equation-eq-discrete-sum-rule" title="Link to this equation">#</a></span>\[
   \sum_i \cprob{x_i}{I} = 1,
\]</div>
<p>i.e., the sum of the probabilities of all possible outcomes is equal
to one. In quantum mechanics we are used to this as the outcome of
summing over a complete, orthonormal set of states, and indeed, in
that case the basis includes all possible values (itâ€™s complete), and
there is no overlap between its members (they are orthogonal).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <a class="reference internal" href="#equation-eq-discrete-sum-rule">(4.1)</a> we include <span class="math notranslate nohighlight">\(I\)</span> (for â€œInformationâ€) generically as the quantities or statements that the probability of <span class="math notranslate nohighlight">\(x_i\)</span> is contingent on. We use <span class="math notranslate nohighlight">\(I\)</span> to avoid having to specify explicitly all the details, but we should remember that these probabilities (and probability densities introduced below) are always conditional on some information.</p>
</div>
<p>The sum rule implies a key tool in the Bayesianâ€™s arsenal,
<em>marginalization</em></p>
<div class="amsmath math notranslate nohighlight" id="equation-33d8f88c-fe92-4ce9-8c01-6b102f75710c">
<span class="eqno">(4.2)<a class="headerlink" href="#equation-33d8f88c-fe92-4ce9-8c01-6b102f75710c" title="Permalink to this equation">#</a></span>\[\begin{align}
      \cprob{x}{I} = \sum_j \cprob{x, y_j}{I} 
\end{align}\]</div>
<p>with the second equality being the generalization of marginalization
to the context of variables <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> that can take on a
continuous set of values, rather
than just a discrete set of outcomes <span class="math notranslate nohighlight">\(\{x_i\}\)</span> and <span class="math notranslate nohighlight">\(\{y_j\}\)</span>.</p>
<p>We will use marginalization a lot! Note that the marginalization
takes place in the presence of the conditional I, i.e., all
probabilities involved are ``given the information Iâ€™â€™. The given
information is held fixed, while the sum of all possibilities is
constructed.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Although we alluded to the analogy between inserting a complete set
of states and marginalization above this analogy breaks down in
general. Itâ€™s ok to use this as a mnemonic though.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A rule from probability says <span class="math notranslate nohighlight">\(\prob(A \cup B) = \prob(A) + \prob(B) - \prob(A \cap B)\)</span>. (That is, to calculate the probability of the union of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> we need to subtract the probability of the intersection from the sum of probabilities.) This may seem to contradict our marginalization rule. The difference is that if <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are <em>exclusive</em>, as we assume, then <span class="math notranslate nohighlight">\(\prob(A \cap B) = 0\)</span>.</p>
</div>
</section>
<section id="product-rule">
<h3>Product rule<a class="headerlink" href="#product-rule" title="Link to this heading">#</a></h3>
<p>The product rule tells us how to expand the joint probability of <span class="math notranslate nohighlight">\(x\)</span>
and <span class="math notranslate nohighlight">\(y\)</span>, i.e.,</p>
<div class="math notranslate nohighlight" id="equation-eq-joint-prob">
<span class="eqno">(4.3)<a class="headerlink" href="#equation-eq-joint-prob" title="Link to this equation">#</a></span>\[  
\cprob{x,y}{I} = \cprob{x}{y, I} \cprob{y}{I} 
\]</div>
<p>In words we say that the probability of both <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> occurring is
the probability that <span class="math notranslate nohighlight">\(x\)</span> occurs, given that <span class="math notranslate nohighlight">\(y\)</span> has occurred, times
the probability that <span class="math notranslate nohighlight">\(y\)</span> occurs.</p>
<p>Note, once again, that the given information I is held fixed, i.e., it
if present on the left-hand side it appears in both probabilities on
the right-hand side.</p>
<p>If <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are <em>mutually independent</em>, then <span class="math notranslate nohighlight">\(\cprob{x}{y,I} = \cprob{x}{I}\)</span> and <a class="reference internal" href="#equation-eq-joint-prob">(4.3)</a> reduces to</p>
<div class="math notranslate nohighlight" id="equation-eq-conditional-independence">
<span class="eqno">(4.4)<a class="headerlink" href="#equation-eq-conditional-independence" title="Link to this equation">#</a></span>\[
\cprob{x,y}{I} = \cprob{x}{I} \times \cprob{y}{I}
\]</div>
<p>This is a rule you are probably (but we hesitate to quantify our
belief) familiar with. Crucially, <a class="reference internal" href="#equation-eq-joint-prob">(4.3)</a> does <em>not</em>
rely on the independence of the events <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When considering whether <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are mutually independent, you can ask yourself: does knowing <span class="math notranslate nohighlight">\(y\)</span> is true give me any information about whether <span class="math notranslate nohighlight">\(x\)</span> is true? If yes, then we need to keep it on the right side of the bar in <span class="math notranslate nohighlight">\(\cprob{x}{y,I}\)</span>. If no, then it doesnâ€™t change the probability of <span class="math notranslate nohighlight">\(x\)</span> whether it is there or not, so the probability of <span class="math notranslate nohighlight">\(x\)</span> being true is the same if <span class="math notranslate nohighlight">\(y\)</span> is omitted.
Hence <span class="math notranslate nohighlight">\(\cprob{x}{y,I} = \cprob{x}{I}\)</span> follows.</p>
</div>
</section>
<section id="bayes-theorem">
<h3>Bayesâ€™ theorem<a class="headerlink" href="#bayes-theorem" title="Link to this heading">#</a></h3>
<p>It is just a short step from the product rule to Bayesâ€™
theorem. Although we wrote <a class="reference internal" href="#equation-eq-joint-prob">(4.3)</a> so that the
<span class="math notranslate nohighlight">\(\cprob{x}{y,I}\)</span> appeared on the right-hand side there is no reason to
privilege <span class="math notranslate nohighlight">\(x\)</span> over <span class="math notranslate nohighlight">\(y\)</span>. We could equally have written:</p>
<div class="math notranslate nohighlight">
\[  
\cprob{x,y}{I} = \cprob{y}{x, I} \cprob{x}{I} 
\]</div>
<p>Equating this to the expression in <a class="reference internal" href="#equation-eq-joint-prob">(4.3)</a> yields <strong>Bayesâ€™ Rule</strong> (or Theorem):</p>
<div class="amsmath math notranslate nohighlight" id="equation-ee873ae2-cb07-474a-99cb-bfc32c24eed5">
<span class="eqno">(4.5)<a class="headerlink" href="#equation-ee873ae2-cb07-474a-99cb-bfc32c24eed5" title="Permalink to this equation">#</a></span>\[\begin{equation}
\cprob{x}{y,I} = \frac{\cprob{y}{x,I} \cprob{x}{I}}{\cprob{y}{I}}
\end{equation}\]</div>
<p>Bayesâ€™ theorem tells us how to reverse the conditional: <span class="math notranslate nohighlight">\(\cprob{x}{y}
\Rightarrow \cprob{y}{x}\)</span>. The first thing to realize is that these two
probabilities are not the same thing.</p>
<div class="my-checkpoint admonition">
<p class="admonition-title">Checkpoint question</p>
<p>Construct your own example of <span class="math notranslate nohighlight">\(\cprob{x}{y} \neq \cprob{y}{x}\)</span></p>
<div class="dropdown my-hint admonition">
<p class="admonition-title">Possible answers</p>
<p>The probability that there is a cloud in the sky given that it is
raining is not the same as the probability that itâ€™s raining given
that there is a cloud in the sky.</p>
</div>
</div>
<div class="exercise admonition" id="exercise:Inferenceandpdfs:sumandproductrule">

<p class="admonition-title"><span class="caption-number">Exercise 4.1 </span> (Practicing the sum and product rule with population characteristics)</p>
<section id="exercise-content">
<p>In the exercise <a class="reference internal" href="exercise_sum_product_rule.html#exercise-checkingsumproduct"><span class="std std-ref">Exercise: Checking the sum and product rules</span></a>
you can go through some calculations based on the sum and product
rule. You estimate the probabilities of finding an individual in a
population with a particular characteristic (tall with brown eyes for
example) based on what youâ€™re told about the population and the usual
frequentist interpretation of probability. Then the exercise will take
you through applying the sum and product rules.</p>
</section>
</div>
<div class="exercise admonition" id="exercise:Inferenceandpdfs:medicalexample">

<p class="admonition-title"><span class="caption-number">Exercise 4.2 </span> (Using Bayesian rules of probability on a standard medical problem)</p>
<section id="exercise-content">
<p>In the exercise <a class="reference internal" href="exercise_medical_example_by_Bayes.html#exercise-medicalexample"><span class="std std-ref">Exercise: Standard medical example using Bayes</span></a> your goal is to find the probability that you actually have an unknown disease given some information about the test for it. This is a problem for which your intuition and personal probability reasoning logic is likely to fail. But Bayes leads the way to the correct answer! It is good practice in translating statements to probabilities (and distinguishing between joint and conditional probabilities).</p>
</section>
</div>
</section>
</section>
<section id="probability-density-functions">
<h2><span class="section-number">4.3. </span>Probability density functions<a class="headerlink" href="#probability-density-functions" title="Link to this heading">#</a></h2>
<p>The key mathematical entity used over and over again throughout this book will be the â€œprobability density functionâ€, PDF for short. PDFs for continuous quantities are integrated to obtain probabilities. An example familiar to every physicist will be the probability density of a quantum-mechanical particle. Max Born somewhat belatedly decided to put a square on Schrodingerâ€™s wave function when interpreting the integral from <span class="math notranslate nohighlight">\(x=a\)</span> to <span class="math notranslate nohighlight">\(x=b\)</span> as the probability of measuring the particle to be between <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="math notranslate nohighlight">
\[
   \prob(a \leq x \leq b) = \int_a^b |\psi(x)|^2\, dx
\]</div>
<p>We note that the unit of <span class="math notranslate nohighlight">\(|\psi(x)|^2\)</span> is inverse length, and it is generally true that PDFs, unlike probabilities, have units. If we divorce the previous equation from the quantum-mechanical context we would write:</p>
<div class="math notranslate nohighlight">
\[
   \prob(a \leq x \leq b) = \int_a^b \p{x}\, dx
\]</div>
<p>where <span class="math notranslate nohighlight">\(\p{x}\)</span> is the PDF for the continuous variable <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="my-checkpoint admonition">
<p class="admonition-title">Checkpoint question</p>
<p>What is the unit of <span class="math notranslate nohighlight">\(p(\xvec)\)</span> (or <span class="math notranslate nohighlight">\(|\psi(\xvec)|^2\)</span>) in three dimensions (assuming <span class="math notranslate nohighlight">\(\xvec\)</span> is a vector of length)?</p>
<div class="dropdown my-answer admonition">
<p class="admonition-title">Answer</p>
<p>We can use the normalization equation (which is the sum rule):</p>
<div class="math notranslate nohighlight">
\[
  \int d^3x\, p(\xvec) = 1 ,
\]</div>
<p>which is dimensionless on the right side, so the unit of <span class="math notranslate nohighlight">\(p(\xvec)\)</span> (or <span class="math notranslate nohighlight">\(|\psi(\xvec)|^2\)</span>) is the inverse unit of <span class="math notranslate nohighlight">\(d^3x\)</span>, or <span class="math notranslate nohighlight">\(1/\text{length}^3\)</span>. Note that if <span class="math notranslate nohighlight">\(\xvec\)</span> represented a different quantity, the unit of <span class="math notranslate nohighlight">\(p(\xvec)\)</span> would differ accordingly.</p>
</div>
</div>
<p>We note that the case of a discrete random variable, e.g., the numbers
that can be rolled on a die, mean that the PDF only is non-zero at a
discrete set of numbers. In that case <span class="math notranslate nohighlight">\(\p{x}\)</span> can be represented as
a sum of Dirac delta functions, and the resulting object is sometimes
called a probability mass function. Thus the cases discussed above,
in which <span class="math notranslate nohighlight">\(\prob\)</span> represented probability measured over a discrete set
of outcomes, can be understood as special cases of pdfs.</p>
<p>In fact, working this from the other end,  the sum and product rules
to pdfs in just the same way as they do to probabilities. Therefore Bayesâ€™ theorem (or
rule) can also be applied to relate the pdf of x given y, <span class="math notranslate nohighlight">\(\pdf{x}{y}\)</span>
to the pdf of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(\pdf{y}{x}\)</span>.</p>
<p>In Bayesian statistics there are PDFs (or PMFs if discrete) for
everything. Here are a few examples:</p>
<ul class="simple">
<li><p>fit parameters â€” like the slope and intercept of a line in a
straight-line fit</p></li>
<li><p>experimental <em>and</em> theoretical imperfections that lead to
uncertainties in the final result</p></li>
<li><p>events (â€œWill it rain tomorrow?â€)</p></li>
</ul>
<p>We will stick to the <span class="math notranslate nohighlight">\(\p{\cdot}\)</span> notation here, but itâ€™s worth pointing out that many different variants of the letter <span class="math notranslate nohighlight">\(p\)</span> are used to represent probabilities and PDFs in the literature. If we are interested in the PDF in a higher-dimensional space (say the probability of finding a particle in 3D space) you might see <span class="math notranslate nohighlight">\(p(\vec x) = p(\mathbf{x}) = P(\vec x) = \text{pr}(\vec x) = \text{prob}(\vec x) = \ldots\)</span>.</p>
<div class="my-checkpoint admonition">
<p class="admonition-title">Checkpoint question</p>
<p>What is the PDF <span class="math notranslate nohighlight">\(\p{x}\)</span> if we know <strong>definitely</strong> that <span class="math notranslate nohighlight">\(x = x_0\)</span> (i.e., fixed)?</p>
<div class="dropdown my-answer admonition">
<p class="admonition-title">Answer</p>
<p><span class="math notranslate nohighlight">\(\p{x} = \delta(x-x_0)\quad\)</span>  [Note that <span class="math notranslate nohighlight">\(p(x)\)</span> is normalized.]</p>
</div>
</div>
<section id="joint-pdfs-marginal-pdfs-and-an-example-of-marginalization">
<h3>Joint PDFs, marginal PDFs, and an example of marginalization<a class="headerlink" href="#joint-pdfs-marginal-pdfs-and-an-example-of-marginalization" title="Link to this heading">#</a></h3>
<p><span class="math notranslate nohighlight">\(\p{x_1, x_2}\)</span> is the <em>joint</em> probability density of <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>. In quantum mechanics the probability density <span class="math notranslate nohighlight">\(|\Psi(x_1, x_2)|^2\)</span> to find particle 1 at <span class="math notranslate nohighlight">\(x_1\)</span> and particle 2 at <span class="math notranslate nohighlight">\(x_2\)</span> is a joint probability density.</p>
<div class="my-checkpoint admonition">
<p class="admonition-title">Checkpoint question</p>
<p>What is the probability to find particle 1 at <span class="math notranslate nohighlight">\(x_1\)</span> while particle 2 is <em>anywhere</em>?</p>
<div class="dropdown my-answer admonition">
<p class="admonition-title">Answer</p>
<p><span class="math notranslate nohighlight">\(\int_{-\infty}^{+\infty} |\psi(x_1, x_2)|^2\, dx_2\ \ \)</span> or, more generally, integrated over the domain of <span class="math notranslate nohighlight">\(x_2\)</span>.</p>
</div>
</div>
<p>This is a specific example of the marginalization rule, now in the pdf
context, that we
discussed the general form for probabilities above. The <em>marginal
probability density</em> of <span class="math notranslate nohighlight">\(x_1\)</span> is the result when we marginalize the
<em>joint probability distribution</em> over <span class="math notranslate nohighlight">\(x_2\)</span>:</p>
<div class="math notranslate nohighlight">
\[
 \p{x_1} = \int \p{x_1, x_2} \,dx_2 .
\]</div>
<p>So, in our quantum mechanical example, itâ€™s the probability
density we get when we just focus on particle 1, and donâ€™t care about
particle 2. <em>Marginalizing</em> in the Bayesian context means
``integrating outâ€™â€™ a parameter one isâ€“at least temporarilyâ€“not
interested in, to leave the focus on the PDF of other parameters. This
can be particularly useful if there are â€œnuisanceâ€ parameters in the
statistical model that account for the impact of defects in the
measuring apparatus, but ultimately one is interested in the physics
extracted with the imperfect apparatus.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You may have noticed that we wrote <span class="math notranslate nohighlight">\(\p{x_1}\)</span> and <span class="math notranslate nohighlight">\(\p{x_1, x_2}\)</span> rather than <span class="math notranslate nohighlight">\(\pdf{x_1}{I}\)</span> and <span class="math notranslate nohighlight">\(\pdf{x1,x2}{I}\)</span>, where â€œ<span class="math notranslate nohighlight">\(I\)</span>â€ is information we know but do not specify explicitly. Our PDFs will always be contingent on <em>some</em> information, so we were really being sloppy by trying to be compact. (See <a class="reference internal" href="MoreBayesTheorem.html#sec-continuum-limit"><span class="std std-ref">The continuum limit</span></a> for more careful versions of these equations.)</p>
</div>
</section>
<section id="visualizing-pdfs">
<h3>Visualizing PDFs<a class="headerlink" href="#visualizing-pdfs" title="Link to this heading">#</a></h3>
<p>It is worthwhile at this stage to jump ahead and work through parts of the Jupyter notebook
<a class="reference internal" href="Exploring_pdfs.html"><span class="std std-doc">Exploring PDFs</span></a> to make a first pass at getting familiar with PDFs and how to visualize them. In the notebook you will
work with the python package <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code>, which has many
distributions built in. You can learn more about those distributions by
reading the manual page (which you can find by googling).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference internal" href="../../Reference/Statistics.html#sec-statistics"><span class="std std-ref">Statistics concepts and notation</span></a> in Appendix A for further details on PDFs.</p>
</div>
<p>The diversity of distributions available there should make it clear
that the â€œdefaultâ€ choice of a Gaussian distribution is just that: a
default choice. In <a class="reference internal" href="Gaussians.html#sec-gaussians"><span class="std std-ref">Gaussians: A couple of frequentist connections</span></a> we will explore why this default
choice is often the correct one. But often is not the same as all the
time, and it is frequently the case that other distributions, such as
the Student t, gives a better description of the way data is
distributed.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Trivia: â€œStudentâ€ was the pen name of the Head Brewer at Guinness â€” a pioneer of small-sample experimental design (hence not necessarily Gaussian). His real name was William Sealy Gossett.</p>
</div>
<p>Because we can draw an arbitrary number of samples from the
distributions defined in scipy.stats we can see how the distributions
build up as the number of samples increases, and how there are
fluctuations around the asymptotic distribution that are larger for a
small number of samples.</p>
</section>
</section>
<section id="summary">
<h2><span class="section-number">4.4. </span>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p>We have put on the table the axioms of probability theory and some of their consequences, in particular Bayesâ€™ theorem.
Before looking further at concrete applications of Bayesian inference, we provide further insight into Bayesâ€™ theorem in <a class="reference internal" href="MoreBayesTheorem.html#sec-morebayestheorem"><span class="std std-ref">More on Bayesâ€™ theorem</span></a> and introduce some additional ingredients for Bayesian inference in <a class="reference internal" href="DataModelsPredictions.html#sec-datamodelspredictions"><span class="std std-ref">Data, models, and predictions</span></a>. The latter include the idea of a statistical model, how to predict future data conditioned on (i.e., given) past data and background information (the posterior predictive distribution), and Bayesian parameter estimation.</p>
<p>In Appendix A there is a summary and further details on <a class="reference internal" href="../../Reference/Statistics.html#sec-statistics"><span class="std std-ref">Statistics concepts and notation</span></a>. Particularly important are <a class="reference internal" href="../../Reference/Statistics.html#sec-expectationvaluesandmoments"><span class="std std-ref">Expectation values and moments</span></a> and <a class="reference internal" href="../../Reference/Statistics.html#sec-centralmoments"><span class="std std-ref">Central moments: Variance and Covariance</span></a>; we summarize the key discrete and continuous definitions here.</p>
<div class="admonition-brief-summary-of-expectation-values-and-moments admonition">
<p class="admonition-title">Brief summary of expectation values and moments</p>
<p>The <em>expectation value</em> of a function <span class="math notranslate nohighlight">\(h\)</span> of the random variable <span class="math notranslate nohighlight">\(X\)</span> with respect to its distribution <span class="math notranslate nohighlight">\(p(x_i)\)</span> (a PMF) or <span class="math notranslate nohighlight">\(p(x)\)</span> (a PDF) is</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}_{p}[h] =  \sum_{i}\! h(x_i)p(x_i) \quad\Longrightarrow\quad
  \mathbb{E}_p[h] = \int_{-\infty}^\infty \! h(x)p(x)\,dx .
\]</div>
<p>The <span class="math notranslate nohighlight">\(p\)</span> subscript is usually omitted. <em>Moments</em> correspond to <span class="math notranslate nohighlight">\(h(x) = x^n\)</span>, with <span class="math notranslate nohighlight">\(n=0\)</span> giving 1 (this is the normalization condition) and the mean <span class="math notranslate nohighlight">\(\mu\)</span> by <span class="math notranslate nohighlight">\(n=1\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[X] \equiv \mu =  \sum_{i}\! x_ip(x_i) \quad\Longrightarrow\quad
  \mathbb{E}[X] \equiv \mu = \int_{-\infty}^\infty \! xp(x)\,dx .
\]</div>
<p>The variance and covariance are moments with respect to the mean for one and two random variables (we give only the continuous version here):</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\text{Var}(X) &amp;\equiv \sigma^2  \equiv \mathbb{E}\left[ \left( X - \mathbb{E}[X] \right)^2 \right] \\
\text{Cov}(X,Y) &amp;\equiv \sigma_{XY}^2 \equiv \mathbb{E}\left[ \left( X - \mathbb{E}[X] \right) \left( Y - \mathbb{E}[Y] \right)  \right].
\end{align}\end{split}\]</div>
<p>The standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> is simply the square root of the variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>.
The <strong>correlation coefficient</strong> of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> (for non-zero variances) is</p>
<div class="math notranslate nohighlight">
\[
\rho_{XY} \equiv \frac{\text{Cov}(X,Y)}{\sqrt{\text{Var}(X)\text{Var}(Y)}},
\]</div>
</div>
<div class="my-checkpoint admonition">
<p class="admonition-title">Checkpoint question</p>
<p>Show that we can also write</p>
<div class="math notranslate nohighlight">
\[
\sigma^2 = \mathbb{E}[X^2]  - \mathbb{E}[X]^2
\]</div>
<div class="dropdown my-answer admonition">
<p class="admonition-title">Answer</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\sigma^2 &amp;= \int_{-\infty}^\infty (x-\mathbb{E}[X] )^2 p(x)dx\\
&amp;=  \int_{-\infty}^\infty \left(x^2 - 2 x \mathbb{E}[X] +\mathbb{E}[X]^2\right)p(x)dx \\
&amp; =  \mathbb{E}[X^2]  - 2 \mathbb{E}[X] \mathbb{E}[X]  + \mathbb{E}[X]^2 \\
&amp;=  \mathbb{E}[X^2]  - \mathbb{E}[X]^2
\end{align}\end{split}\]</div>
<p>Make sure you can justify each step.</p>
</div>
</div>
<div class="my-checkpoint admonition">
<p class="admonition-title">Checkpoint question</p>
<p>Show that the mean and variance of the normalized Gaussian distribution</p>
<div class="math notranslate nohighlight">
\[
p \longrightarrow \mathcal{N}(x | \mu,\sigma^2) = \frac{1}{\sigma\sqrt{2\pi}} \exp{\Bigl(-\frac{(x-\mu)^2}{2\sigma^2}\Bigr)},
\]</div>
<p>are <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>, respectively.</p>
<div class="dropdown my-answer admonition">
<p class="admonition-title">Answer</p>
<p>Just do the integrals!</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
  \mu &amp;= \int_{-\infty}^\infty \! x \frac{1}{\sigma\sqrt{2\pi}} \exp{\Bigl(-\frac{(x-\mu)^2}{2\sigma^2}\Bigr)}\,dx = \mu \quad\checkmark \\
  \sigma^2 &amp;= \int_{-\infty}^\infty (x-\mu )^2 
  \frac{1}{\sigma\sqrt{2\pi}} \exp{\Bigl(-\frac{(x-\mu)^2}{2\sigma^2}\Bigr)} \,dx
  = \sigma^2 \quad\checkmark
\end{align}\end{split}\]</div>
<p>In doing these integrals, simplify by changing the integration variable to <span class="math notranslate nohighlight">\(x' = x-\mu\)</span> and use that the distribution is normalized (integrates to one) and that integrals of odd integrands are zero.</p>
</div>
</div>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./LearningFromData-content/BayesianStatistics/BayesianBasics"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="RootBayesianBasics.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Bayesian methods for scientific modeling</p>
      </div>
    </a>
    <a class="right-next"
       href="exercise_sum_product_rule.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4.5. </span>Exercise: Checking the sum and product rules</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statements">4.1. Statements</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#manipulating-probabilities-bayesian-rules-of-probability-as-principles-of-logic">4.2. Manipulating probabilities: Bayesian rules of probability as principles of logic</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sum-rule">Sum rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#product-rule">Product rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem">Bayesâ€™ theorem</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-density-functions">4.3. Probability density functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-pdfs-marginal-pdfs-and-an-example-of-marginalization">Joint PDFs, marginal PDFs, and an example of marginalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-pdfs">Visualizing PDFs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">4.4. Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christian ForssÃ©n, Dick Furnstahl, and Daniel Phillips
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>