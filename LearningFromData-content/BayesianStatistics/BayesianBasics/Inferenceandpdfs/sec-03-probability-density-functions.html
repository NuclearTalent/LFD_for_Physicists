
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4.3. Probability density functions &#8212; Combined Learning from Data materials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/coloredpages.css?v=0a037ad7" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/myadmonitions.css?v=89ac28d1" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"loader": {"load": ["[tex]/textmacros"]}, "chtml": {"mtextInheritFont": true}, "tex": {"packages": {"[+]": ["textmacros"]}, "macros": {"data": "\\mathcal{D}", "pars": "\\boldsymbol{\\theta}", "para": "\\theta", "optpars": "\\pars^*", "optpara": "\\para^*", "prob": "\\mathbb{P}", "cprob": ["\\prob\\left( #1 \\, \\left\\vert \\, #2 \\right. \\right)", 2], "cprobsub": ["\\prob_{#1}\\left( #2 \\, \\left\\vert \\, #3 \\right. \\right)", 3], "pdf": ["p \\left( #1 \\, \\left\\vert \\, #2 \\right. \\right)", 2], "pdfsub": ["p_{#1} \\left( #2 \\, \\left\\vert \\, #3 \\right. \\right)", 3], "p": ["p \\left( #1 \\right)", 1], "psub": ["p_{#1} \\left( #2 \\right)", 2], "futuredata": "\\mathcal{F}", "expect": ["\\mathbb{E} \\left[ #1 \\right]", 1], "var": ["\\text{Var} \\left( #1 \\right)", 1], "std": ["\\text{Std} \\left( #1 \\right)", 1], "cov": ["\\text{Cov} \\left( #1, #2 \\right)", 2], "dmat": "\\boldsymbol{X}", "models": ["\\boldsymbol{M}\\left( #1 \\, ; \\, #2 \\right)", 2], "model": ["M\\left( #1 \\, ; \\, #2 \\right)", 2], "modeloutputs": "\\boldsymbol{M}", "modeloutput": "M", "MLmodel": ["\\boldsymbol{\\hat{y}}\\left( #1 \\right)", 1], "MLoutputs": "\\boldsymbol{\\hat{y}}", "MLoutput": "\\hat{y}", "outputs": "\\boldsymbol{y}", "inputs": "\\boldsymbol{x}", "targets": "\\boldsymbol{t}", "weights": "\\boldsymbol{w}", "testoutputs": "\\boldsymbol{y}^\\odot", "testinputs": "\\boldsymbol{x}^\\odot", "output": "y", "inputt": "x", "target": "t", "weight": "w", "testoutput": "y^\\odot", "MLtestoutput": "\\hat{y}^\\odot", "testinput": "x^\\odot", "trainingdata": "\\mathcal{T}", "LaTeX": "\\text{LaTeX}", "residual": "\\epsilon", "residuals": "\\boldsymbol{\\epsilon}", "zeros": "\\boldsymbol{0}", "covres": "\\boldsymbol{\\Sigma_{\\epsilon}}", "covpars": "\\boldsymbol{\\Sigma_{\\pars}}", "tildecovpars": "\\boldsymbol{\\widetilde{\\Sigma}_{\\pars}}", "sigmas": "\\boldsymbol{\\sigma}", "sigmai": "\\sigma_i", "sigmares": "\\sigma_{\\epsilon}", "cbar": "\\bar c", "Lra": "\\Longrightarrow", "yth": "y_{\\text{th}}", "yexp": "y_{\\text{exp}}", "ym": "y_{\\text{m}}", "thetavec": "\\boldsymbol{\\theta}", "parsLR": "\\boldsymbol{\\beta}", "paraLR": "\\beta", "covparsLR": "\\boldsymbol{\\Sigma_{\\parsLR}}", "optparsLR": "\\parsLR^*", "optparaLR": "\\paraLR^*", "tildecovparsLR": "\\boldsymbol{\\widetilde{\\Sigma}_{\\parsLR}}", "alphavec": "\\boldsymbol{\\alpha}", "muvec": "\\boldsymbol{\\mu}", "phivec": "\\boldsymbol{\\phi}", "betavec": "\\boldsymbol{\\beta}", "sigmavec": "\\boldsymbol{\\sigma}", "Sigmavec": "\\boldsymbol{\\Sigma}", "thetavechat": "\\widehat\\thetavec", "avec": "\\boldsymbol{a}", "Bvec": "\\boldsymbol{B}", "fvec": "\\boldsymbol{f}", "mvec": "\\boldsymbol{m}", "qvec": "\\boldsymbol{q}", "rvec": "\\boldsymbol{r}", "uvec": "\\boldsymbol{u}", "wvec": "\\boldsymbol{w}", "xvec": "\\boldsymbol{x}", "yvec": "\\boldsymbol{y}", "wt": "\\widetilde", "nb": "n_b", "mel": ["\\langle #1 | #2 | #3 \\rangle", 3], "qoi": "\\mathbf{Q}", "ytrue": "y_{\\text{true}}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'LearningFromData-content/BayesianStatistics/BayesianBasics/Inferenceandpdfs/sec-03-probability-density-functions';</script>
    <script src="../../../../_static/custom.js?v=33f35b7a"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="4.4. Looking ahead" href="sec-04-summary.html" />
    <link rel="prev" title="4.2. Manipulating probabilities: Bayesian rules of probability as principles of logic" href="sec-02-manipulating-probabilities-bayesian-rules-of-probability-as.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../Intro/About.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../../_static/logo-copilot.png" class="logo__image only-light" alt="Combined Learning from Data materials - Home"/>
    <script>document.write(`<img src="../../../../_static/logo-copilot.png" class="logo__image only-dark" alt="Combined Learning from Data materials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../Intro/About.html">
                    Learning from data for physicists
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../Intro/Invitation.html">1. Invitation to inductive inference</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../Intro/Introduction.html">2. Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../Intro/Introduction/sec-01-physicist-s-perspective.html">2.1. Physicist’s perspective</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Intro/Introduction/sec-02-bayesian-workflow.html">2.2. Bayesian workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Intro/Introduction/sec-03-machine-learning.html">2.3. Machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Intro/Introduction/sec-04-virtues.html">2.4. Virtues</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I: Bayesian methods for scientific modeling</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../RootBayesianBasics.html">3. Overview of Part I</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../Inferenceandpdfs.html">4. Inference and PDFs</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="sec-01-statements.html">4.1. Statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="sec-02-manipulating-probabilities-bayesian-rules-of-probability-as.html">4.2. Manipulating probabilities: Bayesian rules of probability as principles of logic</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">4.3. Probability density functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="sec-04-summary.html">4.4. Looking ahead</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MoreBayesTheorem.html">4.5. Review of Bayes’ theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../DataModelsPredictions.html">4.6. Data, models, and predictions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Bayesian_epistemology.html">4.7. *Aside: Bayesian epistemology</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Posteriors.html">5. Bayesian posteriors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../Exploring_pdfs.html">5.1. 📥 Exploring PDFs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../Exploring_pdfs_followups.html">Follow-ups to Exploring PDFs</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../Visualizing_correlated_gaussians.html">5.2. 📥 Visualizing correlated Gaussian distributions</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../Gaussians.html">5.3. Gaussians: A couple of frequentist connections</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../visualization_of_CLT.html">📥 Visualization of the Central Limit Theorem</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianParameterEstimation/Interpreting2Dposteriors.html">5.4. Interpreting 2D posteriors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chi_squared_tests.html">5.5. 📥 Demonstration: Sum of normal variables squared</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../CoinTossing.html">6. Updating via Bayes' rule</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../CoinTossing/sec-01-coin-tossing-frequentists-and-bayesaians.html">6.1. Coin tossing: Frequentists and Bayesaians</a></li>
<li class="toctree-l2"><a class="reference internal" href="../CoinTossing/sec-02-when-do-priors-matter-when-don-t-they-matter.html">6.2. When do priors matter? When don’t they matter?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../CoinTossing/sec-03-computing-the-posterior-analytically.html">6.3. Computing the posterior analytically</a></li>
<li class="toctree-l2"><a class="reference internal" href="../CoinTossing/sec-04-degree-of-belief-credibility-intervals-vs-frequentist-1-sigm.html">6.4. Degree of belief/credibility intervals vs frequentist 1-sigma intervals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../CoinTossing/sec-05-take-aways-and-follow-up-questions-from-coin-flipping.html">6.5. Take-aways and follow-up questions from coin flipping</a></li>
<li class="toctree-l2"><a class="reference internal" href="../demo-BayesianBasics.html">6.6. 📥 Demonstration:  Bayesian Coin Tossing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Bayesian_updating_coinflip_interactive.html">6.7. 📥 Demonstration: Coin tossing (with widget)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../UsingBayes.html">7. Bayes in practice</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianAdvantages.html">7.1. Advantages of the Bayesian approach</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianWorkflow/BayesianWorkflow.html">7.2. Bayesian research workflow</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../BayesianLinearRegression/BayesianLinearRegression_rjf.html">7.3. Bayesian Linear Regression (BLR)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../ModelingOptimization/demo-ModelValidation.html">📥 Demonstration: Linear Regression and Model Validation</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ErrorPropagation.html">8. Error propagation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ErrorPropagation/sec-01-error-propagation-i-nuisance-parameters-and-marginalization.html">8.1. Error propagation (I): Nuisance parameters and marginalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ErrorPropagation/sec-02-error-propagation-ii-changing-variables.html">8.2. Error propagation (II): Changing variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ErrorPropagation/sec-03-error-propagation-iii-a-useful-approximation.html">8.3. Error propagation (III): A useful approximation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ErrorPropagation/sec-04-solutions.html">8.4. Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../BayesianParameterEstimation/Exercises_parameter_estimation.html">9. Exercises for Part I</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../exercise_sum_product_rule.html">9.1. Exercise: Checking the sum and product rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../exercise_medical_example_by_Bayes.html">9.2. Exercise: Standard medical example using Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianParameterEstimation/parameter_estimation_Gaussian_noise.html">9.3. 📥 Parameter estimation I: Gaussian mean and variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianParameterEstimation/radioactive_lighthouse_exercise.html">9.4. 📥 Radioactive lighthouse problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianParameterEstimation/amplitude_in_presence_of_background.html">9.5. 📥 Amplitude of a signal in the presence of background</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianParameterEstimation/parameter_estimation_fitting_straight_line_I.html">9.6. 📥 Parameter estimation example: fitting a straight line</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianParameterEstimation/parameter_estimation_fitting_straight_line_II.html">9.7. 📥 Parameter estimation example: fitting a straight line II</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II: Advanced Bayesian methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../RootAdvancedMethods.html">10. Overview of Part II</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../OtherTopics/DiscrepancyModels.html">11. Discrepancy Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../OtherTopics/DiscrepancyModels/sec-01-koh-and-boh-discrepancy-models.html">11.1. KOH and BOH discrepancy models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../OtherTopics/DiscrepancyModels/sec-02-framework.html">11.2. Framework</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../OtherTopics/DiscrepancyModels/sec-03-the-ball-drop-model.html">11.3. The ball-drop model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../OtherTopics/MD_balldrop_v1.html">11.4. 📥 Ball-drop experiment notebook</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../AssigningProbabilities/Assigning.html">12. Assigning probabilities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../AssigningProbabilities/IgnorancePDF.html">12.1. Assigning probabilities (I): Indifferences and translation groups</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../AssigningProbabilities/demo-straight_lines.html">Alternative notebook with MCMC sampling</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../AssigningProbabilities/MaxEnt2.html">12.2. Assigning probabilities (II): The principle of maximum entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../AssigningProbabilities/MaxEnt_Function_Reconstruction.html">12.3. 📥 Maximum Entropy for reconstructing a function from its moments</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../BayesianParameterEstimation/dealing_with_outliers.html">13. 📥 Dealing with outliers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ComputationalBayes/BayesLinear.html">14. Bayes goes linear: History matching</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Multimodel_inference.html">15. Multi-model inference with Bayes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../ModelSelection/ModelSelection.html">15.1. Model Selection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ModelSelection/BUQ/Evidence_for_model_EFT_coefficients.html">Evidence calculation for EFT expansions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ModelSelection/BUQ/two_model_evidence.html">Follow-up to EFT evidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ModelSelection/BUQ/computing_evidence.html">Computing the evidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ModelSelection/BUQ/MCMC-parallel-tempering_ptemcee_vs_zeus.html">Demo: Multimodal distributions with two samplers</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelMixing/model_mixing.html">15.2. Model averaging and mixing </a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III: MCMC sampling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../StochasticProcesses/RootMCMC.html">16. Overview of Part III</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../StochasticProcesses/StochasticProcesses.html">17. Stochastic processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../StochasticProcesses/BUQ/Metropolis_Poisson_example.html">17.7. Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../StochasticProcesses/demo-MCMC.html">17.8. Demonstration: Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../StochasticProcesses/Recap_BUQ.html">17.9. Recap of Poisson and more about MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../StochasticProcesses/BUQ/parameter_estimation_Gaussian_noise-2.html">17.10. Parameter estimation example: Gaussian noise and averages II</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../StochasticProcesses/BUQ/MCMC-random-walk-and-sampling.html">17.11. Exercise: Random walk</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../StochasticProcesses/MCMC_overview.html">18. Overview of Markov Chain Monte Carlo</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../StochasticProcesses/MarkovChains.html">18.1. Markov chains</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../StochasticProcesses/MCMC.html">18.2. Markov chain Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../StochasticProcesses/MCMC_intro_BUQ.html">18.3. Alternative MCMC introduction (Gregory)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../StochasticProcesses/BUQ/Assignment_extending_radioactive_lighthouse.html">18.4. Assignment: 2D radioactive lighthouse location using MCMC</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../StochasticProcesses/Advanced_MCMC.html">19. Advanced MCMC</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ComputationalBayes/AdvancedMCMC.html">19.1. Advanced Markov chain Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../StochasticProcesses/BUQ/MCMC-diagnostics.html">19.2. Overview: MCMC Diagnostics</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../StochasticProcesses/BUQ/intuition_sampling.html">19.4. Intuition on sampling and best practices</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../StochasticProcesses/Other_samplers.html">20. HMC and other samplers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../StochasticProcesses/BUQ2/HMC_intro_BUQ.html">20.1. Hamiltonian Monte Carlo (HMC) overview and visualization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../StochasticProcesses/BUQ2/Liouville_theorem_visualization.html">Liouville Theorem Visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../StochasticProcesses/BUQ2/Orbital_eqs_with_different_algorithms.html">Solving orbital equations with different algorithms</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../StochasticProcesses/zeus.html">20.2. The Zeus Ensemble Slice Sampler</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../StochasticProcesses/BUQ2/PyMC_intro_updated.html">20.3. PyMC Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../StochasticProcesses/OverviewIntroPyMC.html">Overview of Intro to PyMC notebook</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../StochasticProcesses/BUQ2/parameter_estimation_Gaussian_noise_compare_samplers.html">20.4. Comparing samplers for a simple problem</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV: Machine learning: A Bayesian perspective</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../MachineLearning/RootML.html">21. Overview of Part IV</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../MachineLearning/GP/RootGP.html">22. Overview of Gaussian processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../MachineLearning/GP/GaussianProcesses.html">22.4. Introduction to Gaussian processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../MachineLearning/GP/CF/demo-GaussianProcesses.html">📥 demo-GaussianProcesses notebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../MachineLearning/GP/BUQ/lecture_20.html">GP recap; GP applications; (old lecture 20)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../MachineLearning/GP/Sklearn_demos.html">22.5. scikit-learn demo notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../MachineLearning/GP/BUQ/plot_gpr_noisy_targets.html">📥 One-dimension regression example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../MachineLearning/GP/BUQ/plot_gpr_prior_posterior.html">📥 Prior and posterior with different kernels</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../MachineLearning/GP/GPy_demos.html">22.6. GPy demo notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../MachineLearning/GP/BUQ/demo-GaussianProcesses.html">Gaussian processes demonstration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../MachineLearning/GP/CF/exercise_GP_GPy.html">Exercise: Gaussian processes using <code class="docutils literal notranslate"><span class="pre">GPy</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../MachineLearning/GP/BUQ/Gaussian_processes_exercises.html">Exercise: Gaussian Process models with GPy</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../MachineLearning/LogReg/LogReg.html">23. Logistic Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../MachineLearning/ANN/MachineLearningExamples.html">23.5. Machine Learning: First Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../MachineLearning/ANN/NeuralNet/exercises_LogReg_NeuralNet.html">23.6. Exercise: Logistic Regression and neural networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../MachineLearning/ANN/MachineLearning.html">24. Machine learning: Overview and notation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../MachineLearning/ANN/NeuralNet.html">24.5. Artifical neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../MachineLearning/ANN/NeuralNet/demo-NeuralNet.html">24.6. Demonstration: Neural network classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../MachineLearning/ANN/Neural_Network_for_simple_function_in_PyTorch.html">24.7. 📥 ANN from ChatGPT using PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../MachineLearning/ANN/ModelValidation.html">24.8. Model validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../MachineLearning/ANN/DataBiasFairness.html">24.9. Data bias and fairness in machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../MachineLearning/ANN/NeuralNet/NeuralNetBackProp.html">24.10. *Neural networks: Backpropagation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../OtherTopics/ANNFT.html">25. ANNs in the large-width limit (ANNFT)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../OtherTopics/random_initialized_ANN_vs_width.html">25.3. 📥 Distributions of Randomly-Initialized ANNs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../MachineLearning/BNN/bnn.html">26. Bayesian neural nets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../MachineLearning/BNN/demo-bnn.html">26.4. Demonstration: Variational Inference and Bayesian Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../MachineLearning/BNN/exercises_BNN.html">26.5. Exercise: Bayesian neural networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../MachineLearning/CNN/cnn.html">27. *Convolutional neural nets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../MachineLearning/CNN/demo-cnn.html">27.6. Demonstration: Image recognition with Convolutional Neural Networks</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V: Other topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../OtherTopics/RootOtherTopics.html">28. Overview of Part V </a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../OtherTopics/Emulators.html">29. Emulators</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ComputationalBayes/BayesFast.html">29.1. Bayes goes fast: Emulators (from CF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ComputationalBayes/extra_RBM_emulators.html">29.2. RBM emulators (BUQ)</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../OtherTopics/Student_t_distribution_from_Gaussians.html">30. 📥 Student t distribution from Gaussians</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../OtherTopics/SVD.html">31. PCA, SVD, and all that</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../OtherTopics/linear_algebra_games_including_SVD.html">31.5. 📥 demo-SVD notebook</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../OtherTopics/qbism.html">32. QBism: Bayesian quantum mechanics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Backmatter</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../Backmatter/bibliography.html">33. Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Backmatter/JB_tests.html">34. Guide to Jupyter Book markdown</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix A: Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../Reference/Statistics.html">35. Statistics concepts and notation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ModelingOptimization/GradientDescent.html">36. Gradient-descent optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix B: Scientific modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../ModelingOptimization/RootScientificModeling.html">37. Overview of scientific modeling material</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../ModelingOptimization/OverviewModeling.html">38. Overview of modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../ModelingOptimization/OverviewModeling/sec-01-notation.html">38.1. Notation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ModelingOptimization/OverviewModeling/sec-02-models-in-science.html">38.2. Models in science</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ModelingOptimization/OverviewModeling/sec-03-parametric-versus-non-parametric-models.html">38.3. Parametric versus non-parametric models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ModelingOptimization/OverviewModeling/sec-04-linear-versus-non-linear-models.html">38.4. Linear versus non-linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ModelingOptimization/OverviewModeling/sec-05-regression-analysis-optimization-versus-inference.html">38.5. Regression analysis: optimization versus inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ModelingOptimization/OverviewModeling/sec-06-exercises.html">38.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ModelingOptimization/OverviewModeling/sec-07-solutions.html">38.7. Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../ModelingOptimization/LinearModels.html">39. Linear models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../ModelingOptimization/LinearModels/sec-01-definition-of-linear-models.html">39.1. Definition of linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ModelingOptimization/LinearModels/sec-02-regression-analysis-with-linear-models.html">39.2. Regression analysis with linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ModelingOptimization/LinearModels/sec-03-ordinary-linear-regression-warmup.html">39.3. Ordinary linear regression: warmup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ModelingOptimization/LinearModels/sec-04-ordinary-linear-regression-in-practice.html">39.4. Ordinary linear regression in practice</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ModelingOptimization/LinearModels/sec-05-solutions.html">39.5. Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../ModelingOptimization/MathematicalOptimization.html">40. Mathematical optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../ModelingOptimization/MathematicalOptimization/sec-01-gradient-descent-optimization.html">40.1. Gradient-descent optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ModelingOptimization/MathematicalOptimization/sec-02-batch-stochastic-and-mini-batch-gradient-descent.html">40.2. Batch, stochastic and mini-batch gradient descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ModelingOptimization/MathematicalOptimization/sec-03-adaptive-gradient-descent-algorithms.html">40.3. Adaptive gradient descent algorithms</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix C: Getting started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../Setup/RootGettingStarted.html">41. Overview of Getting started material</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Setup/exercise_Intro_01_Jupyter_Python.html">42. 📥 Exercise: Jupyter notebooks and Python</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../Setup/more_python_and_jupyter.html">43. More about Python and Jupyter notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../Setup/exercise_Intro_02_Jupyter_Python.html">43.4. 📥 Python lists and iterations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Setup/exercise_Intro_03_Numpy.html">43.5. 📥 Linear algebra operations with NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Setup/demo-Intro.html">43.6. 📥 Reading data and fitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Setup/Simple_widgets_v1.html">43.7. 📥 Making a simple widget-based UI</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../Setup/setting_up.html">44. Setting up for using this Jupyter Book</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../Setup/installing_anaconda.html">44.1. Using Anaconda</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Setup/using_github.html">44.2. Using GitHub</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">TALENT mini-projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../Mini-projects/RootMiniProjects.html">Overview of mini-projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Mini-projects/mini-project_I_toy_model_of_EFT.html">📥 MP I: Parameter estimation for a toy model of an EFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Mini-projects/model-selection_mini-project-IIa.html">📥 MP IIa: Model selection basics</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../Mini-projects/model-selection_mini-project-IIb_How_many_lines_ptemcee.html">📥 MP IIb: How many lines?</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../Mini-projects/Mini-project_IIb_overview.html">Overview of Mini-project IIb: How many lines?</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Mini-projects/mini-project_IIIa_bayesian_optimization.html">📥 MP IIIa: Bayesian optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Mini-projects/mini-project_IIIb_Bayesian_neural_networks_from_demo.html">📥 MP IIIb: Bayesian Neural Networks</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/NuclearTalent/LFD_for_Physicists" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/NuclearTalent/LFD_for_Physicists/issues/new?title=Issue%20on%20page%20%2FLearningFromData-content/BayesianStatistics/BayesianBasics/Inferenceandpdfs/sec-03-probability-density-functions.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../../_sources/LearningFromData-content/BayesianStatistics/BayesianBasics/Inferenceandpdfs/sec-03-probability-density-functions.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Probability density functions</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-pdfs-marginal-pdfs-and-an-example-of-marginalization">Joint PDFs, marginal PDFs, and an example of marginalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem-applied-to-pdfs">Bayes’ theorem applied to PDFs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-pdfs">Visualizing PDFs</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="probability-density-functions">
<span id="sec-inference-pdfs"></span><h1><span class="section-number">4.3. </span>Probability density functions<a class="headerlink" href="#probability-density-functions" title="Link to this heading">#</a></h1>
<p>The key mathematical entity used over and over again throughout this book will be the “probability density function”, PDF for short. PDFs for continuous quantities are integrated to obtain probabilities. An example familiar to every physicist will be the probability density of a quantum-mechanical particle. Max Born somewhat belatedly decided to put a square on Schrodinger’s wave function when interpreting the integral from <span class="math notranslate nohighlight">\(x=a\)</span> to <span class="math notranslate nohighlight">\(x=b\)</span> as the probability of measuring the particle to be between <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="math notranslate nohighlight">
\[
   \prob(a \leq x \leq b) = \int_a^b |\psi(x)|^2\, dx
\]</div>
<p>We note that the unit of <span class="math notranslate nohighlight">\(|\psi(x)|^2\)</span> is inverse length, and it is generally true that PDFs, unlike probabilities, have units. If we divorce the previous equation from the quantum-mechanical context we would write:</p>
<div class="math notranslate nohighlight">
\[
   \prob(a \leq x \leq b) = \int_a^b \p{x}\, dx
\]</div>
<p>where <span class="math notranslate nohighlight">\(\p{x}\)</span> is the PDF for the continuous variable <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="my-checkpoint admonition">
<p class="admonition-title">Checkpoint question</p>
<p>What is the unit of <span class="math notranslate nohighlight">\(p(\xvec)\)</span> (or <span class="math notranslate nohighlight">\(|\psi(\xvec)|^2\)</span>) in three dimensions (assuming <span class="math notranslate nohighlight">\(\xvec\)</span> is a vector of length)?</p>
<div class="dropdown my-answer admonition">
<p class="admonition-title">Answer</p>
<p>We can use the normalization equation (which is the sum rule):</p>
<div class="math notranslate nohighlight">
\[
  \int d^3x\, p(\xvec) = 1 ,
\]</div>
<p>which is dimensionless on the right side, so the unit of <span class="math notranslate nohighlight">\(p(\xvec)\)</span> (or <span class="math notranslate nohighlight">\(|\psi(\xvec)|^2\)</span>) is the inverse unit of <span class="math notranslate nohighlight">\(d^3x\)</span>, or <span class="math notranslate nohighlight">\(1/\text{length}^3\)</span>. Note that if <span class="math notranslate nohighlight">\(\xvec\)</span> represented a different quantity, the unit of <span class="math notranslate nohighlight">\(p(\xvec)\)</span> would differ accordingly.</p>
</div>
</div>
<p>We note that the case of a discrete random variable, e.g., the numbers
that can be rolled on a die, mean that the PDF only is non-zero at a
discrete set of numbers. In that case <span class="math notranslate nohighlight">\(\p{x}\)</span> can be represented as
a sum of Dirac delta functions, and the resulting object is sometimes
called a probability mass function. Thus the cases discussed above,
in which <span class="math notranslate nohighlight">\(\prob\)</span> represented probability measured over a discrete set
of outcomes, can be understood as special cases of pdfs.</p>
<p>In fact, working this from the other end,  the sum and product rules
to pdfs in just the same way as they do to probabilities. Therefore Bayes’ theorem (or
rule) can also be applied to relate the pdf of x given y, <span class="math notranslate nohighlight">\(\pdf{x}{y}\)</span>
to the pdf of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(x\)</span>, <span class="math notranslate nohighlight">\(\pdf{y}{x}\)</span>.</p>
<p>In Bayesian statistics there are PDFs (or PMFs if discrete) for
everything. Here are a few examples:</p>
<ul class="simple">
<li><p>fit parameters — like the slope and intercept of a line in a
straight-line fit</p></li>
<li><p>experimental <em>and</em> theoretical imperfections that lead to
uncertainties in the final result</p></li>
<li><p>events (“Will it rain tomorrow?”)</p></li>
</ul>
<p>We will stick to the <span class="math notranslate nohighlight">\(\p{\cdot}\)</span> notation here, but it’s worth pointing out that many different variants of the letter <span class="math notranslate nohighlight">\(p\)</span> are used to represent probabilities and PDFs in the literature. If we are interested in the PDF in a higher-dimensional space (say the probability of finding a particle in 3D space) you might see <span class="math notranslate nohighlight">\(p(\vec x) = p(\mathbf{x}) = P(\vec x) = \text{pr}(\vec x) = \text{prob}(\vec x) = \ldots\)</span>.</p>
<div class="my-checkpoint admonition">
<p class="admonition-title">Checkpoint question</p>
<p>What is the PDF <span class="math notranslate nohighlight">\(\p{x}\)</span> if we know <strong>definitely</strong> that <span class="math notranslate nohighlight">\(x = x_0\)</span> (i.e., fixed)?</p>
<div class="dropdown my-answer admonition">
<p class="admonition-title">Answer</p>
<p><span class="math notranslate nohighlight">\(\p{x} = \delta(x-x_0)\quad\)</span>  [Note that <span class="math notranslate nohighlight">\(p(x)\)</span> is normalized.]</p>
</div>
</div>
<section id="joint-pdfs-marginal-pdfs-and-an-example-of-marginalization">
<h2>Joint PDFs, marginal PDFs, and an example of marginalization<a class="headerlink" href="#joint-pdfs-marginal-pdfs-and-an-example-of-marginalization" title="Link to this heading">#</a></h2>
<p><span class="math notranslate nohighlight">\(\p{x_1, x_2}\)</span> is the <em>joint</em> probability density of <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>. In quantum mechanics the probability density <span class="math notranslate nohighlight">\(|\Psi(x_1, x_2)|^2\)</span> to find particle 1 at <span class="math notranslate nohighlight">\(x_1\)</span> and particle 2 at <span class="math notranslate nohighlight">\(x_2\)</span> is a joint probability density.</p>
<div class="my-checkpoint admonition">
<p class="admonition-title">Checkpoint question</p>
<p>What is the probability to find particle 1 at <span class="math notranslate nohighlight">\(x_1\)</span> while particle 2 is <em>anywhere</em>?</p>
<div class="dropdown my-answer admonition">
<p class="admonition-title">Answer</p>
<p><span class="math notranslate nohighlight">\(\int_{-\infty}^{+\infty} |\psi(x_1, x_2)|^2\, dx_2\ \ \)</span> or, more generally, integrated over the domain of <span class="math notranslate nohighlight">\(x_2\)</span>.</p>
</div>
</div>
<p>This is a specific example of the marginalization rule, now in the pdf
context, that we
discussed the general form for probabilities above. The <em>marginal
probability density</em> of <span class="math notranslate nohighlight">\(x_1\)</span> is the result when we marginalize the
<em>joint probability distribution</em> over <span class="math notranslate nohighlight">\(x_2\)</span>:</p>
<div class="math notranslate nohighlight">
\[
 \p{x_1} = \int \p{x_1, x_2} \,dx_2 .
\]</div>
<p>So, in our quantum mechanical example, it’s the probability
density we get when we just focus on particle 1, and don’t care about
particle 2. <em>Marginalizing</em> in the Bayesian context means
``integrating out’’ a parameter one is–at least temporarily–not
interested in, to leave the focus on the PDF of other parameters. This
can be particularly useful if there are “nuisance” parameters in the
statistical model that account for the impact of defects in the
measuring apparatus, but ultimately one is interested in the physics
extracted with the imperfect apparatus.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You may have noticed that we wrote <span class="math notranslate nohighlight">\(\p{x_1}\)</span> and <span class="math notranslate nohighlight">\(\p{x_1, x_2}\)</span> rather than <span class="math notranslate nohighlight">\(\pdf{x_1}{I}\)</span> and <span class="math notranslate nohighlight">\(\pdf{x1,x2}{I}\)</span>, where “<span class="math notranslate nohighlight">\(I\)</span>” is information we know but do not specify explicitly. Our PDFs will always be contingent on <em>some</em> information, so we were really being sloppy by trying to be compact. (See <a class="reference internal" href="../MoreBayesTheorem.html#sec-continuum-limit"><span class="std std-ref">The continuum limit</span></a> for more careful versions of these equations.)</p>
</div>
</section>
<section id="bayes-theorem-applied-to-pdfs">
<h2>Bayes’ theorem applied to PDFs<a class="headerlink" href="#bayes-theorem-applied-to-pdfs" title="Link to this heading">#</a></h2>
<p>The rules applied to probabilities in <a class="reference internal" href="sec-02-manipulating-probabilities-bayesian-rules-of-probability-as.html#sec-inference-manipulation"><span class="std std-numref">Section 4.2</span></a> directly extend to continuous functions, i.e., PDFs; we’ll summarize the extension briefly here.
Conventionally we denote by <span class="math notranslate nohighlight">\(\thetavec\)</span> a vector of continuous parameters we seek to determine (for now we’ll use <span class="math notranslate nohighlight">\(\alphavec\)</span> as another vector of parameters).
As before, information <span class="math notranslate nohighlight">\(I\)</span> is kept implicit.</p>
<p><strong>Sum rule</strong></p>
<p>Probabilities integrate to one (assuming exhaustive and exclusive <span class="math notranslate nohighlight">\(\thetavec\)</span>):</p>
<div class="math notranslate nohighlight">
\[
  \int d\thetavec\, \pdf{\thetavec}{I} = 1 .
\]</div>
<p>The sum rule <em>implies</em> marginalization:</p>
<div class="math notranslate nohighlight">
\[ 
     p(\thetavec|I) = \int d\alphavec\, p(\thetavec, \alphavec | I) .
\]</div>
<p><strong>Product rule</strong></p>
<p>Expanding a joint probability of <span class="math notranslate nohighlight">\(\thetavec\)</span> and <span class="math notranslate nohighlight">\(\alphavec\)</span></p>
<div class="math notranslate nohighlight" id="equation-eq-pdf-joint-prob">
<span class="eqno">(4.6)<a class="headerlink" href="#equation-eq-pdf-joint-prob" title="Link to this equation">#</a></span>\[  
   p(\thetavec, \alphavec| I) = p(\thetavec | \alphavec, I) p(\alphavec,I) = p(\alphavec| \thetavec,I) p(\thetavec,I) .
\]</div>
<p>As with discrete probabilities, there is a symmetry between the first and second equalities.
If <span class="math notranslate nohighlight">\(\thetavec\)</span> and <span class="math notranslate nohighlight">\(\alphavec\)</span> are <em>mutually independent</em>, then <span class="math notranslate nohighlight">\(p(\thetavec | \alphavec,I) = p(\thetavec | I)\)</span> and</p>
<div class="math notranslate nohighlight">
\[
  p(\thetavec,\alphavec | I) = p(\thetavec|I) \times p(\alphavec | I) .
\]</div>
<p>Rearranging the 2nd equality in <a class="reference internal" href="#equation-eq-pdf-joint-prob">(4.6)</a> yields <strong>Bayes’ Rule</strong> (or Theorem) just like in the discrete case:</p>
<div class="math notranslate nohighlight">
\[
  p(\thetavec | \alphavec,I) = \frac{p(\alphavec|\thetavec,I) p(\thetavec|I)}{p(\alphavec | I)}
\]</div>
<p>Bayes’ rule tells us how to reverse the conditional: <span class="math notranslate nohighlight">\(p(\thetavec|\alphavec) \rightarrow p(\alphavec|\thetavec)\)</span>.
A typical application is when <span class="math notranslate nohighlight">\(\alphavec\)</span> is a vector of data <span class="math notranslate nohighlight">\(\data\)</span>. Then Bayes’ rule is</p>
<div class="math notranslate nohighlight">
\[
  \overbrace{ \pdf{\thetavec}{\data,I)} }^{\textrm{posterior}} =
  \frac{ \color{red}{ \overbrace{ \pdf{\data}{\thetavec,I} }^{\textrm{likelihood}}} 
 \color{black}{\ \times\ } 
  \color{blue}{ \overbrace{ \pdf{\thetavec}{I}}^{\textrm{prior}}}    
 } 
 { \color{darkgreen}{ \underbrace{ \pdf{\data}{I} }_{\textrm{evidence}}} }
\]</div>
<p>Viewing the prior as the initial information we have about <span class="math notranslate nohighlight">\(\thetavec\)</span> (i.e., before using the data <span class="math notranslate nohighlight">\(\data\)</span>), summarized as a probability density function, then Bayes’ theorem tells us how to <strong>update</strong> that information after observing some data: this is the posterior PDF.  In <a class="reference internal" href="../CoinTossing.html#sec-updatingbayes"><span class="std std-numref">Section 6</span></a> we will give some examples of how this plays out when tossing biased coins.</p>
</section>
<section id="visualizing-pdfs">
<h2>Visualizing PDFs<a class="headerlink" href="#visualizing-pdfs" title="Link to this heading">#</a></h2>
<p>It is worthwhile at this stage to jump ahead and work through parts of the Jupyter notebook
<a class="reference internal" href="../Exploring_pdfs.html"><span class="std std-doc">Exploring PDFs</span></a> to make a first pass at getting familiar with PDFs and how to visualize them. In the notebook you will
work with the python package <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code>, which has many
distributions built in. You can learn more about those distributions by
reading the manual page (which you can find by googling).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference internal" href="../../../Reference/Statistics.html#sec-statistics"><span class="std std-ref">Statistics concepts and notation</span></a> in Appendix A for further details on PDFs.</p>
</div>
<p>The diversity of distributions available there should make it clear
that the “default” choice of a Gaussian distribution is just that: a
default choice. In <a class="reference internal" href="../Gaussians.html#sec-gaussians"><span class="std std-ref">Gaussians: A couple of frequentist connections</span></a> we will explore why this default
choice is often the correct one. But often is not the same as all the
time, and it is frequently the case that other distributions, such as
the Student t, gives a better description of the way data is
distributed.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Trivia: “Student” was the pen name of the Head Brewer at Guinness — a pioneer of small-sample experimental design (hence not necessarily Gaussian). His real name was William Sealy Gossett.</p>
</div>
<p>Because we can draw an arbitrary number of samples from the
distributions defined in scipy.stats we can see how the distributions
build up as the number of samples increases, and how there are
fluctuations around the asymptotic distribution that are larger for a
small number of samples.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./LearningFromData-content/BayesianStatistics/BayesianBasics/Inferenceandpdfs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="sec-02-manipulating-probabilities-bayesian-rules-of-probability-as.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">4.2. </span>Manipulating probabilities: Bayesian rules of probability as principles of logic</p>
      </div>
    </a>
    <a class="right-next"
       href="sec-04-summary.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4.4. </span>Looking ahead</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-pdfs-marginal-pdfs-and-an-example-of-marginalization">Joint PDFs, marginal PDFs, and an example of marginalization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem-applied-to-pdfs">Bayes’ theorem applied to PDFs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-pdfs">Visualizing PDFs</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christian Forssén, Dick Furnstahl, and Daniel Phillips
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>