
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>12.1. Model Selection &#8212; Combined Learning from Data materials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/coloredpages.css?v=0a037ad7" />
    <link rel="stylesheet" type="text/css" href="../../../_static/myadmonitions.css?v=89ac28d1" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"loader": {"load": ["[tex]/textmacros"]}, "chtml": {"mtextInheritFont": true}, "tex": {"packages": {"[+]": ["textmacros"]}, "macros": {"data": "\\mathcal{D}", "pars": "\\boldsymbol{\\theta}", "para": "\\theta", "optpars": "\\pars^*", "optpara": "\\para^*", "prob": "\\mathbb{P}", "cprob": ["\\prob\\left( #1 \\, \\left\\vert \\, #2 \\right. \\right)", 2], "cprobsub": ["\\prob_{#1}\\left( #2 \\, \\left\\vert \\, #3 \\right. \\right)", 3], "pdf": ["p \\left( #1 \\, \\left\\vert \\, #2 \\right. \\right)", 2], "pdfsub": ["p_{#1} \\left( #2 \\, \\left\\vert \\, #3 \\right. \\right)", 3], "p": ["p \\left( #1 \\right)", 1], "psub": ["p_{#1} \\left( #2 \\right)", 2], "futuredata": "\\mathcal{F}", "expect": ["\\mathbb{E} \\left[ #1 \\right]", 1], "var": ["\\text{Var} \\left( #1 \\right)", 1], "std": ["\\text{Std} \\left( #1 \\right)", 1], "cov": ["\\text{Cov} \\left( #1, #2 \\right)", 2], "dmat": "\\boldsymbol{X}", "models": ["\\boldsymbol{M}\\left( #1 \\, ; \\, #2 \\right)", 2], "model": ["M\\left( #1 \\, ; \\, #2 \\right)", 2], "modeloutputs": "\\boldsymbol{M}", "modeloutput": "M", "MLmodel": ["\\boldsymbol{\\hat{y}}\\left( #1 \\right)", 1], "MLoutputs": "\\boldsymbol{\\hat{y}}", "MLoutput": "\\hat{y}", "outputs": "\\boldsymbol{y}", "inputs": "\\boldsymbol{x}", "targets": "\\boldsymbol{t}", "weights": "\\boldsymbol{w}", "testoutputs": "\\boldsymbol{y}^\\odot", "testinputs": "\\boldsymbol{x}^\\odot", "output": "y", "inputt": "x", "target": "t", "weight": "w", "testoutput": "y^\\odot", "MLtestoutput": "\\hat{y}^\\odot", "testinput": "x^\\odot", "trainingdata": "\\mathcal{T}", "LaTeX": "\\text{LaTeX}", "residual": "\\epsilon", "residuals": "\\boldsymbol{\\epsilon}", "zeros": "\\boldsymbol{0}", "covres": "\\boldsymbol{\\Sigma_{\\epsilon}}", "covpars": "\\boldsymbol{\\Sigma_{\\pars}}", "tildecovpars": "\\boldsymbol{\\widetilde{\\Sigma}_{\\pars}}", "sigmas": "\\boldsymbol{\\sigma}", "sigmai": "\\sigma_i", "sigmares": "\\sigma_{\\epsilon}", "cbar": "\\bar c", "Lra": "\\Longrightarrow", "yth": "y_{\\text{th}}", "yexp": "y_{\\text{exp}}", "ym": "y_{\\text{m}}", "thetavec": "\\boldsymbol{\\theta}", "parsLR": "\\boldsymbol{\\beta}", "paraLR": "\\beta", "covparsLR": "\\boldsymbol{\\Sigma_{\\parsLR}}", "optparsLR": "\\parsLR^*", "optparaLR": "\\paraLR^*", "tildecovparsLR": "\\boldsymbol{\\widetilde{\\Sigma}_{\\parsLR}}", "alphavec": "\\boldsymbol{\\alpha}", "muvec": "\\boldsymbol{\\mu}", "phivec": "\\boldsymbol{\\phi}", "betavec": "\\boldsymbol{\\beta}", "sigmavec": "\\boldsymbol{\\sigma}", "Sigmavec": "\\boldsymbol{\\Sigma}", "thetavechat": "\\widehat\\thetavec", "avec": "\\boldsymbol{a}", "Bvec": "\\boldsymbol{B}", "fvec": "\\boldsymbol{f}", "mvec": "\\boldsymbol{m}", "qvec": "\\boldsymbol{q}", "rvec": "\\boldsymbol{r}", "uvec": "\\boldsymbol{u}", "wvec": "\\boldsymbol{w}", "xvec": "\\boldsymbol{x}", "yvec": "\\boldsymbol{y}", "wt": "\\widetilde", "nb": "n_b", "mel": ["\\langle #1 | #2 | #3 \\rangle", 3], "qoi": "\\mathbf{Q}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'LearningFromData-content/BayesianStatistics/ModelSelection/ModelSelection';</script>
    <script src="../../../_static/custom.js?v=33f35b7a"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Evidence calculation for EFT expansions" href="BUQ/Evidence_for_model_EFT_coefficients.html" />
    <link rel="prev" title="12. Multi-model inference with Bayes" href="../Multimodel_inference.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../Intro/About.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/logo-copilot.png" class="logo__image only-light" alt="Combined Learning from Data materials - Home"/>
    <script>document.write(`<img src="../../../_static/logo-copilot.png" class="logo__image only-dark" alt="Combined Learning from Data materials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../Intro/About.html">
                    About this Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Intro/Invitation.html">1. Invitation to inductive inference</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Intro/Introduction.html">2. Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Intro/Introduction/sec-01-physicist-s-perspective.html">2.1. Physicistâ€™s perspective</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Intro/Introduction/sec-02-bayesian-workflow.html">2.2. Bayesian workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Intro/Introduction/sec-03-machine-learning.html">2.3. Machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Intro/Introduction/sec-04-virtues.html">2.4. Virtues</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I: Bayesian methods for scientific modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../BayesianBasics/RootBayesianBasics.html">3. Overview of Part I</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianBasics/Inferenceandpdfs.html">4. Inference and PDFs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianBasics/Inferenceandpdfs/sec-01-statements.html">4.1. Statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianBasics/Inferenceandpdfs/sec-02-manipulating-probabilities-bayesian-rules-of-probability-as.html">4.2. Manipulating probabilities: Bayesian rules of probability as principles of logic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianBasics/Inferenceandpdfs/sec-03-probability-density-functions.html">4.3. Probability density functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianBasics/Inferenceandpdfs/sec-04-summary.html">4.4. Looking ahead</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianBasics/exercise_sum_product_rule.html">4.5. Exercise: Checking the sum and product rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianBasics/exercise_medical_example_by_Bayes.html">4.6. Exercise: Standard medical example using Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianBasics/MoreBayesTheorem.html">4.7. More on Bayesâ€™ theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianBasics/Bayesian_epistemology.html">4.8. *Aside: Bayesian epistemology</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../BayesianBasics/DataModelsPredictions.html">4.9. Data, models, and predictions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../BayesianParameterEstimation/parameter_estimation_Gaussian_noise.html">ðŸ“¥ Parameter estimation I: Gaussian mean and variance</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianBasics/Posteriors.html">5. Bayesian posteriors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../BayesianBasics/Exploring_pdfs.html">5.1. ðŸ“¥ Exploring PDFs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../BayesianBasics/Exploring_pdfs_followups.html">Follow-ups to Exploring PDFs</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../BayesianBasics/Gaussians.html">5.2. Gaussians: A couple of frequentist connections</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../BayesianBasics/visualization_of_CLT.html">ðŸ“¥ Visualization of the Central Limit Theorem</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/Interpreting2Dposteriors.html">5.3. Interpreting 2D posteriors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianBasics/chi_squared_tests.html">5.4. ðŸ“¥ Demonstration: Sum of normal variables squared</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianBasics/CoinTossing.html">6. Updating via Bayes' rule</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianBasics/CoinTossing/sec-01-coin-tossing-frequentists-and-bayesaians.html">6.1. Coin tossing: Frequentists and Bayesaians</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianBasics/CoinTossing/sec-02-when-do-priors-matter-when-don-t-they-matter.html">6.2. When do priors matter? When donâ€™t they matter?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianBasics/CoinTossing/sec-03-computing-the-posterior-analytically.html">6.3. Computing the posterior analytically</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianBasics/CoinTossing/sec-04-degree-of-belief-credibility-intervals-vs-frequentist-1-sigm.html">6.4. Degree of belief/credibility intervals vs frequentist 1-sigma intervals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianBasics/CoinTossing/sec-05-take-aways-and-follow-up-questions-from-coin-flipping.html">6.5. Take-aways and follow-up questions from coin flipping:</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianBasics/demo-BayesianBasics.html">6.6. ðŸ“¥ Demonstration:  Bayesian Coin Tossing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianBasics/Bayesian_updating_coinflip_interactive.html">6.7. ðŸ“¥ Demonstration: Coin tossing (with widget)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianBasics/UsingBayes.html">7. Bayes in practice</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianBasics/BayesianAdvantages.html">7.1. Advantages of the Bayesian approach</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianWorkflow/BayesianWorkflow.html">7.2. Bayesian research workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianLinearRegression/BayesianLinearRegression_rjf.html">7.3. Bayesian Linear Regression (BLR)</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II: Advanced Bayesian methods</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../RootAdvancedMethods.html">8. Overview of Part II</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BayesianParameterEstimation/ParameterEstimation.html">9. More Bayesian parameter estimation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/radioactive_lighthouse_exercise.html">9.1. ðŸ“¥ Radioactive lighthouse problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/amplitude_in_presence_of_background.html">9.2. ðŸ“¥ Amplitude of a signal in the presence of background</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/parameter_estimation_fitting_straight_line_I.html">9.3. Parameter estimation example: fitting a straight line</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/parameter_estimation_fitting_straight_line_II.html">9.4. ðŸ“¥ Parameter estimation example: fitting a straight line II</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BayesianParameterEstimation/dealing_with_outliers.html">9.5. ðŸ“¥ Dealing with outliers</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../AssigningProbabilities/Assigning.html">10. Assigning probabilities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../AssigningProbabilities/IgnorancePDF.html">10.1. Assigning probabilities (I): Indifferences and translation groups</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../AssigningProbabilities/demo-straight_lines.html">Alternative notebook with MCMC sampling</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../AssigningProbabilities/MaxEnt2.html">10.2. Assigning probabilities (II): The principle of maximum entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../AssigningProbabilities/MaxEnt_Function_Reconstruction.html">10.3. ðŸ“¥ Maximum Entropy for reconstructing a function from its moments</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../ComputationalBayes/BayesLinear.html">11. Bayes goes linear: History matching</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../Multimodel_inference.html">12. Multi-model inference with Bayes</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active has-children"><a class="current reference internal" href="#">12.1. Model Selection</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="BUQ/Evidence_for_model_EFT_coefficients.html">Evidence calculation for EFT expansions</a></li>
<li class="toctree-l3"><a class="reference internal" href="BUQ/two_model_evidence.html">Follow-up to EFT evidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="BUQ/computing_evidence.html">Computing the evidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="BUQ/MCMC-parallel-tempering_ptemcee_vs_zeus.html">Demo: Multimodal distributions with two samplers</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../ModelMixing/model_mixing.html">12.2. Model averaging and mixing </a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/DiscrepancyModels.html">13. Discrepancy Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/MD_balldrop_v1.html">13.4. ðŸ“¥ Ball-drop experiment</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III: MCMC sampling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../StochasticProcesses/RootMCMC.html">14. Overview of Part III</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/StochasticProcesses.html">15. Stochastic processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/Metropolis_Poisson_example.html">15.7. Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/demo-MCMC.html">15.8. Demonstration: Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/Recap_BUQ.html">15.9. Recap of Poisson and more about MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/parameter_estimation_Gaussian_noise-2.html">15.10. Parameter estimation example: Gaussian noise and averages II</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/MCMC-random-walk-and-sampling.html">15.11. Exercise: Random walk</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/MCMC_overview.html">16. Overview of Markov Chain Monte Carlo</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/MarkovChains.html">16.1. Markov chains</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/MCMC.html">16.2. Markov chain Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/MCMC_intro_BUQ.html">16.3. Alternative MCMC introduction (Gregory)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/Assignment_extending_radioactive_lighthouse.html">16.4. Assignment: 2D radioactive lighthouse location using MCMC</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/Advanced_MCMC.html">17. Advanced MCMC</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ComputationalBayes/AdvancedMCMC.html">17.1. Advanced Markov chain Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/MCMC-diagnostics.html">17.2. Overview: MCMC Diagnostics</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/intuition_sampling.html">17.4. Intuition on sampling and best practices</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/Other_samplers.html">18. HMC and other samplers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../StochasticProcesses/BUQ2/HMC_intro_BUQ.html">18.1. Hamiltonian Monte Carlo (HMC) overview and visualization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../StochasticProcesses/BUQ2/Liouville_theorem_visualization.html">Liouville Theorem Visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../StochasticProcesses/BUQ2/Orbital_eqs_with_different_algorithms.html">Solving orbital equations with different algorithms</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/zeus.html">18.2. The Zeus Ensemble Slice Sampler</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../StochasticProcesses/BUQ2/PyMC_intro_updated.html">18.3. PyMC Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../StochasticProcesses/OverviewIntroPyMC.html">Overview of Intro to PyMC notebook</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ2/parameter_estimation_Gaussian_noise_compare_samplers.html">18.4. Comparing samplers for a simple problem</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV: Machine learning: A Bayesian perspective</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../MachineLearning/RootML.html">19. Overview of Part IV</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/GP/RootGP.html">20. Overview of Gaussian processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../MachineLearning/GP/GaussianProcesses.html">20.4. Introduction to Gaussian processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/CF/demo-GaussianProcesses.html">ðŸ“¥ demo-GaussianProcesses notebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/lecture_20.html">GP recap; GP applications; (old lecture 20)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../MachineLearning/GP/Sklearn_demos.html">20.5. scikit-learn demo notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/plot_gpr_noisy_targets.html">ðŸ“¥ One-dimension regression example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/plot_gpr_prior_posterior.html">ðŸ“¥ Prior and posterior with different kernels</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../MachineLearning/GP/GPy_demos.html">20.6. GPy demo notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/demo-GaussianProcesses.html">Gaussian processes demonstration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/CF/exercise_GP_GPy.html">Exercise: Gaussian processes using <code class="docutils literal notranslate"><span class="pre">GPy</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MachineLearning/GP/BUQ/Gaussian_processes_exercises.html">Exercise: Gaussian Process models with GPy</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/LogReg/LogReg.html">21. Logistic Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/MachineLearningExamples.html">21.5. Machine Learning: First Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/NeuralNet/exercises_LogReg_NeuralNet.html">21.6. Exercise: Logistic Regression and neural networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/ANN/MachineLearning.html">22. Machine learning: Overview and notation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/NeuralNet.html">22.5. Artifical neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/NeuralNet/demo-NeuralNet.html">22.6. Demonstration: Neural network classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/Neural_Network_for_simple_function_in_PyTorch.html">22.7. ðŸ“¥ ANN from ChatGPT using PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/ModelValidation.html">22.8. Model validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/DataBiasFairness.html">22.9. Data bias and fairness in machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/ANN/NeuralNet/NeuralNetBackProp.html">22.10. *Neural networks: Backpropagation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/ANNFT.html">23. ANNs in the large-width limit (ANNFT)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/random_initialized_ANN_vs_width.html">23.3. ðŸ“¥ Distributions of Randomly-Initialized ANNs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/BNN/bnn.html">24. Bayesian neural nets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/BNN/demo-bnn.html">24.4. Demonstration: Variational Inference and Bayesian Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/BNN/exercises_BNN.html">24.5. Exercise: Bayesian neural networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../MachineLearning/CNN/cnn.html">25. *Convolutional neural nets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../MachineLearning/CNN/demo-cnn.html">25.6. Demonstration: Image recognition with Convolutional Neural Networks</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V: Other topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../OtherTopics/RootOtherTopics.html">26. Overview of Part V </a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/Emulators.html">27. Emulators</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ComputationalBayes/BayesFast.html">27.1. Bayes goes fast: Emulators (from CF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ComputationalBayes/extra_RBM_emulators.html">27.2. RBM emulators (BUQ)</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../OtherTopics/Student_t_distribution_from_Gaussians.html">28. ðŸ“¥ Student t distribution from Gaussians</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/SVD.html">29. PCA, SVD, and all that</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/linear_algebra_games_including_SVD.html">29.5. ðŸ“¥ demo-SVD notebook</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../OtherTopics/qbism.html">30. QBism: Bayesian quantum mechanics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Backmatter</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Backmatter/bibliography.html">31. Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Backmatter/JB_tests.html">32. Guide to Jupyter Book markdown</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix A: Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Reference/Statistics.html">33. Statistics concepts and notation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/GradientDescent.html">34. Gradient-descent optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix B: Scientific modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/RootScientificModeling.html">35. Overview of scientific modeling material</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling.html">36. Overview of modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-01-notation.html">36.1. Notation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-02-models-in-science.html">36.2. Models in science</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-03-parametric-versus-non-parametric-models.html">36.3. Parametric versus non-parametric models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-04-linear-versus-non-linear-models.html">36.4. Linear versus non-linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-05-regression-analysis-optimization-versus-inference.html">36.5. Regression analysis: optimization versus inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-06-exercises.html">36.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-07-solutions.html">36.7. Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ModelingOptimization/LinearModels.html">37. Linear models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/LinearModels/sec-01-definition-of-linear-models.html">37.1. Definition of linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/LinearModels/sec-02-regression-analysis-with-linear-models.html">37.2. Regression analysis with linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/LinearModels/sec-03-ordinary-linear-regression-warmup.html">37.3. Ordinary linear regression: warmup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/LinearModels/sec-04-ordinary-linear-regression-in-practice.html">37.4. Ordinary linear regression in practice</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/LinearModels/sec-05-solutions.html">37.5. Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ModelingOptimization/MathematicalOptimization.html">38. Mathematical optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/MathematicalOptimization/sec-01-gradient-descent-optimization.html">38.1. Gradient-descent optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/MathematicalOptimization/sec-02-batch-stochastic-and-mini-batch-gradient-descent.html">38.2. Batch, stochastic and mini-batch gradient descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/MathematicalOptimization/sec-03-adaptive-gradient-descent-algorithms.html">38.3. Adaptive gradient descent algorithms</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix C: Getting started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Setup/RootGettingStarted.html">39. Overview of Getting started material</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Setup/exercise_Intro_01_Jupyter_Python.html">40. ðŸ“¥ Exercise: Jupyter notebooks and Python</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Setup/more_python_and_jupyter.html">41. More about Python and Jupyter notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/exercise_Intro_02_Jupyter_Python.html">41.4. ðŸ“¥ Python lists and iterations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/exercise_Intro_03_Numpy.html">41.5. ðŸ“¥ Linear algebra operations with NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/demo-Intro.html">41.6. ðŸ“¥ Reading data and fitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/Simple_widgets_v1.html">41.7. ðŸ“¥ Making a simple widget-based UI</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Setup/setting_up.html">42. Setting up for using this Jupyter Book</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/installing_anaconda.html">42.1. Using Anaconda</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/using_github.html">42.2. Using GitHub</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">TALENT mini-projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/RootMiniProjects.html">Overview of mini-projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/mini-project_I_toy_model_of_EFT.html">ðŸ“¥ MP I: Parameter estimation for a toy model of an EFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/model-selection_mini-project-IIa.html">ðŸ“¥ MP IIa: Model selection basics</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Mini-projects/model-selection_mini-project-IIb_How_many_lines_ptemcee.html">ðŸ“¥ MP IIb: How many lines?</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Mini-projects/Mini-project_IIb_overview.html">Overview of Mini-project IIb: How many lines?</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/mini-project_IIIa_bayesian_optimization.html">ðŸ“¥ MP IIIa: Bayesian optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/mini-project_IIIb_Bayesian_neural_networks_from_demo.html">ðŸ“¥ MP IIIb: Bayesian Neural Networks</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/NuclearTalent/LFD_for_Physicists" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/NuclearTalent/LFD_for_Physicists/issues/new?title=Issue%20on%20page%20%2FLearningFromData-content/BayesianStatistics/ModelSelection/ModelSelection.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/LearningFromData-content/BayesianStatistics/ModelSelection/ModelSelection.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Model Selection</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frequentist-hypothesis-testing">Frequentist hypothesis testing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-idea">Basic idea</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-testing-with-the-chi-squared-statistic">Hypothesis testing with the chi-squared statistic</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-model-selection">Bayesian model selection</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-story-of-dr-a-and-prof-b">The Story of Dr. A and Prof. B</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#one-adjustable-parameter-each">One adjustable parameter each</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#one-adjustable-parameter-each-different-prior-ranges">One adjustable parameter each; different prior ranges</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-with-parameter-estimation">Comparison with parameter estimation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evidence-calculations">Evidence calculations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#laplaces-method">Laplaceâ€™s method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization-of-a-multivariate-gaussian">Normalization of a multivariate Gaussian</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#correlations">Correlations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#two-independent-parameters">Two independent parameters</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#two-dependent-parameters">Two dependent parameters</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="model-selection">
<span id="sec-modelselection"></span><h1><span class="section-number">12.1. </span>Model Selection<a class="headerlink" href="#model-selection" title="Link to this heading">#</a></h1>
<!-- !split -->
<p>So far, we have been concerned with the problem of parameter estimation. In studying the linear relationship between two quantities, for example, we discussed how to infer the slope and the offset of the associated straight-line model. Often, however, there is a question as to whether another functional form (such as quadratic or cubic) might be a more appropriate model. In this lecture, we will consider the broad class of scientific problems when there is uncertainty as to which one of a set of alternative models is most suitable. In the Bayesian terminology these can be labeled as <strong>Model Selection</strong> problems and we will discuss them in some depth.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Good references for model selection are Sivia, chapter 4 <span id="id1">[<a class="reference internal" href="../../Backmatter/bibliography.html#id7" title="D. S. Sivia and J. Skilling. Data Analysis - A Bayesian Tutorial. Oxford Science Publications. Oxford University Press, 2nd edition, 2006.">SS06</a>]</span>, <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0022249699912786"><em>Bayesian Model Selection and Model Averaging</em></a> by Wasserman, and chapter 7 of BDA3 <span id="id2">[<a class="reference internal" href="../../Backmatter/bibliography.html#id2" title="A. Gelman, J.B. Carlin, H.S. Stern, D.B. Dunson, A. Vehtari, and D.B. Rubin. Bayesian Data Analysis, Third Edition. Chapman &amp; Hall/CRC Texts in Statistical Science. Taylor &amp; Francis, 2013. URL: http://www.stat.columbia.edu/~gelman/book/BDA3.pdf.">GCS+13</a>]</span>.</p>
</div>
<!-- !split -->
<p>One of the main objectives in science is that of inferring the truth of one or more hypotheses about how some aspect of nature works. Because we are always in a state of incomplete information, we can never prove any hypothesis (theory) is true.</p>
<!-- !split -->
<p>We will start, however, with a brief discussion on sampling theory and the frequentist approach to <strong>hypothesis testing</strong>. This will involve the introduction of the <span class="math notranslate nohighlight">\(P\)</span>-value or significance measureâ€”quantities that are often misinterpreted even by scientists themselves. See, for example, the following comment published in Nature (March 20, 2019): <a class="reference external" href="https://www.nature.com/articles/d41586-019-00857-9">Scientists rise up against statistical significance</a>.</p>
<!-- !split -->
<section id="frequentist-hypothesis-testing">
<h2>Frequentist hypothesis testing<a class="headerlink" href="#frequentist-hypothesis-testing" title="Link to this heading">#</a></h2>
<p>Recall that in frequentist statistics, probability statements are restricted to random variables. A hypothesis can not be considered a random variable, and therefore we are restricted to a much more indirect approach when trying to infer its truth, or rather when attempting to falsify it.</p>
<!-- !split -->
<section id="basic-idea">
<h3>Basic idea<a class="headerlink" href="#basic-idea" title="Link to this heading">#</a></h3>
<p>The standard sampling theory approach to hypothesis testing is to construct a statistical test. The basic idea is the following:</p>
<p><em>Frequentist hypothesis testing.</em>
The sampling theory hypothesis test is designed to compare a selected statistic from the measured data with expected results from a very large number of hypothetical repeated measurements under the assumption that a chosen null hypothesis (<span class="math notranslate nohighlight">\(\mathcal{H}_0\)</span>) is true.</p>
<!-- !split -->
<ul class="simple">
<li><p>The null hypothesis is accepted or rejected purely on the basis of how unexpected the data were to <span class="math notranslate nohighlight">\(\mathcal{H}_0\)</span>, not on how much better the alternative hypothesis (<span class="math notranslate nohighlight">\(\mathcal{H}_A\)</span>) predicted the data.</p></li>
</ul>
<!-- !bpop -->
<ul class="simple">
<li><p>The degree of â€˜â€˜unexpectednessâ€™â€™ is based on a statistic, such as the sample mean or the <span class="math notranslate nohighlight">\(\chi^2\)</span> statistic.</p></li>
<li><p>The statistic is a random variable and it is chosen so that its distribution can be easily computed given the truth of the null hypothesis. In other words, this is the distribution of the chosen statistic for a very large number of hypothetical repeated measurements under the assumption that the null hypothesis is true.</p></li>
<li><p>This statistic is then computed for the observed data set and its value is compared with the distribution that is associated with the truth of the null hypothesis.</p></li>
<li><p>If the statistic from the observed data falls in a very unlikely spot on this distribution (the threshold is to be defined beforehand) we choose to reject the null hypothesis at some confidence level on the basis of the measured data set.</p></li>
</ul>
<!-- !epop -->
<!-- !split -->
<section id="hypothesis-testing-with-the-chi-squared-statistic">
<h4>Hypothesis testing with the chi-squared statistic<a class="headerlink" href="#hypothesis-testing-with-the-chi-squared-statistic" title="Link to this heading">#</a></h4>
<p>A very common statistic to use is the <span class="math notranslate nohighlight">\(\chi^2\)</span> measure. A good example is found in Gregory, ch 7.2.1, with the measurements of flux density from a distant galaxy over a period of 6000 days. The main steps of the presented analysis are the following:</p>
<ul class="simple">
<li><p>Choose as a null hypothesis that the galaxy has an unknown, but constant, flux density. If we can reject this hypothesis at e.g. the 95% confidence level, then this provides indirect evidence(?) for the alternative hypothesis that the radio emission is variable.</p></li>
<li><p>In this example, it is assumed that the measurement errors are independent and identically distributed (<strong>iid</strong>) according to a normal distribution with a fixed standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> that is known beforehand.</p></li>
<li><p>The <span class="math notranslate nohighlight">\(\chi^2\)</span> statistic from the data set is evaluated (<span class="math notranslate nohighlight">\(x_i\)</span> is the data and <span class="math notranslate nohighlight">\(\bar{x}\)</span> is the average from the sample)</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-c4ae8923-b511-4c37-bfa5-34122f172ddc">
<span class="eqno">(12.1)<a class="headerlink" href="#equation-c4ae8923-b511-4c37-bfa5-34122f172ddc" title="Permalink to this equation">#</a></span>\[\begin{equation}

\chi^2 = \sum_{i=1}^n \frac{(x_i - \bar{x})^2}{\sigma^2}.

\end{equation}\]</div>
<!-- !split -->
<ul class="simple">
<li><p>In our example we had 15 data points, but we are using them first to estimate the mean <span class="math notranslate nohighlight">\(\mu\)</span>. Therefore, we lose one degree of freedom and are left with 14. This number will determine the form of the <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution that will be used for comparison with our actual <span class="math notranslate nohighlight">\(\chi^2\)</span> statistic.</p></li>
<li><p>The question of how unlikely is this value of <span class="math notranslate nohighlight">\(\chi^2\)</span> is by convention interpreted in terms of the area in the tail of the <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution beyond this line. This is called the <span class="math notranslate nohighlight">\(P\)</span>-value or significance.</p></li>
<li><p>In some cases, a two-sided statistic should be considered instead.</p></li>
</ul>
<!-- !split -->
<!-- ![<p><em>The $\chi^2$ distribution for 14 degrees of freedom. The value computed from the measurements of flux density from a galaxy is indicated by a vertical line. The shaded area is the $P$-value. It is 2% in this particular example so we would reject the null hypothesis with 98% confidence. (Gregory, Fig. 7.2)</em></p>](./figs/gregory_7_2.png) -->
<figure class="align-default" id="fig-gregory-7-2">
<img alt="../../../_images/gregory_7_2.png" src="../../../_images/gregory_7_2.png" />
<figcaption>
<p><span class="caption-number">Fig. 12.1 </span><span class="caption-text">The <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution for 14 degrees of freedom. The value computed from the measurements of flux density from a galaxy is indicated by a vertical line. The shaded area is the <span class="math notranslate nohighlight">\(P\)</span>-value. It is 2% in this particular example so we would reject the null hypothesis with 98% confidence. (Gregory <span id="id3">[<a class="reference internal" href="../../Backmatter/bibliography.html#id22" title="Phil Gregory. Bayesian Logical Data Analysis for the Physical Sciences: A Comparative Approach with MathematicaÂ® Support. Cambridge University Press, 2005. doi:10.1017/CBO9780511791277.">Gre05</a>]</span>, Fig. 7.2)</span><a class="headerlink" href="#fig-gregory-7-2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<!-- !split -->
<p>At the point of performing this comparison, and making a final statement, the sampling theory school divides itself into two camps:</p>
<!-- !bpop -->
<ol class="arabic simple">
<li><p>One camp uses the following protocol: first, before looking at the data, pick the significance level of the test (e.g. 5%), and determine the critical value of <span class="math notranslate nohighlight">\(\chi^2\)</span> above which the null hypothesis will be rejected. (The significance level is the fraction of times that the statistic <span class="math notranslate nohighlight">\(\chi^2\)</span> would exceed the critical value, if the null hypothesis were true.) Then, compare the actual <span class="math notranslate nohighlight">\(\chi^2\)</span> with the critical value, and declare the outcome of the test, and its significance level (which was fixed beforehand).</p></li>
<li><p>The second camp looks at the data, finds <span class="math notranslate nohighlight">\(\chi^2\)</span>, then looks in the table of <span class="math notranslate nohighlight">\(\chi^2\)</span>-distributions for the significance level, <span class="math notranslate nohighlight">\(P\)</span>, for which the observed value of <span class="math notranslate nohighlight">\(\chi^2\)</span> would be the critical value. The result of the test is then reported by giving this value of <span class="math notranslate nohighlight">\(p\)</span>, which is the fraction of times that a result as extreme as the one observed, or more extreme, would be expected to arise if the null hypothesis were true.</p></li>
</ol>
<!-- !epop -->
<!-- !split -->
</section>
</section>
</section>
<section id="bayesian-model-selection">
<h2>Bayesian model selection<a class="headerlink" href="#bayesian-model-selection" title="Link to this heading">#</a></h2>
<!-- !split -->
<!-- ======= The Story of Dr. A and Prof. B ======= -->
<section id="the-story-of-dr-a-and-prof-b">
<h3>The Story of Dr. A and Prof. B<a class="headerlink" href="#the-story-of-dr-a-and-prof-b" title="Link to this heading">#</a></h3>
<p>[Reproduced, with some modifications, from Sivia, 2006].</p>
<blockquote>
<div><p><em>Dr. A has a theory; Prof. B also has a theory, but with an adjustable parameter <span class="math notranslate nohighlight">\(\lambda\)</span>. Whose theory should we prefer on the basis of data <span class="math notranslate nohighlight">\(D\)</span>?</em>â€” Jefferys (1939), Gull (1988), Sivia (2006)</p>
</div></blockquote>
<!-- !split -->
<p>It is clear that we need to evaluate the posterior probabilities for A and B being correct to ascertain the relative merit of the two theories. If the ratio of the posterior probabilities,</p>
<div class="math notranslate nohighlight" id="equation-eq-sivia-41">
<span class="eqno">(12.2)<a class="headerlink" href="#equation-eq-sivia-41" title="Link to this equation">#</a></span>\[
\text{posterior ratio} = \frac{p(A |D, I )}{p(B|D,I)}
\]</div>
<p>is very much greater than one, then we will prefer Aâ€™s theory; if it is very much less than one, then we prefer that of B; and if it is of order unity, then the current data is insufficient to make an informed judgement.</p>
<p>To estimate the odds, let us start by applying Bayesâ€™ theorem to both the numerator and the denominator; this gives</p>
<div class="math notranslate nohighlight" id="equation-eq-sivia-42">
<span class="eqno">(12.3)<a class="headerlink" href="#equation-eq-sivia-42" title="Link to this equation">#</a></span>\[
\frac{p(A|D,I)}{p(B|D,I)} = \frac{p(D|A,I) p(A|I)}{p(D|B,I) p(B|I)}
\]</div>
<p>because the term <span class="math notranslate nohighlight">\(p(D|I)\)</span> cancels out, top and bottom. As usual, probability theory warns us immediately that the answer to our question depends partly on what we thought about the two theories before the analysis of the data. To be fair, we might take the ratio of the prior terms, on the far right of Eq. <a class="reference internal" href="#equation-eq-sivia-42">(12.3)</a>, to be unity; a harsher assignment could be based on the past track records of the theorists! To assign the probabilities involving the experimental measurements, <span class="math notranslate nohighlight">\(p(D|A,I)\)</span> and <span class="math notranslate nohighlight">\(p(D|B,I)\)</span>, we need to be able to compare the data with the predictions of A and B: the larger the mismatch, the lower the corresponding probability. This calculation is straightforward for Dr A, but not for Prof B; the latter cannot make predictions without a value for <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>To circumvent this difficulty, we can use the sum and product rule to relate the probability we require to other pdfs which might be easier to assign. In particular, marginalization and the product rule allow us to express <span class="math notranslate nohighlight">\(p(D | B , I )\)</span> as</p>
<div class="math notranslate nohighlight" id="equation-eq-sivia-43">
<span class="eqno">(12.4)<a class="headerlink" href="#equation-eq-sivia-43" title="Link to this equation">#</a></span>\[
p(D|B,I) = \int d\lambda p(D,\lambda|B,I) = 
\int d\lambda p(D|\lambda,B,I) p(\lambda|B,I). 
\]</div>
<p>The first term in the integral <span class="math notranslate nohighlight">\(p(D | \lambda, B , I )\)</span>, where the value of <span class="math notranslate nohighlight">\(\lambda\)</span> is given, is now just an ordinary likelihood function; as such, it is on a par with <span class="math notranslate nohighlight">\(p(D|A,I)\)</span>. The second term is Bâ€™s prior pdf for <span class="math notranslate nohighlight">\(\lambda\)</span>; the onus is, therefore, on the theorist to articulate his or her state of knowledge, or ignorance, before getting access to the data.</p>
<p>To proceed further analytically, let us make some simplifying approximations. Assume that, a priori, Prof B is only prepared to say that <span class="math notranslate nohighlight">\(\lambda\)</span> must lie between the limits <span class="math notranslate nohighlight">\(\lambda_\mathrm{min}\)</span> and <span class="math notranslate nohighlight">\(\lambda_\mathrm{max}\)</span>; we can then naively assign a uniform prior within this range:</p>
<div class="math notranslate nohighlight" id="equation-eq-sivia-44">
<span class="eqno">(12.5)<a class="headerlink" href="#equation-eq-sivia-44" title="Link to this equation">#</a></span>\[
p(\lambda|B,I) = \frac{1}{\lambda_\mathrm{max}-\lambda_\mathrm{min}} \quad \text{for } \lambda_\mathrm{min} \leq \lambda \leq \lambda_\mathrm{max}, 
\]</div>
<p>and zero otherwise. Let us also take it that there is a value <span class="math notranslate nohighlight">\(\lambda_0\)</span> which yields the closest agreement with the measurements; the corresponding probability <span class="math notranslate nohighlight">\(p(D|\lambda_0,B,I)\)</span> will be the maximum of Bâ€™s likelihood function. As long as this adjustable parameter lies in the neighbourhood of the optimal value, <span class="math notranslate nohighlight">\(\lambda_0 \pm (\delta\lambda)\)</span>, we would expect a reasonable fit to the data; this can be represented by the Gaussian pdf</p>
<div class="math notranslate nohighlight" id="equation-eq-sivia-45">
<span class="eqno">(12.6)<a class="headerlink" href="#equation-eq-sivia-45" title="Link to this equation">#</a></span>\[
p(D|\lambda,B,I) = p(D|\lambda_0,B,I) \exp \left[ âˆ’ \frac{(\lambdaâˆ’\lambda_0)^2}{2(\delta\lambda)^2} \right]. 
\]</div>
<p>The assignments of the prior <a class="reference internal" href="#equation-eq-sivia-44">(12.5)</a> and the likelihood <a class="reference internal" href="#equation-eq-sivia-45">(12.6)</a> are illustrated in the figure below. We may note that, unlike the prior pdf <span class="math notranslate nohighlight">\(p(\lambda|B,I)\)</span>, Bâ€™s likelihood function need not be normalized with respect to <span class="math notranslate nohighlight">\(\lambda\)</span>; in other words, <span class="math notranslate nohighlight">\(p(D|\lambda_0,B,I)\)</span> need not equal <span class="math notranslate nohighlight">\(1/ (\delta\lambda) \sqrt{2\pi}\)</span> . This is because the <span class="math notranslate nohighlight">\(\lambda\)</span> in <span class="math notranslate nohighlight">\(p(D|\lambda,B,I)\)</span> appears in the conditioning statement, whereas the normalization requirement applies to quantities to the left of the â€˜|â€™ symbol.</p>
<p><img alt="A schematic representation of the prior pdf (dashed line) and the likelihood function (solid line) for the parameter  in Prof Bâ€™s theory. " src="../../../_images/fig41.png" /></p>
<p>In the evaluation of <span class="math notranslate nohighlight">\(p(D | B , I )\)</span>, we can make use of the fact that the prior does not depend explicitly on <span class="math notranslate nohighlight">\(\lambda\)</span>; this enables us to take <span class="math notranslate nohighlight">\(p(\lambda|B,I)\)</span> outside the integral in Eq. <a class="reference internal" href="#equation-eq-sivia-43">(12.4)</a>.</p>
<div class="math notranslate nohighlight" id="equation-eq-sivia-46">
<span class="eqno">(12.7)<a class="headerlink" href="#equation-eq-sivia-46" title="Link to this equation">#</a></span>\[
p(D|B,I) = \frac{1}{\lambda_\mathrm{max} - \lambda_\mathrm{min}} \int_{\lambda_\mathrm{min}}^{\lambda_\mathrm{max}} d\lambda
p(D|\lambda,B,I),
\]</div>
<p>having set the limits according to the specified range. Assuming that the sharp cut-offs
do not cause a significant truncation of the Gaussian pdf of the likelihood,
its integral will be equal to <span class="math notranslate nohighlight">\((\delta\lambda) \sqrt{2\pi}\)</span> times <span class="math notranslate nohighlight">\(p(D|\lambda_0,B,I)\)</span>. The troublesome term then reduces to</p>
<div class="amsmath math notranslate nohighlight" id="equation-263666df-09c4-450a-8c31-e87705468619">
<span class="eqno">(12.8)<a class="headerlink" href="#equation-263666df-09c4-450a-8c31-e87705468619" title="Permalink to this equation">#</a></span>\[\begin{equation}
p(D|B,I) = \frac{1}{\lambda_\mathrm{max} - \lambda_\mathrm{min}} p(D|\lambda_0,B,I) (\delta\lambda) \sqrt{2\pi}.
\end{equation}\]</div>
<p>Substituting this into Eq. <a class="reference internal" href="#equation-eq-sivia-42">(12.3)</a>, we finally see that the ratio of the posteriors required to answer our original question decomposes into the product of three terms:</p>
<div class="math notranslate nohighlight" id="equation-eq-sivia-48">
<span class="eqno">(12.9)<a class="headerlink" href="#equation-eq-sivia-48" title="Link to this equation">#</a></span>\[
\frac{p(A|D,I)}{p(B|D,I)} =  \frac{p(A|I)}{p(B|I)} \times \frac{p(D|A,I)}{p(D|\lambda_0,B,I)} \times \frac{\lambda_\mathrm{max} - \lambda_\mathrm{min}}{(\delta\lambda) \sqrt{2\pi}}. 
\]</div>
<p>The first term on the right-hand side reflects our relative prior preference for the alternative theories; to be fair, let us take it to be unity. The second term is a measure of how well the best predictions from each of the models agree with the data; with the added flexibility of his adjustable parameter, this maximum likelihood ratio can only favour B. The goodness-of-fit, however, cannot be the only thing that matters; if it was, we would always prefer more complicated explanations. Probability theory tells us that there is, indeed, another term to be considered. As assumed earlier in the evaluation of the marginal integral of Eq. <a class="reference internal" href="#equation-eq-sivia-43">(12.4)</a>, the prior range <span class="math notranslate nohighlight">\(\lambda_\mathrm{max} - \lambda_\mathrm{min}\)</span> will generally be much larger than the uncertainty <span class="math notranslate nohighlight">\(\pm(\delta\lambda)\)</span> permitted by the data. As such, the final term in Eq. <a class="reference internal" href="#equation-eq-sivia-48">(12.9)</a> acts to penalize B for the additional parameter; for this reason, it is often called an Ockham factor. That is to say, we have naturally encompassed the spirit of Ockhamâ€™s Razor: <em>â€˜Frustra fit per plura quod potest fieri per paucioraâ€™</em> or, in English, <em>â€˜it is vain to do with more what can be done with fewerâ€™</em> .</p>
<p>Although it is satisfying to quantify the everyday guiding principle attributed to the thirteenth-century Franciscan monk William of Ockham (or Occam, in Latin), that we should prefer the simplest theory which agrees with the empirical evidence, we should not get too carried away by it. After all, what do we mean by the simpler theory if alternative models have the same number of adjustable parameters? In the choice between Gaussian and Lorentzian peak shapes, for example, both are defined by the position of the maximum and their width. All that we are obliged to do, and have done, in addressing such questions is to adhere to the rules of probability.</p>
<p>While accepting the clear logic leading to Eq <a class="reference internal" href="#equation-eq-sivia-48">(12.9)</a>, many people rightly worry about the question of the limits <span class="math notranslate nohighlight">\(\lambda_\mathrm{min}\)</span> and <span class="math notranslate nohighlight">\(\lambda_\mathrm{max}\)</span>. Jeffreys (1939) himself was concerned and pointed out that there would be an infinite penalty for any new parameter if the range was allowed to go to <span class="math notranslate nohighlight">\(\pm\infty\)</span>. Stated in the abstract, this would appear to be a severe limitation. In practice, however, it is not generally such a problem: since the analysis is always used in specific contexts, a suitable choice can usually be made on the basis of the relevant background information. Even in uncharted territory, a small amount of thought soon reveals that our state of ignorance is always far from the <span class="math notranslate nohighlight">\(\pm\infty\)</span> scenario. If <span class="math notranslate nohighlight">\(\lambda\)</span> was the coupling constant (or strength) for a possible fifth force, for example, then we could put an upper bound on its magnitude because everybody would have noticed it by now if it had been large enough! We should also not lose sight of the fact that the precise form of Eq <a class="reference internal" href="#equation-eq-sivia-48">(12.9)</a> stems from our stated simplifying approximations; if these are not appropriate, then Eqs. <a class="reference internal" href="#equation-eq-sivia-42">(12.3)</a> and <a class="reference internal" href="#equation-eq-sivia-43">(12.4)</a> will lead us to a somewhat different formula.</p>
<p>In most cases, our relative preference for A or B is dominated by the goodness of the fit to the data; that is to say, the maximum likelihood ratio in eqn <a class="reference internal" href="#equation-eq-sivia-48">(12.9)</a> tends to overwhelm the contributions of the other two terms. The Ockham factor can play a crucial role, however, when both theories give comparably good agreement with the measurements. Indeed, it becomes increasingly important if Bâ€™s theory fails to give a significantly better fit as the quality of the data improves. In that case, <span class="math notranslate nohighlight">\((\delta\lambda)\)</span> continues to become smaller but the ratio of best-fit likelihoods remains close to unity; according to Eq. <a class="reference internal" href="#equation-eq-sivia-48">(12.9)</a>, therefore, Aâ€™s theory is favoured ever more strongly. By the same token, the Ockham effect disappears if the data are either few in number, of poor quality or just fail to shed new light on the problem at hand. This is simply because the posterior ratio of Eq. <a class="reference internal" href="#equation-eq-sivia-48">(12.9)</a> is then roughly equal to the complementary prior one, since the empirical evidence is very weak; hence, there is no inherent preference for Aâ€™s theory unless it is explicitly encoded in <span class="math notranslate nohighlight">\(p(A|I)/p(B|I)\)</span>. This property can be verified formally by going back to Eqs. <a class="reference internal" href="#equation-eq-sivia-42">(12.3)</a>, <a class="reference internal" href="#equation-eq-sivia-45">(12.6)</a> and <a class="reference internal" href="#equation-eq-sivia-46">(12.7)</a>, and considering the poor-data limit in which
<span class="math notranslate nohighlight">\((\delta\lambda) \gg \lambda_\mathrm{max}-\lambda_\mathrm{min}\)</span> and <span class="math notranslate nohighlight">\(p(D|\lambda_0,B,I) \approx p(D|A,I)\)</span>.</p>
<!-- !split -->
<section id="one-adjustable-parameter-each">
<h4>One adjustable parameter each<a class="headerlink" href="#one-adjustable-parameter-each" title="Link to this heading">#</a></h4>
<p>Some further interesting features arise when we consider the case where Dr A also has one adjustable parameter; call it <span class="math notranslate nohighlight">\(\mu\)</span>. If we make the same sort of probability assignments, and simplifying approximations, as for Prof B, then we find that</p>
<div class="math notranslate nohighlight" id="equation-eq-sivia-49">
<span class="eqno">(12.10)<a class="headerlink" href="#equation-eq-sivia-49" title="Link to this equation">#</a></span>\[
\frac{p(A|D,I)}{p(B|D,I)} =  \frac{p(A|I)}{p(B|I)} \times \frac{p(D|\mu_0,A,I)}{p(D|\lambda_0,B,I)} \times \frac{\delta\mu(\lambda_\mathrm{max} - \lambda_\mathrm{min})}{\delta\lambda(\mu_\mathrm{max} - \mu_\mathrm{min})}. 
\]</div>
<!-- !split -->
<p>This could represent the situation where we have to choose between a Gaussian and Lorentzian shape for a signal peak, but one associated parameter is not known. The position of the maximum may be fixed at the origin by theory, for example, and the amplitude constrained by the normalization of the data; A and B could then be the hypotheses favouring the alternative lineshapes, where <span class="math notranslate nohighlight">\(\delta\mu\)</span> and <span class="math notranslate nohighlight">\(\delta\lambda\)</span> are their related full-width-half-maxima. If we give equal weight to A and B before the analysis, and assign a similar large prior range for both <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\lambda\)</span>, then Eq. <a class="reference internal" href="#equation-eq-sivia-49">(12.10)</a> reduces to</p>
<div class="amsmath math notranslate nohighlight" id="equation-ea28d245-2c85-4994-b302-f177ef7d8c70">
<span class="eqno">(12.11)<a class="headerlink" href="#equation-ea28d245-2c85-4994-b302-f177ef7d8c70" title="Permalink to this equation">#</a></span>\[\begin{equation}
\frac{p(A|D,I)}{p(B|D,I)} \approx  \frac{p(D|\mu_0,A,I)}{p(D|\lambda_0,B,I)} \times \frac{\delta\mu}{\delta\lambda}. 
\end{equation}\]</div>
<!-- !split -->
<p>For data of good quality, the dominant factor will tend to be the best-fit likelihood ratio. If both give comparable agreement with the measurements, however, then the shape with the larger error-bar for its associated parameter will be favoured. At first sight, it might seem rather odd that the less discriminating theory can gain the upper hand. It appears less strange once we realize that, in the context of model selection, a larger â€˜error-barâ€™ means that more parameter values are consistent with the given hypothesis; hence its preferential treatment.</p>
<!-- !split -->
</section>
<section id="one-adjustable-parameter-each-different-prior-ranges">
<h4>One adjustable parameter each; different prior ranges<a class="headerlink" href="#one-adjustable-parameter-each-different-prior-ranges" title="Link to this heading">#</a></h4>
<p>Finally, we can also consider the situation where Mr A and Mr B have the same physical theory but assign a different prior range for <span class="math notranslate nohighlight">\(\lambda\)</span> (or <span class="math notranslate nohighlight">\(\mu\)</span>). Although Eq. <a class="reference internal" href="#equation-eq-sivia-48">(12.9)</a> can be seen as representing the case when <span class="math notranslate nohighlight">\((\mu_\mathrm{max} - \mu_\mathrm{min})\)</span> is infinitesimally small, so that A has no flexibility, Eq. <a class="reference internal" href="#equation-eq-sivia-49">(12.10)</a> is more appropriate when the limits set by both theorists are large enough to encompass all the parameter values giving a reasonable fit to the data. With equal initial weighting towards A and B, the latter reduces to</p>
<div class="amsmath math notranslate nohighlight" id="equation-45391b98-a57f-4dd3-a903-ee3dc143c288">
<span class="eqno">(12.12)<a class="headerlink" href="#equation-45391b98-a57f-4dd3-a903-ee3dc143c288" title="Permalink to this equation">#</a></span>\[\begin{equation}
\frac{p(A|D,I)}{p(B|D,I)} =  \frac{\lambda_\mathrm{max} - \lambda_\mathrm{min}}{\mu_\mathrm{max} - \mu_\mathrm{min}}. 
\end{equation}\]</div>
<p>because the best-fit likelihood ratio will be unity (since <span class="math notranslate nohighlight">\(\lambda_0 = \mu_0\)</span>) and <span class="math notranslate nohighlight">\(\delta\lambda = \delta\mu\)</span>. Thus, our analysis will lead us to prefer the theorist who gives the narrower prior range; this is not unreasonable as he must have had some additional insight to be able to predict the value of the parameter more accurately.</p>
<!-- !split -->
</section>
</section>
<section id="comparison-with-parameter-estimation">
<h3>Comparison with parameter estimation<a class="headerlink" href="#comparison-with-parameter-estimation" title="Link to this heading">#</a></h3>
<p>The dependence of the result in Eq. <a class="reference internal" href="#equation-eq-sivia-48">(12.9)</a> on the prior range <span class="math notranslate nohighlight">\((\lambda_\mathrm{max} - \lambda_\mathrm{min})\)</span> can seem a little strange, since we havenâ€™t encountered such behaviour in the preceding chapters. It is instructive, therefore, to compare the model selection analysis with parameter estimation. To infer the value of <span class="math notranslate nohighlight">\(\lambda\)</span> from the data, given that Bâ€™s theory is correct, we use Bayesâ€™ theorem:</p>
<div class="math notranslate nohighlight" id="equation-eq-sivia-410">
<span class="eqno">(12.13)<a class="headerlink" href="#equation-eq-sivia-410" title="Link to this equation">#</a></span>\[
p(\lambda|D,B,I) = \frac{p(D|\lambda,B,I) p(\lambda|B,I)}{p(D|B,I)}. 
\]</div>
<p>The numerator is the familiar product of a prior and likelihood, and the denominator is usually omitted since it does not depend explicitly on <span class="math notranslate nohighlight">\(\lambda\)</span>; hence this relationship is often written as a proportionality. From the story of Dr A and Prof B, however, we find that the neglected term on the bottom plays a crucial role in ascertaining the merit of Bâ€™s theory relative to a competing alternative.</p>
<!-- !split -->
<ul class="simple">
<li><p>In recognition of its new-found importance, the denominator in Bayesâ€™ theorem is sometimes called the <strong>â€˜evidenceâ€™</strong> for B; it is also referred to as the â€˜marginal likelihoodâ€™, the â€˜global likelihoodâ€™ and the â€˜prior predictiveâ€™.</p></li>
<li><p>Since all the components necessary for both parameter estimation and model selection appear in Eq. <a class="reference internal" href="#equation-eq-sivia-410">(12.13)</a>, we are not dealing with any new principles; the only thing that sets them apart is that we are asking different questions of the data.</p></li>
</ul>
<!-- !split -->
<p>A simple way to think about the difference between parameter estimation and model selection is to note that, to a good approximation, the former requires the location of the maximum of the likelihood function whereas the latter entails the calculation of its average value. As long as <span class="math notranslate nohighlight">\(\lambda_\mathrm{min}\)</span> and <span class="math notranslate nohighlight">\(\lambda_\mathrm{max}\)</span> encompass the significant region of <span class="math notranslate nohighlight">\(p(D|\lambda,B,I)\)</span> around <span class="math notranslate nohighlight">\(\lambda_0\)</span>, the precise bounds do not matter for estimating the optimal parameter and need not be specified. Since the prior range defines the domain over which the mean likelihood is computed, due thought is necessary when dealing with model selection. Indeed, it is precisely this act of comparing â€˜averageâ€™ likelihoods rather than â€˜maximumâ€™ ones which introduces the desired Ockham balance to the goodness- of-fit criterion. Any likelihood gain from a better agreement with the data, allowed by the greater flexibility of a more complicated model, has to be weighed against the additional cost of averaging it over a larger parameter space.</p>
<!-- !split -->
</section>
</section>
<section id="evidence-calculations">
<h2>Evidence calculations<a class="headerlink" href="#evidence-calculations" title="Link to this heading">#</a></h2>
<p>The actual computation of Bayesian evidences can be a challenging task. Recall that we often have knowledge of the posterior distribution only through sampling. In many cases, the simple Laplace method can be used to compute the evidence approximately, while in other cases we have to rely on special sampling algorithms such as nested sampling or parallel tempering with thermodynamic integration.</p>
<!-- ===== Laplace's method ===== -->
<!-- !split -->
<section id="laplaces-method">
<h3>Laplaceâ€™s method<a class="headerlink" href="#laplaces-method" title="Link to this heading">#</a></h3>
<p>The idea behind Laplaceâ€™s method is simple, namely to use the Taylor expansion of a function around its peak to construct a Gaussian approximation which can be easily integrated. Let us consider an unnormalized density <span class="math notranslate nohighlight">\(P^*(\theta)\)</span> that has a peak at a point <span class="math notranslate nohighlight">\(\theta_0\)</span>.
We are interested in the evidence, <span class="math notranslate nohighlight">\(Z_P\)</span>, which is given by the integral</p>
<div class="amsmath math notranslate nohighlight" id="equation-a0723358-a5b7-4787-9704-19afdf8d16fa">
<span class="eqno">(12.14)<a class="headerlink" href="#equation-a0723358-a5b7-4787-9704-19afdf8d16fa" title="Permalink to this equation">#</a></span>\[\begin{equation}
Z_P = \int P^*(\theta) d^K\theta,
\end{equation} \]</div>
<p>where we consider the general case in which <span class="math notranslate nohighlight">\(\theta\)</span> is a <span class="math notranslate nohighlight">\(K\)</span>-dimensional vector of parameters.</p>
<!-- !split -->
<p>We Taylor-expand the logarithm <span class="math notranslate nohighlight">\(\log P^*\)</span> around the peak:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c6fe805b-c620-48a4-b8a2-9b80f37d532f">
<span class="eqno">(12.15)<a class="headerlink" href="#equation-c6fe805b-c620-48a4-b8a2-9b80f37d532f" title="Permalink to this equation">#</a></span>\[\begin{equation}
\log P^*(\theta) = \log P^*(\theta_0) - \frac{1}{2} (\theta - \theta_0)^T \Sigma^{-1} (\theta - \theta_0) + \ldots,
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Sigma^{-1} \equiv H\)</span> is the (Hessian) matrix of second derivatives at the maximum</p>
<div class="amsmath math notranslate nohighlight" id="equation-8ee8ea93-9ef4-471f-9aed-d28511a9888f">
<span class="eqno">(12.16)<a class="headerlink" href="#equation-8ee8ea93-9ef4-471f-9aed-d28511a9888f" title="Permalink to this equation">#</a></span>\[\begin{equation}
H_{ij} = - \left. \frac{\partial^2}{\partial \theta_i \partial \theta_j}  \log P^*(\theta)\right|_{\theta=\theta_0}.
\end{equation}\]</div>
<!-- !split -->
<p>By truncating the Taylor expansion at the second order we find that <span class="math notranslate nohighlight">\(P^*(\theta)\)</span>
is approximated by an unnormalized Gaussian,</p>
<div class="amsmath math notranslate nohighlight" id="equation-7d7f1c9d-c3d6-48c1-a77f-aa024d29c69c">
<span class="eqno">(12.17)<a class="headerlink" href="#equation-7d7f1c9d-c3d6-48c1-a77f-aa024d29c69c" title="Permalink to this equation">#</a></span>\[\begin{equation}
P^*(\theta) \approx P^*(\theta_0) \exp \left[ - \frac{1}{2}(\theta - \theta_0)^T \Sigma^{-1} (\theta - \theta_0) \right].
\end{equation}\]</div>
<p>The integral of this Gaussian is given by <span class="math notranslate nohighlight">\(P^*(\theta_0)\)</span> times (the inverse of) its normalization constant. Thus we approximate <span class="math notranslate nohighlight">\(Z_P\)</span> by</p>
<div class="amsmath math notranslate nohighlight" id="equation-9c2f52c3-bfad-4b12-8493-bb51dd9b2ef9">
<span class="eqno">(12.18)<a class="headerlink" href="#equation-9c2f52c3-bfad-4b12-8493-bb51dd9b2ef9" title="Permalink to this equation">#</a></span>\[\begin{equation}
Z_P \approx P^*(\theta_0) \sqrt{\frac{(2\pi)^K}{\det\Sigma^{-1}}}.
\end{equation}\]</div>
<p>Predictions can then be made using this approximation. Physicists also call this widely-used approximation the saddle-point approximation.</p>
<!-- !split -->
<p>Note, in particular, that if we consider a chi-squared likelihood: <span class="math notranslate nohighlight">\(P^*(\theta) = \mathcal{L}(D|\theta) = \exp \left( -\frac{1}{2} \chi^2(\theta)\right)\)</span>, then we get</p>
<div class="amsmath math notranslate nohighlight" id="equation-9a89d542-4bba-4c89-8d50-b40fb48b4e0e">
<span class="eqno">(12.19)<a class="headerlink" href="#equation-9a89d542-4bba-4c89-8d50-b40fb48b4e0e" title="Permalink to this equation">#</a></span>\[\begin{equation}
Z_P \approx \exp \left( -\frac{1}{2} \chi^2(\theta_0)\right) \sqrt{\frac{(4\pi)^K}{\det\Sigma^{-1}}},
\end{equation}\]</div>
<p>where there is a factor <span class="math notranslate nohighlight">\(2^{K/2}\)</span> that comes from the extra factor <span class="math notranslate nohighlight">\(1/2\)</span> multiplying the covariance matrix <span class="math notranslate nohighlight">\(\Sigma^{-1}\)</span> and therefore appearing in all <span class="math notranslate nohighlight">\(K\)</span> eigenvalues.</p>
<!-- !split -->
<p>Note that the Laplace approximation is basis-dependent: if <span class="math notranslate nohighlight">\(\theta\)</span> is transformed to a nonlinear function <span class="math notranslate nohighlight">\(u(\theta)\)</span> and the density is transformed to <span class="math notranslate nohighlight">\(P(u) = P(\theta) |d\theta/du|\)</span> then in general the approximate normalizing constants <span class="math notranslate nohighlight">\(Z_Q\)</span> will be different. This can be viewed as a defectâ€”since the true value <span class="math notranslate nohighlight">\(Z_P\)</span> is basis-independent in this approximationâ€”or an opportunity, because we can hunt for a choice of basis in which the Laplace approximation is most accurate.</p>
<!-- !split -->
</section>
<section id="normalization-of-a-multivariate-gaussian">
<h3>Normalization of a multivariate Gaussian<a class="headerlink" href="#normalization-of-a-multivariate-gaussian" title="Link to this heading">#</a></h3>
<p>The fact that the normalizing constant of a Gaussian is given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-cee2ba5c-23d6-4a4b-8557-b01a703a251c">
<span class="eqno">(12.20)<a class="headerlink" href="#equation-cee2ba5c-23d6-4a4b-8557-b01a703a251c" title="Permalink to this equation">#</a></span>\[\begin{equation}
\int d^K\theta \exp \left[ - \frac{1}{2}\theta^T \Sigma^{-1} \theta \right] = \sqrt{\frac{(2\pi)^K}{\det\Sigma^{-1}}},
\end{equation}\]</div>
<p>can be proved by making an orthogonal transformation into the basis <span class="math notranslate nohighlight">\(u\)</span> in which <span class="math notranslate nohighlight">\(\Sigma\)</span> is transformed into a diagonal matrix of eigenvalues <span class="math notranslate nohighlight">\(\lambda_i\)</span>. The integral then separates into a product of one-dimensional integrals, each of the form</p>
<div class="amsmath math notranslate nohighlight" id="equation-b2d658e9-f729-43bd-871e-de4dd35da08e">
<span class="eqno">(12.21)<a class="headerlink" href="#equation-b2d658e9-f729-43bd-871e-de4dd35da08e" title="Permalink to this equation">#</a></span>\[\begin{equation}
\int du_i \exp \left[ -\frac{1}{2} \lambda_i u_i^2 \right] = \sqrt{\frac{2\pi}{\lambda_i}}
\end{equation}\]</div>
<p>The product of the eigenvalues <span class="math notranslate nohighlight">\(\lambda_i\)</span> is the determinant of <span class="math notranslate nohighlight">\(\Sigma^{-1}\)</span>.</p>
</section>
<section id="correlations">
<h3>Correlations<a class="headerlink" href="#correlations" title="Link to this heading">#</a></h3>
<p>In the â€œfitting a straight-lineâ€ example you should find that the joint pdf for the slope and the intercept <span class="math notranslate nohighlight">\([m, b]\)</span> corresponds to a slanted ellipse. That result implies that the model parameters are (anti) <strong>correlated</strong>.</p>
<ul class="simple">
<li><p>Try to understand the correlation that you find in this example.</p></li>
</ul>
<p>Let us explore correlations by studying the behavior of a bivariate pdf near the maximum where we employ the Laplace approximation (neglecting terms beyond the quadratic one in a Taylor expansion). We start by considering two independent parameters <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, before studying the dependent case.</p>
<section id="two-independent-parameters">
<h4>Two independent parameters<a class="headerlink" href="#two-independent-parameters" title="Link to this heading">#</a></h4>
<p>Independence implies that <span class="math notranslate nohighlight">\(p(x,y) = p_x(x) p_y(y)\)</span>. We will again consider the log-pdf <span class="math notranslate nohighlight">\(L(x,y) = \log\left( p(x,y) \right)\)</span> which will then be</p>
<div class="amsmath math notranslate nohighlight" id="equation-07fc0caf-eb2f-4e2d-a3e1-eaf94f0dc6fb">
<span class="eqno">(12.22)<a class="headerlink" href="#equation-07fc0caf-eb2f-4e2d-a3e1-eaf94f0dc6fb" title="Permalink to this equation">#</a></span>\[\begin{equation}
L(x,y) = L_x(x) + L_y(y).
\end{equation}\]</div>
<p>At the mode we will have <span class="math notranslate nohighlight">\(\partial p / \partial x = \partial p_x / \partial x = \partial L_x / \partial x = 0\)</span>, and similarly <span class="math notranslate nohighlight">\(\partial L_y / \partial y = 0\)</span>.</p>
<p>The second derivatives will be</p>
<div class="amsmath math notranslate nohighlight" id="equation-4a6ef677-739d-45bd-9459-b37ba030cbc6">
<span class="eqno">(12.23)<a class="headerlink" href="#equation-4a6ef677-739d-45bd-9459-b37ba030cbc6" title="Permalink to this equation">#</a></span>\[\begin{equation}
A \equiv \left. \frac{\partial^2 L_x}{\partial x^2} \right|_{x=x_0} &lt; 0, \quad
B \equiv \left. \frac{\partial^2 L_y}{\partial y^2} \right|_{y=y_0} &lt; 0, \quad
C \equiv \left. \frac{\partial L(x,y)}{\partial x \partial y} \right|_{x=x_0,y=y_0} = 0.
\end{equation}\]</div>
<p>such that our approximated (log) pdf near the mode will be</p>
<div class="amsmath math notranslate nohighlight" id="equation-6b7ab48f-e815-4c00-a2f4-0bea4ca9b71b">
<span class="eqno">(12.24)<a class="headerlink" href="#equation-6b7ab48f-e815-4c00-a2f4-0bea4ca9b71b" title="Permalink to this equation">#</a></span>\[\begin{equation}
L(x,y) = L(x_0, y_0) - \frac{1}{2} |A| (x-x_0)^2 - \frac{1}{2} |B| (y-y_0)^2.
\end{equation}\]</div>
<p>We could visualize this bivariate pdf by plotting iso-probability contours. Or, equivalently, iso-log-probability contours which correspond to</p>
<div class="amsmath math notranslate nohighlight" id="equation-1dbc4c5e-0751-459c-862e-fa68b2f0c24b">
<span class="eqno">(12.25)<a class="headerlink" href="#equation-1dbc4c5e-0751-459c-862e-fa68b2f0c24b" title="Permalink to this equation">#</a></span>\[\begin{equation}
|A| (x-x_0)^2 + |B| (y-y_0)^2 = \mathrm{constant}.
\end{equation}\]</div>
<p>This you should recognize as the equation for an ellipse with its center in <span class="math notranslate nohighlight">\((x_0, y_0)\)</span>, the principal axes corresponding to the <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> directions, and with the width and height parameters <span class="math notranslate nohighlight">\(\sigma_x = 1/\sqrt{|A|}\)</span> and <span class="math notranslate nohighlight">\(\sigma_y = 1/\sqrt{|B|}\)</span>, respectively.</p>
</section>
<section id="two-dependent-parameters">
<h4>Two dependent parameters<a class="headerlink" href="#two-dependent-parameters" title="Link to this heading">#</a></h4>
<p>For two dependent parameters we cannot separate <span class="math notranslate nohighlight">\(p(x,y)\)</span> into a product of one-dimensional pdf:s. Instead, the Taylor expansion for the bivariate log-pdf <span class="math notranslate nohighlight">\(L(x,y)\)</span> around the mode <span class="math notranslate nohighlight">\((x_0,y_0)\)</span> gives</p>
<div class="amsmath math notranslate nohighlight" id="equation-098049fd-187b-4bf4-ade7-9d9d501f2b6b">
<span class="eqno">(12.26)<a class="headerlink" href="#equation-098049fd-187b-4bf4-ade7-9d9d501f2b6b" title="Permalink to this equation">#</a></span>\[\begin{equation}
L(x,y) \approx L(x_0,y_0) + \frac{1}{2} \begin{pmatrix} x-x_0 &amp; y-y_0 \end{pmatrix}
H
\begin{pmatrix} x-x_0 \\ y-y_0 \end{pmatrix},
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(H\)</span> is the symmetric Hessian matrix</p>
<div class="amsmath math notranslate nohighlight" id="equation-0319c00f-9f85-461b-a7d3-2b1b3e6f1b5b">
<span class="eqno">(12.27)<a class="headerlink" href="#equation-0319c00f-9f85-461b-a7d3-2b1b3e6f1b5b" title="Permalink to this equation">#</a></span>\[\begin{equation}
\begin{pmatrix}
A &amp; C \\ C &amp; B
\end{pmatrix}, 
\end{equation}\]</div>
<p>with elements</p>
<div class="amsmath math notranslate nohighlight" id="equation-6fe78e21-6fbc-44b6-ac2b-93b053d4f76a">
<span class="eqno">(12.28)<a class="headerlink" href="#equation-6fe78e21-6fbc-44b6-ac2b-93b053d4f76a" title="Permalink to this equation">#</a></span>\[\begin{equation}
A = \left. \frac{\partial^2 L}{\partial x^2} \right|_{x_0,y_0} &lt; 0, \quad
B = \left. \frac{\partial^2 L}{\partial y^2} \right|_{x_0,y_0} &lt; 0, \quad
C = \left. \frac{\partial^2 L}{\partial x \partial y} \right|_{x_0,y_0} \neq 0.
\end{equation}\]</div>
<!-- !split -->
<ul class="simple">
<li><p>So in this quadratic approximation the contour is an ellipse centered at <span class="math notranslate nohighlight">\((x_0,y_0)\)</span> with orientation and eccentricity determined by <span class="math notranslate nohighlight">\(A,B,C\)</span>.</p></li>
<li><p>The principal axes are found from the eigenvectors of <span class="math notranslate nohighlight">\(H\)</span>.</p></li>
<li><p>Depending on the skewness of the ellipse, the parameters are either (i) not correlated (<span class="math notranslate nohighlight">\(C=0\)</span>), (ii) correlated, or (iii) anti-correlated.</p></li>
<li><p>Take a minute to consider what that implies.</p></li>
</ul>
<p>Let us be explicit. The Hessian can be diagonalized (we will also change sign)</p>
<div class="amsmath math notranslate nohighlight" id="equation-e27eef1d-6339-4bee-ba14-8527a2a10d37">
<span class="eqno">(12.29)<a class="headerlink" href="#equation-e27eef1d-6339-4bee-ba14-8527a2a10d37" title="Permalink to this equation">#</a></span>\[\begin{equation}
-H = U \begin{pmatrix} a &amp; 0 \\ 0 &amp; b \end{pmatrix} U^{-1},
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span> are the (positive) eigenvalues of <span class="math notranslate nohighlight">\(-H\)</span>, and <span class="math notranslate nohighlight">\(U = \begin{pmatrix} a_x &amp; b_x \\ a_y &amp; b_y \end{pmatrix}\)</span> is constructed from the eigenvectors. Defining a new set of translated and linearly combined parameters</p>
<div class="amsmath math notranslate nohighlight" id="equation-d12d8edb-347d-4926-880c-ad2226b3f672">
<span class="eqno">(12.30)<a class="headerlink" href="#equation-d12d8edb-347d-4926-880c-ad2226b3f672" title="Permalink to this equation">#</a></span>\[\begin{equation}
x' = a_x (x - x_0) + a_y (y - y_0) \\
y' = b_x (x - x_0) + b_y (y - y_0) 
\end{equation}\]</div>
<p>we find that the log-pdf in the new coordinates becomes</p>
<div class="amsmath math notranslate nohighlight" id="equation-bf031b12-78b4-4371-82cf-a21be3e08cbd">
<span class="eqno">(12.31)<a class="headerlink" href="#equation-bf031b12-78b4-4371-82cf-a21be3e08cbd" title="Permalink to this equation">#</a></span>\[\begin{equation}
\begin{gathered}
L(x',y') &amp;= L(0,0) - \frac{1}{2} \begin{pmatrix} x' &amp; y' \end{pmatrix}
\begin{pmatrix} a &amp; 0 \\ 0 &amp; b \end{pmatrix}
\begin{pmatrix} x' \\ y' \end{pmatrix} \\
&amp;= L(0, 0) - \frac{1}{2} a (x')^2 - \frac{1}{2} b (y')^2.
\end{gathered}
\end{equation}\]</div>
<div class="admonition-discuss admonition">
<p class="admonition-title">Discuss</p>
<p>What has been achieved by this change of variables?</p>
</div>
<div class="toggle docutils container">
<p>The joint pdf now factorizes which implies that the transformed variables are independent.</p>
</div>
</section>
</section>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./LearningFromData-content/BayesianStatistics/ModelSelection"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../Multimodel_inference.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">12. </span>Multi-model inference with Bayes</p>
      </div>
    </a>
    <a class="right-next"
       href="BUQ/Evidence_for_model_EFT_coefficients.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Evidence calculation for EFT expansions</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frequentist-hypothesis-testing">Frequentist hypothesis testing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-idea">Basic idea</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-testing-with-the-chi-squared-statistic">Hypothesis testing with the chi-squared statistic</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-model-selection">Bayesian model selection</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-story-of-dr-a-and-prof-b">The Story of Dr. A and Prof. B</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#one-adjustable-parameter-each">One adjustable parameter each</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#one-adjustable-parameter-each-different-prior-ranges">One adjustable parameter each; different prior ranges</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison-with-parameter-estimation">Comparison with parameter estimation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evidence-calculations">Evidence calculations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#laplaces-method">Laplaceâ€™s method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization-of-a-multivariate-gaussian">Normalization of a multivariate Gaussian</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#correlations">Correlations</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#two-independent-parameters">Two independent parameters</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#two-dependent-parameters">Two dependent parameters</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christian ForssÃ©n, Dick Furnstahl, and Daniel Phillips
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>