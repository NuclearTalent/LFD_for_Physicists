
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>22.4. Gaussian processes &#8212; Combined Learning from Data materials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/coloredpages.css?v=0a037ad7" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=6bd7df4c" />
    <link rel="stylesheet" type="text/css" href="../../../_static/myadmonitions.css?v=89ac28d1" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"loader": {"load": ["[tex]/textmacros"]}, "chtml": {"mtextInheritFont": true}, "tex": {"packages": {"[+]": ["textmacros"]}, "macros": {"data": "\\mathcal{D}", "pars": "\\boldsymbol{\\theta}", "para": "\\theta", "optpars": "\\pars^*", "optpara": "\\para^*", "prob": "\\mathbb{P}", "cprob": ["\\prob\\left( #1 \\, \\left\\vert \\, #2 \\right. \\right)", 2], "cprobsub": ["\\prob_{#1}\\left( #2 \\, \\left\\vert \\, #3 \\right. \\right)", 3], "pdf": ["p \\left( #1 \\, \\left\\vert \\, #2 \\right. \\right)", 2], "pdfsub": ["p_{#1} \\left( #2 \\, \\left\\vert \\, #3 \\right. \\right)", 3], "p": ["p \\left( #1 \\right)", 1], "psub": ["p_{#1} \\left( #2 \\right)", 2], "futuredata": "\\mathcal{F}", "expect": ["\\mathbb{E} \\left[ #1 \\right]", 1], "var": ["\\text{Var} \\left( #1 \\right)", 1], "std": ["\\text{Std} \\left( #1 \\right)", 1], "cov": ["\\text{Cov} \\left( #1, #2 \\right)", 2], "dmat": "\\boldsymbol{X}", "models": ["\\boldsymbol{M}\\left( #1 \\, ; \\, #2 \\right)", 2], "model": ["M\\left( #1 \\, ; \\, #2 \\right)", 2], "modeloutputs": "\\boldsymbol{M}", "modeloutput": "M", "MLmodel": ["\\boldsymbol{\\hat{y}}\\left( #1 \\right)", 1], "MLoutputs": "\\boldsymbol{\\hat{y}}", "MLoutput": "\\hat{y}", "outputs": "\\boldsymbol{y}", "inputs": "\\boldsymbol{x}", "targets": "\\boldsymbol{t}", "weights": "\\boldsymbol{w}", "testoutputs": "\\boldsymbol{y}^\\odot", "testinputs": "\\boldsymbol{x}^\\odot", "output": "y", "inputt": "x", "target": "t", "weight": "w", "testoutput": "y^\\odot", "MLtestoutput": "\\hat{y}^\\odot", "testinput": "x^\\odot", "trainingdata": "\\mathcal{T}", "LaTeX": "\\text{LaTeX}", "residual": "\\epsilon", "residuals": "\\boldsymbol{\\epsilon}", "zeros": "\\boldsymbol{0}", "covres": "\\boldsymbol{\\Sigma_{\\epsilon}}", "covpars": "\\boldsymbol{\\Sigma_{\\pars}}", "tildecovpars": "\\boldsymbol{\\widetilde{\\Sigma}_{\\pars}}", "sigmas": "\\boldsymbol{\\sigma}", "sigmai": "\\sigma_i", "sigmares": "\\sigma_{\\epsilon}", "cbar": "\\bar c", "Lra": "\\Longrightarrow", "yth": "y_{\\text{th}}", "yexp": "y_{\\text{exp}}", "ym": "y_{\\text{m}}", "thetavec": "\\boldsymbol{\\theta}", "parsLR": "\\boldsymbol{\\beta}", "paraLR": "\\beta", "covparsLR": "\\boldsymbol{\\Sigma_{\\parsLR}}", "optparsLR": "\\parsLR^*", "optparaLR": "\\paraLR^*", "tildecovparsLR": "\\boldsymbol{\\widetilde{\\Sigma}_{\\parsLR}}", "alphavec": "\\boldsymbol{\\alpha}", "muvec": "\\boldsymbol{\\mu}", "phivec": "\\boldsymbol{\\phi}", "betavec": "\\boldsymbol{\\beta}", "sigmavec": "\\boldsymbol{\\sigma}", "Sigmavec": "\\boldsymbol{\\Sigma}", "thetavechat": "\\widehat\\thetavec", "avec": "\\boldsymbol{a}", "Bvec": "\\boldsymbol{B}", "fvec": "\\boldsymbol{f}", "mvec": "\\boldsymbol{m}", "qvec": "\\boldsymbol{q}", "rvec": "\\boldsymbol{r}", "uvec": "\\boldsymbol{u}", "wvec": "\\boldsymbol{w}", "xvec": "\\boldsymbol{x}", "yvec": "\\boldsymbol{y}", "wt": "\\widetilde", "nb": "n_b", "mel": ["\\langle #1 | #2 | #3 \\rangle", 3], "qoi": "\\mathbf{Q}", "ytrue": "y_{\\text{true}}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'LearningFromData-content/MachineLearning/GP/GaussianProcesses';</script>
    <script src="../../../_static/custom.js?v=33f35b7a"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Demonstration: Gaussian processes" href="CF/demo-GaussianProcesses.html" />
    <link rel="prev" title="22. Overview of Gaussian process" href="RootGP.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../Intro/About.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/logo-copilot.png" class="logo__image only-light" alt="Combined Learning from Data materials - Home"/>
    <script>document.write(`<img src="../../../_static/logo-copilot.png" class="logo__image only-dark" alt="Combined Learning from Data materials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../Intro/About.html">
                    Learning from data for physicists:
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Intro/Invitation.html">1. Invitation to inductive inference</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Intro/Introduction.html">2. Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Intro/Introduction/sec-01-physicist-s-perspective.html">2.1. Physicist’s perspective</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Intro/Introduction/sec-02-bayesian-workflow.html">2.2. Bayesian workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Intro/Introduction/sec-03-machine-learning.html">2.3. Machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Intro/Introduction/sec-04-virtues.html">2.4. Virtues</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I: Bayesian methods for scientific modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/RootBayesianBasics.html">3. Overview of Part I</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Inferenceandpdfs.html">4. Inference and PDFs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Inferenceandpdfs/sec-01-statements.html">4.1. Statements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Inferenceandpdfs/sec-02-manipulating-probabilities-bayesian-rules-of-probability-as.html">4.2. Manipulating probabilities: Bayesian rules of probability as principles of logic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Inferenceandpdfs/sec-03-probability-density-functions.html">4.3. Probability density functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Inferenceandpdfs/sec-04-summary.html">4.4. Looking ahead</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/MoreBayesTheorem.html">4.5. Review of Bayes’ theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/DataModelsPredictions.html">4.6. Data, models, and predictions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Bayesian_epistemology.html">4.7. *Aside: Bayesian epistemology</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Posteriors.html">5. Bayesian posteriors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Exploring_pdfs.html">5.1. 📥 Exploring PDFs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Exploring_pdfs_followups.html">Follow-ups to Exploring PDFs</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Visualizing_correlated_gaussians.html">5.2. 📥 Visualizing correlated Gaussian distributions</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Gaussians.html">5.3. Gaussians: A couple of frequentist connections</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/visualization_of_CLT.html">📥 Visualization of the Central Limit Theorem</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianParameterEstimation/Interpreting2Dposteriors.html">5.4. Interpreting 2D posteriors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/chi_squared_tests.html">5.5. 📥 Demonstration: Sum of normal variables squared</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/CoinTossing.html">6. Updating via Bayes' rule</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/CoinTossing/sec-01-coin-tossing-frequentists-and-bayesaians.html">6.1. Coin tossing: Frequentists and Bayesaians</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/CoinTossing/sec-02-when-do-priors-matter-when-don-t-they-matter.html">6.2. When do priors matter? When don’t they matter?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/CoinTossing/sec-03-computing-the-posterior-analytically.html">6.3. Computing the posterior analytically</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/CoinTossing/sec-04-degree-of-belief-credibility-intervals-vs-frequentist-1-sigm.html">6.4. Degree of belief/credibility intervals vs frequentist 1-sigma intervals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/CoinTossing/sec-05-take-aways-and-follow-up-questions-from-coin-flipping.html">6.5. Take-aways and follow-up questions from coin flipping</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/demo-BayesianBasics.html">6.6. 📥 Demonstration:  Bayesian Coin Tossing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Bayesian_updating_coinflip_interactive.html">6.7. 📥 Demonstration: Coin tossing (with widget)</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/ErrorPropagation.html">7. Error propagation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/ErrorPropagation/sec-01-error-propagation-i-nuisance-parameters-and-marginalization.html">7.1. Error propagation (I): Nuisance parameters and marginalization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/ErrorPropagation/sec-02-error-propagation-ii-changing-variables.html">7.2. Error propagation (II): Changing variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/ErrorPropagation/sec-03-error-propagation-iii-a-useful-approximation.html">7.3. Error propagation (III): A useful approximation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/ErrorPropagation/sec-04-solutions.html">7.4. Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/UsingBayes.html">8. Bayes in practice</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/BayesianAdvantages.html">8.1. Advantages of the Bayesian approach</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianWorkflow/BayesianWorkflow.html">8.2. Bayesian research workflow</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../BayesianStatistics/BayesianLinearRegression/BayesianLinearRegression_rjf.html">8.3. Bayesian Linear Regression (BLR)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ModelingOptimization/demo-ModelValidation.html">📥 Demonstration: Linear Regression and Model Validation</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../BayesianStatistics/BayesianParameterEstimation/Exercises_parameter_estimation.html">9. Exercises for Part I</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/exercise_sum_product_rule.html">9.1. Exercise: Checking the sum and product rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/exercise_medical_example_by_Bayes.html">9.2. Exercise: Standard medical example using Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianParameterEstimation/parameter_estimation_Gaussian_noise.html">9.3. 📥 Parameter estimation I: Gaussian mean and variance</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianParameterEstimation/radioactive_lighthouse_exercise.html">9.4. 📥 Radioactive lighthouse problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianParameterEstimation/amplitude_in_presence_of_background.html">9.5. 📥 Amplitude of a signal in the presence of background</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianParameterEstimation/parameter_estimation_fitting_straight_line_I.html">9.6. 📥 Parameter estimation example: fitting a straight line</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianParameterEstimation/parameter_estimation_fitting_straight_line_II.html">9.7. 📥 Parameter estimation example: fitting a straight line II</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II: Advanced Bayesian methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../BayesianStatistics/RootAdvancedMethods.html">10. Overview of Part II</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/DiscrepancyModels.html">11. Discrepancy Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/DiscrepancyModels/sec-01-koh-and-boh-discrepancy-models.html">11.1. KOH and BOH discrepancy models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/DiscrepancyModels/sec-02-framework.html">11.2. Framework</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/DiscrepancyModels/sec-03-the-ball-drop-model.html">11.3. The ball-drop model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/MD_balldrop_v1.html">11.4. 📥 Ball-drop experiment notebook</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../BayesianStatistics/AssigningProbabilities/Assigning.html">12. Assigning probabilities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../BayesianStatistics/AssigningProbabilities/IgnorancePDF.html">12.1. Assigning probabilities (I): Indifferences and translation groups</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../BayesianStatistics/AssigningProbabilities/demo-straight_lines.html">Alternative notebook with MCMC sampling</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/AssigningProbabilities/MaxEnt2.html">12.2. Assigning probabilities (II): The principle of maximum entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/AssigningProbabilities/MaxEnt_Function_Reconstruction.html">12.3. 📥 Maximum Entropy for reconstructing a function from its moments</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../BayesianStatistics/BayesianParameterEstimation/dealing_with_outliers.html">13. 📥 Dealing with outliers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../BayesianStatistics/ComputationalBayes/BayesLinear.html">14. Bayes goes linear: History matching</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../BayesianStatistics/Multimodel_inference.html">15. Multi-model inference with Bayes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../BayesianStatistics/ModelSelection/ModelSelection.html">15.1. Model Selection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../BayesianStatistics/ModelSelection/BUQ/Evidence_for_model_EFT_coefficients.html">Evidence calculation for EFT expansions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../BayesianStatistics/ModelSelection/BUQ/two_model_evidence.html">Follow-up to EFT evidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../BayesianStatistics/ModelSelection/BUQ/computing_evidence.html">Computing the evidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../BayesianStatistics/ModelSelection/BUQ/MCMC-parallel-tempering_ptemcee_vs_zeus.html">Demo: Multimodal distributions with two samplers</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/ModelMixing/model_mixing.html">15.2. Model averaging and mixing </a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III: MCMC sampling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../StochasticProcesses/RootMCMC.html">16. Overview of Part III</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/StochasticProcesses.html">17. Stochastic processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/Metropolis_Poisson_example.html">17.7. Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/demo-MCMC.html">17.8. Demonstration: Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/Recap_BUQ.html">17.9. Recap of Poisson and more about MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/parameter_estimation_Gaussian_noise-2.html">17.10. Parameter estimation example: Gaussian noise and averages II</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/MCMC-random-walk-and-sampling.html">17.11. Exercise: Random walk</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/MCMC_overview.html">18. Overview of Markov Chain Monte Carlo</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/MarkovChains.html">18.1. Markov chains</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/MCMC.html">18.2. Markov chain Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/MCMC_intro_BUQ.html">18.3. Alternative MCMC introduction (Gregory)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/Assignment_extending_radioactive_lighthouse.html">18.4. Assignment: 2D radioactive lighthouse location using MCMC</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/Advanced_MCMC.html">19. Advanced MCMC</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/ComputationalBayes/AdvancedMCMC.html">19.1. Advanced Markov chain Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/MCMC-diagnostics.html">19.2. Overview: MCMC Diagnostics</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/intuition_sampling.html">19.4. Intuition on sampling and best practices</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/Other_samplers.html">20. HMC and other samplers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../StochasticProcesses/BUQ2/HMC_intro_BUQ.html">20.1. Hamiltonian Monte Carlo (HMC) overview and visualization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../StochasticProcesses/BUQ2/Liouville_theorem_visualization.html">Liouville Theorem Visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../StochasticProcesses/BUQ2/Orbital_eqs_with_different_algorithms.html">Solving orbital equations with different algorithms</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/zeus.html">20.2. The Zeus Ensemble Slice Sampler</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../StochasticProcesses/BUQ2/PyMC_intro_updated.html">20.3. PyMC Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../StochasticProcesses/OverviewIntroPyMC.html">Overview of Intro to PyMC notebook</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ2/parameter_estimation_Gaussian_noise_compare_samplers.html">20.4. Comparing samplers for a simple problem</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV: Machine learning: A Bayesian perspective</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../RootML.html">21. Overview of Part IV</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="RootGP.html">22. Overview of Gaussian processes</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active has-children"><a class="current reference internal" href="#">22.4. Introduction to Gaussian processes</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="CF/demo-GaussianProcesses.html">📥 demo-GaussianProcesses notebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="BUQ/lecture_20.html">GP recap; GP applications; (old lecture 20)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="Sklearn_demos.html">22.5. scikit-learn demo notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="BUQ/plot_gpr_noisy_targets.html">📥 One-dimension regression example</a></li>
<li class="toctree-l3"><a class="reference internal" href="BUQ/plot_gpr_prior_posterior.html">📥 Prior and posterior with different kernels</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="GPy_demos.html">22.6. GPy demo notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="BUQ/demo-GaussianProcesses.html">Gaussian processes demonstration</a></li>
<li class="toctree-l3"><a class="reference internal" href="CF/exercise_GP_GPy.html">Exercise: Gaussian processes using <code class="docutils literal notranslate"><span class="pre">GPy</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="BUQ/Gaussian_processes_exercises.html">Exercise: Gaussian Process models with GPy</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../LogReg/LogReg.html">23. Logistic Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ANN/MachineLearningExamples.html">23.5. Machine Learning: First Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ANN/NeuralNet/exercises_LogReg_NeuralNet.html">23.6. Exercise: Logistic Regression and neural networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../ANN/MachineLearning.html">24. Machine learning: Overview and notation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../ANN/NeuralNet.html">24.5. Artifical neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ANN/NeuralNet/demo-NeuralNet.html">24.6. Demonstration: Neural network classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ANN/Neural_Network_for_simple_function_in_PyTorch.html">24.7. 📥 ANN from ChatGPT using PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ANN/ModelValidation.html">24.8. Model validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ANN/DataBiasFairness.html">24.9. Data bias and fairness in machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ANN/NeuralNet/NeuralNetBackProp.html">24.10. *Neural networks: Backpropagation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/ANNFT.html">25. ANNs in the large-width limit (ANNFT)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/random_initialized_ANN_vs_width.html">25.3. 📥 Distributions of Randomly-Initialized ANNs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BNN/bnn.html">26. Bayesian neural nets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BNN/demo-bnn.html">26.4. Demonstration: Variational Inference and Bayesian Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BNN/exercises_BNN.html">26.5. Exercise: Bayesian neural networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../CNN/cnn.html">27. *Convolutional neural nets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../CNN/demo-cnn.html">27.6. Demonstration: Image recognition with Convolutional Neural Networks</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V: Other topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../OtherTopics/RootOtherTopics.html">28. Overview of Part V </a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/Emulators.html">29. Emulators</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/ComputationalBayes/BayesFast.html">29.1. Bayes goes fast: Emulators (from CF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/ComputationalBayes/extra_RBM_emulators.html">29.2. RBM emulators (BUQ)</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../OtherTopics/Student_t_distribution_from_Gaussians.html">30. 📥 Student t distribution from Gaussians</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/SVD.html">31. PCA, SVD, and all that</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/linear_algebra_games_including_SVD.html">31.5. 📥 demo-SVD notebook</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../OtherTopics/qbism.html">32. QBism: Bayesian quantum mechanics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Backmatter</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Backmatter/bibliography.html">33. Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Backmatter/JB_tests.html">34. Guide to Jupyter Book markdown</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix A: Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Reference/Statistics.html">35. Statistics concepts and notation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/GradientDescent.html">36. Gradient-descent optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix B: Scientific modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/RootScientificModeling.html">37. Overview of scientific modeling material</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling.html">38. Overview of modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-01-notation.html">38.1. Notation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-02-models-in-science.html">38.2. Models in science</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-03-parametric-versus-non-parametric-models.html">38.3. Parametric versus non-parametric models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-04-linear-versus-non-linear-models.html">38.4. Linear versus non-linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-05-regression-analysis-optimization-versus-inference.html">38.5. Regression analysis: optimization versus inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-06-exercises.html">38.6. Exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling/sec-07-solutions.html">38.7. Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ModelingOptimization/LinearModels.html">39. Linear models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/LinearModels/sec-01-definition-of-linear-models.html">39.1. Definition of linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/LinearModels/sec-02-regression-analysis-with-linear-models.html">39.2. Regression analysis with linear models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/LinearModels/sec-03-ordinary-linear-regression-warmup.html">39.3. Ordinary linear regression: warmup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/LinearModels/sec-04-ordinary-linear-regression-in-practice.html">39.4. Ordinary linear regression in practice</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/LinearModels/sec-05-solutions.html">39.5. Solutions</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../ModelingOptimization/MathematicalOptimization.html">40. Mathematical optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/MathematicalOptimization/sec-01-gradient-descent-optimization.html">40.1. Gradient-descent optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/MathematicalOptimization/sec-02-batch-stochastic-and-mini-batch-gradient-descent.html">40.2. Batch, stochastic and mini-batch gradient descent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ModelingOptimization/MathematicalOptimization/sec-03-adaptive-gradient-descent-algorithms.html">40.3. Adaptive gradient descent algorithms</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix C: Getting started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Setup/RootGettingStarted.html">41. Overview of Getting started material</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Setup/exercise_Intro_01_Jupyter_Python.html">42. 📥 Exercise: Jupyter notebooks and Python</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Setup/more_python_and_jupyter.html">43. More about Python and Jupyter notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/exercise_Intro_02_Jupyter_Python.html">43.4. 📥 Python lists and iterations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/exercise_Intro_03_Numpy.html">43.5. 📥 Linear algebra operations with NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/demo-Intro.html">43.6. 📥 Reading data and fitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/Simple_widgets_v1.html">43.7. 📥 Making a simple widget-based UI</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Setup/setting_up.html">44. Setting up for using this Jupyter Book</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/installing_anaconda.html">44.1. Using Anaconda</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/using_github.html">44.2. Using GitHub</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">TALENT mini-projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/RootMiniProjects.html">Overview of mini-projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/mini-project_I_toy_model_of_EFT.html">📥 MP I: Parameter estimation for a toy model of an EFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/model-selection_mini-project-IIa.html">📥 MP IIa: Model selection basics</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Mini-projects/model-selection_mini-project-IIb_How_many_lines_ptemcee.html">📥 MP IIb: How many lines?</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Mini-projects/Mini-project_IIb_overview.html">Overview of Mini-project IIb: How many lines?</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/mini-project_IIIa_bayesian_optimization.html">📥 MP IIIa: Bayesian optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/mini-project_IIIb_Bayesian_neural_networks_from_demo.html">📥 MP IIIb: Bayesian Neural Networks</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/NuclearTalent/LFD_for_Physicists" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/NuclearTalent/LFD_for_Physicists/issues/new?title=Issue%20on%20page%20%2FLearningFromData-content/MachineLearning/GP/GaussianProcesses.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/LearningFromData-content/MachineLearning/GP/GaussianProcesses.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Gaussian processes</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-using-gaussian-processes">Inference using Gaussian processes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-approach">Parametric approach</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-covariance-matrix-as-the-central-object">The covariance matrix as the central object</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-parametric-approach-mean-and-covariance-functions">Non-parametric approach: Mean and covariance functions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stationary-kernels">Stationary kernels</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gp-models-for-regression">GP models for regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elegant-linear-algebra-tricks-to-obtain-boldsymbol-c-n-1-1">Elegant linear algebra tricks to obtain <span class="math notranslate nohighlight">\(\boldsymbol{C}_{N+1}^{-1}\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-the-gp-model-hyperparameters">Choosing the GP model hyperparameters</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="gaussian-processes">
<span id="sec-gaussianprocesses"></span><h1><span class="section-number">22.4. </span>Gaussian processes<a class="headerlink" href="#gaussian-processes" title="Link to this heading">#</a></h1>
<section id="inference-using-gaussian-processes">
<h2>Inference using Gaussian processes<a class="headerlink" href="#inference-using-gaussian-processes" title="Link to this heading">#</a></h2>
<p>Assume that there is a set of input vectors with independent, predictor, variables</p>
<div class="amsmath math notranslate nohighlight" id="equation-3144f9d5-31d4-4822-b0c2-22c74e477017">
<span class="eqno">(22.1)<a class="headerlink" href="#equation-3144f9d5-31d4-4822-b0c2-22c74e477017" title="Permalink to this equation">#</a></span>\[\begin{equation}
 \boldsymbol{X}_N \equiv \{ \boldsymbol{x}^{(i)}\}_{i=1}^N 
\end{equation}\]</div>
<p>and a set of target values</p>
<div class="amsmath math notranslate nohighlight" id="equation-bafdbcb4-a72c-46fa-829c-a28ce3544844">
<span class="eqno">(22.2)<a class="headerlink" href="#equation-bafdbcb4-a72c-46fa-829c-a28ce3544844" title="Permalink to this equation">#</a></span>\[\begin{equation}
 \boldsymbol{t}_N \equiv \{ t^{(i)}\}_{i=1}^N. 
\end{equation}\]</div>
<ul class="simple">
<li><p>Note that we will use the symbol <span class="math notranslate nohighlight">\(t\)</span> to denote the target, or response, variables in the context of Gaussian Processes. Here we will consider single, scalar outputs <span class="math notranslate nohighlight">\(t^{(i)}\)</span>. The extension to vector outputs <span class="math notranslate nohighlight">\(\boldsymbol{t}^{(i)}\)</span> is straightforward.</p></li>
<li><p>Furthermore, we will use the subscript <span class="math notranslate nohighlight">\(N\)</span> to denote a set of <span class="math notranslate nohighlight">\(N\)</span> vectors (or scalars): <span class="math notranslate nohighlight">\(\boldsymbol{X}_N\)</span> (<span class="math notranslate nohighlight">\(\boldsymbol{t}_N\)</span>),</p></li>
<li><p>… while a single instance <span class="math notranslate nohighlight">\(i\)</span> is denoted by a superscript: <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(i)}\)</span> (<span class="math notranslate nohighlight">\(t^{(i)}\)</span>).</p></li>
</ul>
<div class="admonition-notation admonition">
<p class="admonition-title">Notation</p>
<p>We will use the notation <span class="math notranslate nohighlight">\(\mathcal{D}_N = [\boldsymbol{X}_N, \boldsymbol{t}_N]\)</span> for the data.</p>
<ul class="simple">
<li><p>Note that a target is always associated with an input vector. When our focus is on the prediction of targets, we sometimes write <span class="math notranslate nohighlight">\(p(\boldsymbol{t}_N)\)</span> without including <span class="math notranslate nohighlight">\(\boldsymbol{X}_N\)</span> explcitly.</p></li>
</ul>
</div>
<!-- !split -->
<p>We will consider two different <em>inference problems</em>:</p>
<ol class="arabic simple">
<li><p>The prediction of a <em>new target</em> <span class="math notranslate nohighlight">\(t^{(N+1)}\)</span> given the data <span class="math notranslate nohighlight">\(\mathcal{D}_N\)</span> and a new input <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(N+1)}\)</span>.</p></li>
<li><p>The inference of a <em>model function</em> <span class="math notranslate nohighlight">\(y(\boldsymbol{x})\)</span> from the data <span class="math notranslate nohighlight">\(\mathcal{D}_N\)</span>.</p></li>
</ol>
<!-- !split -->
<p>The former can be expressed with the pdf</p>
<div class="amsmath math notranslate nohighlight" id="equation-5e982a1a-7845-4c58-ab59-6b957eb5c862">
<span class="eqno">(22.3)<a class="headerlink" href="#equation-5e982a1a-7845-4c58-ab59-6b957eb5c862" title="Permalink to this equation">#</a></span>\[\begin{equation}
 
p\left( t^{(N+1)} | \mathcal{D}_N, \boldsymbol{x}^{(N+1)} \right)

\end{equation}\]</div>
<p>while the latter can be written using Bayes’ formula (in these notes we will not be including information <span class="math notranslate nohighlight">\(I\)</span> explicitly in the conditional probabilities)</p>
<div class="amsmath math notranslate nohighlight" id="equation-6983491b-ed77-4a91-b0e8-600b43b87ca5">
<span class="eqno">(22.4)<a class="headerlink" href="#equation-6983491b-ed77-4a91-b0e8-600b43b87ca5" title="Permalink to this equation">#</a></span>\[\begin{equation}
 p\left( y(\boldsymbol{x}) | \mathcal{D}_N \right)
= \frac{p\left( \mathcal{D}_N | y(\boldsymbol{x}) \right) p \left( y(\boldsymbol{x}) \right) }
{p\left( \mathcal{D}_N \right) } 
\end{equation}\]</div>
<!-- !split -->
<p>The inference of a function will obviously also allow to make predictions for new targets.
However, we will need to consider in particular the second term in the numerator, which is the <strong>prior</strong> distribution on functions assumed in the model.</p>
<ul class="simple">
<li><p>This prior is implicit in parametric models with priors on the parameters.</p></li>
<li><p>The idea of Gaussian process modeling is to put a prior directly on the <strong>space of functions</strong> without parameterizing <span class="math notranslate nohighlight">\(y(\boldsymbol{x})\)</span>.</p></li>
<li><p>A Gaussian process can be thought of as a generalization of a Gaussian distribution over a finite vector space to a <strong>function space of infinite dimension</strong>.</p></li>
<li><p>Just as a Gaussian distribution is specified by its mean and covariance matrix, a Gaussian process is specified by a <strong>mean and covariance function</strong>.</p></li>
</ul>
<!-- !split -->
<p><em>Gaussian process.</em>
A Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution</p>
<!-- !split -->
<section id="references">
<h3>References:<a class="headerlink" href="#references" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><a class="reference external" href="http://www.gaussianprocess.org/gpml">Gaussian Processes for Machine Learning</a>, Carl Edward Rasmussen and Chris Williams <span id="id1">[<a class="reference internal" href="../../Backmatter/bibliography.html#id27" title="Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). The MIT Press, 2005. ISBN 026218253X.">RW05</a>]</span>, <a class="reference external" href="http://www.gaussianprocess.org/gpml/chapters">online version</a>.</p></li>
<li><p><a class="reference external" href="https://sheffieldml.github.io/GPy/">GPy</a>: a Gaussian Process (GP) framework written in python, from the Sheffield machine learning group.</p></li>
</ol>
<!-- !split -->
</section>
<section id="parametric-approach">
<h3>Parametric approach<a class="headerlink" href="#parametric-approach" title="Link to this heading">#</a></h3>
<p>Let us express <span class="math notranslate nohighlight">\(y(\boldsymbol{x})\)</span> in terms of a model function <span class="math notranslate nohighlight">\(y(\boldsymbol{x}; \boldsymbol{\theta})\)</span> that depends on a vector of model parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.</p>
<p>For example, using a set of basis functions <span class="math notranslate nohighlight">\(\left\{ \phi_{h} (\boldsymbol{x}) \right\}_{h=1}^H\)</span> with linear weights <span class="math notranslate nohighlight">\(\boldsymbol{\theta} \in \mathbb{R}^H\)</span> we have</p>
<div class="amsmath math notranslate nohighlight" id="equation-7c1eadaa-c674-4bf2-9bf9-ca5be1b4bc77">
<span class="eqno">(22.5)<a class="headerlink" href="#equation-7c1eadaa-c674-4bf2-9bf9-ca5be1b4bc77" title="Permalink to this equation">#</a></span>\[\begin{equation}

y (\boldsymbol{x}, \boldsymbol{\theta}) = \sum_{h=1}^H \theta_{h} \phi_{h} (\boldsymbol{x})

\end{equation}\]</div>
<p><em>Notice.</em>
The basis functions can be non-linear in <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> such as Gaussians (aka <em>radial basis functions</em>).
Still, this constitutes a linear model since <span class="math notranslate nohighlight">\(y (\boldsymbol{x}, \boldsymbol{\theta})\)</span> depends linearly on the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.</p>
<p>The inference of model parameters should be a well-known problem by now. We state it in terms of Bayes theorem</p>
<div class="amsmath math notranslate nohighlight" id="equation-744d8bfa-9506-4dd3-a465-e32e90034ff2">
<span class="eqno">(22.6)<a class="headerlink" href="#equation-744d8bfa-9506-4dd3-a465-e32e90034ff2" title="Permalink to this equation">#</a></span>\[\begin{equation}

p \left( \boldsymbol{\theta} | \mathcal{D}_N \right)
= \frac{ p \left( \mathcal{D}_N | \boldsymbol{\theta} \right) p \left( \boldsymbol{\theta} \right)}{p \left( \mathcal{D}_N \right)}

\end{equation}\]</div>
<p>Having solved this inference problem (note that the likelihood is Gaussian, cf linear regression) a prediction can be made through marginalization</p>
<div class="amsmath math notranslate nohighlight" id="equation-a4255443-8851-477b-b3ec-b24d49551a2a">
<span class="eqno">(22.7)<a class="headerlink" href="#equation-a4255443-8851-477b-b3ec-b24d49551a2a" title="Permalink to this equation">#</a></span>\[\begin{equation}

p\left( t^{(N+1)} | \mathcal{D}_N, \boldsymbol{x}^{(N+1)} \right) 
= \int d^H \boldsymbol{\theta} 
p\left( t^{(N+1)} | \boldsymbol{\theta}, \boldsymbol{x}^{(N+1)} \right)
p \left( \boldsymbol{\theta} | \mathcal{D}_N \right).

\end{equation}\]</div>
<p>Here it is important to note that the final answer does not make any explicit reference to our parametric representation of the unknown function <span class="math notranslate nohighlight">\(y(\boldsymbol{x})\)</span>.</p>
<p>Assuming that we have a fixed set of basis functions and Gaussian prior distributions (with zero mean) on the weights <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> we will show that:</p>
<ul class="simple">
<li><p>The joint pdf of the observed data given the model <span class="math notranslate nohighlight">\(p( \mathcal{D}_N)\)</span>, is a multivariate Gaussian with mean zero and with a covariance matrix that is determined by the basis functions.</p></li>
<li><p>This implies that the conditional distribution <span class="math notranslate nohighlight">\(p( t^{(N+1)} | \mathcal{D}_N, \boldsymbol{X}_{N+1})\)</span>, is also a multivariate Gaussian whose mean depends linearly on <span class="math notranslate nohighlight">\(\boldsymbol{t}_N\)</span>.</p></li>
</ul>
<div class="tip admonition">
<p class="admonition-title">Sum of normally distributed random variables.</p>
<p>If <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent random variables that are normally distributed (and therefore also jointly so), then their sum is also normally distributed. i.e., <span class="math notranslate nohighlight">\(Z=X+Y\)</span> is normally distributed with its mean being the sum of the two means, and its variance being the sum of the two variances.</p>
</div>
<p>Consider the linear model and define the <span class="math notranslate nohighlight">\(N \times H\)</span> design matrix <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span> with elements</p>
<div class="amsmath math notranslate nohighlight" id="equation-5c151288-8006-4a14-9117-917dc862d07e">
<span class="eqno">(22.8)<a class="headerlink" href="#equation-5c151288-8006-4a14-9117-917dc862d07e" title="Permalink to this equation">#</a></span>\[\begin{equation}

R_{nh} \equiv \phi_{h} \left( \boldsymbol{x}^{(n)} \right).

\end{equation}\]</div>
<p>Then <span class="math notranslate nohighlight">\(\boldsymbol{y}_N = \boldsymbol{R} \boldsymbol{\theta}\)</span> is the vector of model predictions, i.e.</p>
<div class="amsmath math notranslate nohighlight" id="equation-57c3acac-e962-419f-a8cc-6ffb513dc370">
<span class="eqno">(22.9)<a class="headerlink" href="#equation-57c3acac-e962-419f-a8cc-6ffb513dc370" title="Permalink to this equation">#</a></span>\[\begin{equation}

y^{(n)} = \sum_{h=1}^H R_{nh} \theta_{h}.

\end{equation}\]</div>
<p>Assume that we have a Gaussian prior for the linear model weights <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> with zero mean and a diagonal covariance matrix</p>
<div class="amsmath math notranslate nohighlight" id="equation-49a4fdf3-5cf1-458b-8dfe-ffbe858eefeb">
<span class="eqno">(22.10)<a class="headerlink" href="#equation-49a4fdf3-5cf1-458b-8dfe-ffbe858eefeb" title="Permalink to this equation">#</a></span>\[\begin{equation}

p(\boldsymbol{\theta}) = \mathcal{N} \left( \boldsymbol{\theta}; 0, \sigma_\theta^2 \boldsymbol{I} \right).

\end{equation}\]</div>
<p>Now, since <span class="math notranslate nohighlight">\(y\)</span> is a linear function of <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, it is also Gaussian distributed with mean zero. Its covariance matrix becomes</p>
<div class="amsmath math notranslate nohighlight" id="equation-3fef1a78-51a2-42fc-8959-f1f8cb2c24f7">
<span class="eqno">(22.11)<a class="headerlink" href="#equation-3fef1a78-51a2-42fc-8959-f1f8cb2c24f7" title="Permalink to this equation">#</a></span>\[\begin{equation}

\boldsymbol{Q} = \langle \boldsymbol{y}_N \boldsymbol{y}_N^T \rangle = \langle \boldsymbol{R} \boldsymbol{\theta} \boldsymbol{\theta}^T \boldsymbol{R}^T \rangle
= \sigma_\theta^2 \boldsymbol{R} \boldsymbol{R}^T,

\end{equation}\]</div>
<p>which implies that</p>
<div class="amsmath math notranslate nohighlight" id="equation-e9e99f6f-735a-4645-80bb-56af3278e8b0">
<span class="eqno">(22.12)<a class="headerlink" href="#equation-e9e99f6f-735a-4645-80bb-56af3278e8b0" title="Permalink to this equation">#</a></span>\[\begin{equation}

p(\boldsymbol{y}_N) = \mathcal{N} \left( \boldsymbol{y}_N; 0, \sigma_\theta^2 \boldsymbol{R} \boldsymbol{R}^T \right).

\end{equation}\]</div>
<p>This will be true for any set of points <span class="math notranslate nohighlight">\(\boldsymbol{X}_N\)</span>; which is the defining property of a <strong>Gaussian process</strong>.</p>
<ul class="simple">
<li><p>What about the target values <span class="math notranslate nohighlight">\(\boldsymbol{t}_N\)</span>?</p></li>
</ul>
<p>Well, if <span class="math notranslate nohighlight">\(t^{(n)}\)</span> is assumed to differ by additive Gaussian noise, i.e.,</p>
<div class="amsmath math notranslate nohighlight" id="equation-c9e4b1ff-d1c2-4698-8319-d1d3356d5ae4">
<span class="eqno">(22.13)<a class="headerlink" href="#equation-c9e4b1ff-d1c2-4698-8319-d1d3356d5ae4" title="Permalink to this equation">#</a></span>\[\begin{equation}

t^{(n)} = y^{(n)} + \varepsilon^{(n)}, 

\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\varepsilon^{(n)} \sim \mathcal{N} \left( 0, \sigma_\nu^2 \right)\)</span>; then <span class="math notranslate nohighlight">\(\boldsymbol{t}_N\)</span> also has a Gaussian distribution</p>
<div class="amsmath math notranslate nohighlight" id="equation-8307f604-6d4a-404e-bcd2-e43dbaae7e31">
<span class="eqno">(22.14)<a class="headerlink" href="#equation-8307f604-6d4a-404e-bcd2-e43dbaae7e31" title="Permalink to this equation">#</a></span>\[\begin{equation}

p(\boldsymbol{t}_N) = \mathcal{N} \left( \boldsymbol{t}_N; 0, \boldsymbol{C} \right),

\end{equation}\]</div>
<p>where the covariance matrix of this target distribution is given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-1414d1f9-85a7-4190-ae5b-e0acb3b40a62">
<span class="eqno">(22.15)<a class="headerlink" href="#equation-1414d1f9-85a7-4190-ae5b-e0acb3b40a62" title="Permalink to this equation">#</a></span>\[\begin{equation}

\boldsymbol{C} = \boldsymbol{Q} + \sigma_\nu^2 \boldsymbol{I} = \sigma_\theta^2 \boldsymbol{R} \boldsymbol{R}^T + \sigma_\nu^2 \boldsymbol{I}.

\end{equation}\]</div>
<!-- !split -->
<section id="the-covariance-matrix-as-the-central-object">
<h4>The covariance matrix as the central object<a class="headerlink" href="#the-covariance-matrix-as-the-central-object" title="Link to this heading">#</a></h4>
<p>The covariance matrices are given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-176ddb0d-0442-4e29-a8ea-cd2947466c0b">
<span class="eqno">(22.16)<a class="headerlink" href="#equation-176ddb0d-0442-4e29-a8ea-cd2947466c0b" title="Permalink to this equation">#</a></span>\[\begin{equation}

Q_{nn'} = \sigma_\theta^2 \sum_h \phi_{h} \left( \boldsymbol{x}^{(n)} \right) \phi_{h} \left( \boldsymbol{x}^{(n')} \right),

\end{equation}\]</div>
<p>and</p>
<div class="amsmath math notranslate nohighlight" id="equation-50cc7251-2fb8-46cb-8041-8cbce2eba166">
<span class="eqno">(22.17)<a class="headerlink" href="#equation-50cc7251-2fb8-46cb-8041-8cbce2eba166" title="Permalink to this equation">#</a></span>\[\begin{equation}

C_{nn'} = Q_{nn'} + \delta_{nn'} \sigma_\nu^2.

\end{equation}\]</div>
<p>This means that the correlation between target values <span class="math notranslate nohighlight">\(t^{(n)}\)</span> and <span class="math notranslate nohighlight">\(t^{(n')}\)</span> is determined by the points <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n)}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(n')}\)</span> and the behaviour of the basis functions.</p>
<!-- !split -->
</section>
</section>
<section id="non-parametric-approach-mean-and-covariance-functions">
<h3>Non-parametric approach: Mean and covariance functions<a class="headerlink" href="#non-parametric-approach-mean-and-covariance-functions" title="Link to this heading">#</a></h3>
<p>In fact, we don’t really need the basis functions and their parameters anymore. The influence of these appear only in the covariance matrix that describes the distribution of the targets, which is our key object. We can replace the parametric model altogether with a <strong>covariance function</strong> <span class="math notranslate nohighlight">\(C( \boldsymbol{x}, \boldsymbol{x}' )\)</span> which generates the  elements of the covariance matrix</p>
<div class="amsmath math notranslate nohighlight" id="equation-72ee79dd-e2ab-4bae-bef0-4312f5914517">
<span class="eqno">(22.18)<a class="headerlink" href="#equation-72ee79dd-e2ab-4bae-bef0-4312f5914517" title="Permalink to this equation">#</a></span>\[\begin{equation}

Q_{nn'} = C \left( \boldsymbol{x}^{(n)}, \boldsymbol{x}^{(n')} \right),

\end{equation}\]</div>
<p>for any set of points <span class="math notranslate nohighlight">\(\left\{ \boldsymbol{x}^{(n)} \right\}_{n=1}^N\)</span>.</p>
<p>Note, however, that <span class="math notranslate nohighlight">\(\boldsymbol{Q}\)</span> must be positive-definite. This constrains the set of valid covariance functions.</p>
<p>Once we have defined a covariance function, the covariance matrix for the target values will be given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-ae00c2d1-f39c-444d-9f96-f93d0e52ccfb">
<span class="eqno">(22.19)<a class="headerlink" href="#equation-ae00c2d1-f39c-444d-9f96-f93d0e52ccfb" title="Permalink to this equation">#</a></span>\[\begin{equation}

C_{nn'} = C \left( \boldsymbol{x}^{(n)}, \boldsymbol{x}^{(n')} \right) + \sigma_\nu^2 \delta_{nn'}.

\end{equation}\]</div>
<p>A wide range of different covariance contributions can be <a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_process#Covariance_functions">constructed</a>. These standard covariance functions are typically parametrized with hyperparameters <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}\)</span> so that</p>
<div class="amsmath math notranslate nohighlight" id="equation-aef19349-bca6-4f35-8e1a-583037a7ff4f">
<span class="eqno">(22.20)<a class="headerlink" href="#equation-aef19349-bca6-4f35-8e1a-583037a7ff4f" title="Permalink to this equation">#</a></span>\[\begin{equation}

C_{nn'} = C \left( \boldsymbol{x}^{(n)}, \boldsymbol{x}^{(n')}, \boldsymbol{\alpha} \right) + \delta_{nn'} \Delta \left( \boldsymbol{x}^{(n)};  \boldsymbol{\alpha} \right),

\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\Delta\)</span> is often included as a flexible white noise component. It is usually good practice to include a small white noise term, <span class="math notranslate nohighlight">\(\Delta = \sigma_\nu^2 \ll 1\)</span>, even if your data has negligible errors, since it helps with numerical stability and making sure that your covariance matrix is positive definite.</p>
<!-- !split -->
<section id="stationary-kernels">
<h4>Stationary kernels<a class="headerlink" href="#stationary-kernels" title="Link to this heading">#</a></h4>
<p>The most common types of covariance functions are stationary, or translationally invariant, which implies that</p>
<div class="amsmath math notranslate nohighlight" id="equation-806220e2-719e-4bcb-8123-17d1ad0ec5f5">
<span class="eqno">(22.21)<a class="headerlink" href="#equation-806220e2-719e-4bcb-8123-17d1ad0ec5f5" title="Permalink to this equation">#</a></span>\[\begin{equation}

C \left( \boldsymbol{x}, \boldsymbol{x}', \boldsymbol{\alpha} \right) = D \left(  \boldsymbol{x} - \boldsymbol{x}' ; \boldsymbol{\alpha} \right),

\end{equation}\]</div>
<p>where the function <span class="math notranslate nohighlight">\(D\)</span> is often referred to as a <em>kernel</em>. Note that the <span class="math notranslate nohighlight">\((\boldsymbol{x} - \boldsymbol{x}')\)</span>-dependence must be such that the kernel is symmetric.</p>
<p>A very standard kernel is the RBF (radial basis function, but also known as Exponentiated Quadratic or Gaussian kernel) which is differentiable infinitely many times (hence, very smooth),</p>
<div class="amsmath math notranslate nohighlight" id="equation-85b55efc-5d1f-447b-9062-35a4f94709da">
<span class="eqno">(22.22)<a class="headerlink" href="#equation-85b55efc-5d1f-447b-9062-35a4f94709da" title="Permalink to this equation">#</a></span>\[\begin{equation}
 
C_\mathrm{RBF}(\mathbf{x},\mathbf{x}'; \boldsymbol{\alpha}) = \exp \left[ -\frac{1}{2} \sum_{i=1}^p \frac{(x_{i} - x_{i}')^2}{l_i^2} \right] 

\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(p\)</span> denotes the dimensionality of the input space (i.e., <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^p\)</span>). The hyperparameters of the RBF kernel, <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} = \vec{l}\)</span>, are known as the correlation length(s). Sometimes, a single correlation length <span class="math notranslate nohighlight">\(l_i=l\)</span> is used for all dimensions.</p>
<p>Different kernels can be combined to build the most relevant covariance function. For example, the RBF kernel is often multiplied by a constant kernel (which then becomes known as signal noise) followed by the addition of a diagonal white noise kernel. This would result in</p>
<div class="amsmath math notranslate nohighlight" id="equation-a4f439b5-5455-41d3-9d92-d4065377fb65">
<span class="eqno">(22.23)<a class="headerlink" href="#equation-a4f439b5-5455-41d3-9d92-d4065377fb65" title="Permalink to this equation">#</a></span>\[\begin{equation}
 
C_(\mathbf{x},\mathbf{x}'; \boldsymbol{\alpha}) = \sigma_f^2 \exp \left[ -\frac{1}{2} \sum_{i=1}^p \frac{(x_{i} - x_{i}')^2}{l_i^2} \right] + \sigma_\nu^2 \delta_{\mathbf{x},\mathbf{x}'},

\end{equation}\]</div>
<p>with the complete set of hyperparameters <span class="math notranslate nohighlight">\(\boldsymbol{\alpha} = \{ \sigma_\nu^2, \sigma_f^2, \vec{l} \}\)</span> known as the white noise variance, signal variance, and RBF correlation length(s).</p>
<!-- !split -->
</section>
</section>
</section>
<section id="gp-models-for-regression">
<h2>GP models for regression<a class="headerlink" href="#gp-models-for-regression" title="Link to this heading">#</a></h2>
<p>Let us return to the problem of predicting <span class="math notranslate nohighlight">\(t^{(N+1)}\)</span> given <span class="math notranslate nohighlight">\(\boldsymbol{t}_N\)</span>. The independent variables <span class="math notranslate nohighlight">\(\boldsymbol{X}_{N+1}\)</span> are also given, but will be omitted from the conditional pdfs below.</p>
<p>The joint density is</p>
<div class="amsmath math notranslate nohighlight" id="equation-234d4feb-bd06-4e06-9a0d-6db786e1bf69">
<span class="eqno">(22.24)<a class="headerlink" href="#equation-234d4feb-bd06-4e06-9a0d-6db786e1bf69" title="Permalink to this equation">#</a></span>\[\begin{equation}

p \left( t^{(N+1)}, \boldsymbol{t}_N \right) = p \left( t^{(N+1)} | \boldsymbol{t}_N \right) p \left( \boldsymbol{t}_N \right) 
\quad \Rightarrow \quad
p \left( t^{(N+1)} | \boldsymbol{t}_N \right) = \frac{p \left( t^{(N+1)}, \boldsymbol{t}_N \right)}{p \left( \boldsymbol{t}_N \right) }.

\end{equation}\]</div>
<p>First, let us note that <span class="math notranslate nohighlight">\(\boldsymbol{t}_{N+1} = \left\{ \boldsymbol{t}_N, t^{(N+1)} \right\}\)</span> such that <span class="math notranslate nohighlight">\(p \left( t^{(N+1)}, \boldsymbol{t}_N \right) = p \left(  \boldsymbol{t}_{N+1} \right)\)</span>.</p>
<p>Since both <span class="math notranslate nohighlight">\(p \left( \boldsymbol{t}_{N+1} \right)\)</span> and <span class="math notranslate nohighlight">\(p \left( \boldsymbol{t}_N \right)\)</span> are Gaussian distributions, then the conditional distribution, obtained by the ratio, must also be a Gaussian. Let us use the notation <span class="math notranslate nohighlight">\(\boldsymbol{C}_{N+1}\)</span> for the <span class="math notranslate nohighlight">\((N+1) \times (N+1)\)</span> covariance matrix for <span class="math notranslate nohighlight">\(\boldsymbol{t}_{N+1}\)</span>. This implies that</p>
<div class="amsmath math notranslate nohighlight" id="equation-f0212383-20e2-4a19-89cf-c15b7584ec18">
<span class="eqno">(22.25)<a class="headerlink" href="#equation-f0212383-20e2-4a19-89cf-c15b7584ec18" title="Permalink to this equation">#</a></span>\[\begin{equation}

p \left( \boldsymbol{t}_{N+1} \right) \propto \exp \left[ -\frac{1}{2} \left( \boldsymbol{t}_N, t^{(N+1)} \right) \boldsymbol{C}_{N+1}^{-1} 
\begin{pmatrix}
\boldsymbol{t}_N \\
t^{(N+1)}
\end{pmatrix}
\right]

\end{equation}\]</div>
<div class="admonition-summary admonition">
<p class="admonition-title">Summary</p>
<p>The prediction of the (Gaussian) pdf for <span class="math notranslate nohighlight">\(t^{(N+1)}\)</span> requires an inversion of the covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{C}_{N+1}\)</span>.</p>
</div>
<section id="elegant-linear-algebra-tricks-to-obtain-boldsymbol-c-n-1-1">
<h3>Elegant linear algebra tricks to obtain <span class="math notranslate nohighlight">\(\boldsymbol{C}_{N+1}^{-1}\)</span><a class="headerlink" href="#elegant-linear-algebra-tricks-to-obtain-boldsymbol-c-n-1-1" title="Link to this heading">#</a></h3>
<p>Let us split the <span class="math notranslate nohighlight">\(\boldsymbol{C}_{N+1}\)</span> covariance matrix into four different blocks</p>
<div class="amsmath math notranslate nohighlight" id="equation-6e3621de-e467-4ec7-adf9-a86eee89371e">
<span class="eqno">(22.26)<a class="headerlink" href="#equation-6e3621de-e467-4ec7-adf9-a86eee89371e" title="Permalink to this equation">#</a></span>\[\begin{equation}

\boldsymbol{C}_{N+1} =
\begin{pmatrix}
\boldsymbol{C}_N &amp; \boldsymbol{k} \\
\boldsymbol{k}^T &amp; \kappa
\end{pmatrix},

\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{C}_N\)</span> is the <span class="math notranslate nohighlight">\(N \times N\)</span> covariance matrix (which depends on the positions <span class="math notranslate nohighlight">\(\boldsymbol{X}_N\)</span>), <span class="math notranslate nohighlight">\(\boldsymbol{k}\)</span> is an <span class="math notranslate nohighlight">\(N \times 1\)</span> vector (that describes the covariance of <span class="math notranslate nohighlight">\(\boldsymbol{X}_N\)</span> with <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(N+1)}\)</span>), while <span class="math notranslate nohighlight">\(\kappa\)</span> is the single diagonal element obtained from <span class="math notranslate nohighlight">\(\boldsymbol{x}^{(N+1)}\)</span>.</p>
<p>We can use the partitioned inverse equations (Barnett, 1979) to rewrite <span class="math notranslate nohighlight">\(\boldsymbol{C}_{N+1}^{-1}\)</span> in terms of <span class="math notranslate nohighlight">\(\boldsymbol{C}_{N}^{-1}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{C}_{N+1}\)</span> as follows</p>
<div class="amsmath math notranslate nohighlight" id="equation-1f079bd5-e5fc-4324-9735-68676f5ea6d3">
<span class="eqno">(22.27)<a class="headerlink" href="#equation-1f079bd5-e5fc-4324-9735-68676f5ea6d3" title="Permalink to this equation">#</a></span>\[\begin{equation}

\boldsymbol{C}_{N+1}^{-1} =
\begin{pmatrix}
\boldsymbol{M}_N &amp; \boldsymbol{m} \\
\boldsymbol{m}^T &amp; \mu
\end{pmatrix},

\end{equation}\]</div>
<p>where</p>
<div class="amsmath math notranslate nohighlight" id="equation-17ddc26c-7e66-498d-81b0-45e02ee1d4da">
<span class="eqno">(22.28)<a class="headerlink" href="#equation-17ddc26c-7e66-498d-81b0-45e02ee1d4da" title="Permalink to this equation">#</a></span>\[\begin{align}
\mu &amp;= \left( \kappa - \boldsymbol{k}^T \boldsymbol{C}_N^{-1} \boldsymbol{k} \right)^{-1} \\
\boldsymbol{m} &amp;= -\mu \boldsymbol{C}_N^{-1} \boldsymbol{k} \\
\boldsymbol{M}_N &amp;= \boldsymbol{C}_N^{-1} + \frac{1}{\mu} \boldsymbol{m} \boldsymbol{m}^T.
\end{align}\]</div>
<div class="admonition-question admonition">
<p class="admonition-title">Question</p>
<p>Check that the dimensions of the different blocks are correct.</p>
</div>
<p>The prediction for <span class="math notranslate nohighlight">\(t^{(N+1)}\)</span> is a Gaussian</p>
<div class="math notranslate nohighlight" id="equation-eq-ptn1ratio">
<span class="eqno">(22.29)<a class="headerlink" href="#equation-eq-ptn1ratio" title="Link to this equation">#</a></span>\[\begin{split}
p \left( t^{(N+1)} | \boldsymbol{t}_N \right) 
= \frac{p \left( \boldsymbol{t}_{N+1} \right) }{p \left( \boldsymbol{t}_N \right) }
\propto \frac{\exp \left[ -\frac{1}{2} \left( \boldsymbol{t}_N, t^{(N+1)} \right) \boldsymbol{C}_{N+1}^{-1} 
\begin{pmatrix}
\boldsymbol{t}_N \\
t^{(N+1)}
\end{pmatrix}
\right]}{\exp \left[ -\frac{1}{2} \boldsymbol{t}_N^T \boldsymbol{C}_{N}^{-1} 
\boldsymbol{t}_N \right]},
\end{split}\]</div>
<p>which can be written as a univariate Gaussian (see below).</p>
<div class="tip admonition">
<p class="admonition-title">A new target prediction using a GP</p>
<p>The prediction for <span class="math notranslate nohighlight">\(t^{(N+1)}\)</span> is a Gaussian</p>
<div class="math notranslate nohighlight" id="equation-eq-ptn1">
<span class="eqno">(22.30)<a class="headerlink" href="#equation-eq-ptn1" title="Link to this equation">#</a></span>\[
p \left( t^{(N+1)} | \boldsymbol{t}_N \right) 
= \frac{1}{Z} \exp
\left[
-\frac{\left( t^{(N+1)} - \hat{t}^{(N+1)} \right)^2}{2 \sigma_{\hat{t}_{N+1}}^2}
\right].
\]</div>
<p>The mean and variance are obtained from <a class="reference internal" href="#equation-eq-ptn1ratio">(22.29)</a> after some algebra</p>
<div class="amsmath math notranslate nohighlight" id="equation-c70ff701-cecb-4d6b-8eeb-6a1c82669b76">
<span class="eqno">(22.31)<a class="headerlink" href="#equation-c70ff701-cecb-4d6b-8eeb-6a1c82669b76" title="Permalink to this equation">#</a></span>\[\begin{align}
\mathrm{mean:} &amp; \quad \hat{t}^{(N+1)} = \boldsymbol{k}^T \boldsymbol{C}_N^{-1} \boldsymbol{t}_N \\
\mathrm{variance:} &amp; \quad \sigma_{\hat{t}_{N+1}}^2 = \kappa - \boldsymbol{k}^T \boldsymbol{C}_N^{-1} \boldsymbol{k}.
\end{align}\]</div>
</div>
<p>This implies that we can make a prediction for the Gaussian pdf of <span class="math notranslate nohighlight">\(t^{(N+1)}\)</span> (meaning that we predict its value with an associated uncertainty) for an <span class="math notranslate nohighlight">\(N^3\)</span> computational cost (the inversion of an <span class="math notranslate nohighlight">\(N \times N\)</span> matrix).</p>
<p>In fact, since the prediction only depends on the <span class="math notranslate nohighlight">\(N\)</span> available data we might as well predict several new target values at once. Consider <span class="math notranslate nohighlight">\(\boldsymbol{t}_M = \{ t^{(N+i)} \}_{i=1}^M\)</span> so that</p>
<div class="amsmath math notranslate nohighlight" id="equation-766018ec-62ff-471d-951c-c4b2ea611138">
<span class="eqno">(22.32)<a class="headerlink" href="#equation-766018ec-62ff-471d-951c-c4b2ea611138" title="Permalink to this equation">#</a></span>\[\begin{equation}

\boldsymbol{C}_{N+M} =
\begin{pmatrix}
\boldsymbol{C}_N &amp; \boldsymbol{k} \\
\boldsymbol{k}^T &amp; \boldsymbol{\kappa}
\end{pmatrix},

\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{k}\)</span> is now an <span class="math notranslate nohighlight">\(N \times M\)</span> matrix and <span class="math notranslate nohighlight">\(\boldsymbol{\kappa}\)</span> an <span class="math notranslate nohighlight">\(M \times M\)</span> matrix.</p>
<div class="tip admonition">
<p class="admonition-title">Many new target predictions using a GP</p>
<p>The prediction for <span class="math notranslate nohighlight">\(M\)</span> new targets <span class="math notranslate nohighlight">\(\boldsymbol{t}_M\)</span> becomes a multivariate Gaussian</p>
<div class="math notranslate nohighlight" id="equation-eq-ptm">
<span class="eqno">(22.33)<a class="headerlink" href="#equation-eq-ptm" title="Link to this equation">#</a></span>\[
p \left( \boldsymbol{t}_{M} | \boldsymbol{t}_N \right) = \frac{1}{Z} \exp
\left[
-\frac{1}{2} \left( \boldsymbol{t}_M - \hat{\boldsymbol{t}}_M \right)^T \boldsymbol{\Sigma}_M^{-1} \left( \boldsymbol{t}_M - \hat{\boldsymbol{t}}_M \right)
\right],
\]</div>
<p>where the <span class="math notranslate nohighlight">\(M \times 1\)</span> mean vector and <span class="math notranslate nohighlight">\(M \times M\)</span> covariance matrix are</p>
<div class="amsmath math notranslate nohighlight" id="equation-971073c4-4be2-42f7-80c0-450849757b2e">
<span class="eqno">(22.34)<a class="headerlink" href="#equation-971073c4-4be2-42f7-80c0-450849757b2e" title="Permalink to this equation">#</a></span>\[\begin{align}
\hat{\boldsymbol{t}}_M &amp;= \boldsymbol{k}^T \boldsymbol{C}_N^{-1} \boldsymbol{t}_N \\
\boldsymbol{\Sigma}_M &amp;= \boldsymbol{\kappa} - \boldsymbol{k}^T \boldsymbol{C}_N^{-1} \boldsymbol{k}.
\end{align}\]</div>
</div>
</section>
<section id="choosing-the-gp-model-hyperparameters">
<h3>Choosing the GP model hyperparameters<a class="headerlink" href="#choosing-the-gp-model-hyperparameters" title="Link to this heading">#</a></h3>
<p>Predictions can be made once we have</p>
<ol class="arabic simple">
<li><p>Chosen an appropriate covariance function.</p></li>
<li><p>Determined the hyperparameters.</p></li>
<li><p>Evaluated the relevant blocks in the covariance function and inverted <span class="math notranslate nohighlight">\(\boldsymbol{C}_N\)</span>.</p></li>
</ol>
<p>How do we determine the hyperparameters <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}\)</span>? Well, recall that</p>
<div class="amsmath math notranslate nohighlight" id="equation-4a023dd1-a181-49ce-9811-1599aaa1ecb2">
<span class="eqno">(22.35)<a class="headerlink" href="#equation-4a023dd1-a181-49ce-9811-1599aaa1ecb2" title="Permalink to this equation">#</a></span>\[\begin{equation}

p \left( \boldsymbol{t}_N \right) = \frac{1}{Z_N} \exp \left[ -\frac{1}{2} \boldsymbol{t}_N^T \boldsymbol{C}_{N}^{-1} \boldsymbol{t}_N 
\right].

\end{equation}\]</div>
<p>This pdf is basically a data likelihood.</p>
<ul class="simple">
<li><p>The simplest approach is therefore to find the set of hyperparameters <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}^*\)</span> that maximizes the data likelihood, i.e.,</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-alphastar">
<span class="eqno">(22.36)<a class="headerlink" href="#equation-eq-alphastar" title="Link to this equation">#</a></span>\[
\boldsymbol{\alpha}^* = \underset{\alpha}{\operatorname{argmin}}  \boldsymbol{t}_N^T \boldsymbol{C}_{N}^{-1}(\alpha) \boldsymbol{t}_N
\]</div>
<ul class="simple">
<li><p>A Bayesian approach would be to assign a prior to the hyperparameters and seek a posterior pdf <span class="math notranslate nohighlight">\(p(\boldsymbol{\alpha} | \boldsymbol{t}_N)\)</span> instead which is then propagated using marginalization</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-eq-tn1marg">
<span class="eqno">(22.37)<a class="headerlink" href="#equation-eq-tn1marg" title="Link to this equation">#</a></span>\[
p \left( t^{(N+1)} | \boldsymbol{t}_N \right) = \int d\boldsymbol{\alpha} p \left( t^{(N+1)}, \boldsymbol{\alpha} | \boldsymbol{t}_N \right)
= \int d\boldsymbol{\alpha} p \left( t^{(N+1)} | \boldsymbol{t}_N, \boldsymbol{\alpha} \right) p(\boldsymbol{\alpha} | \boldsymbol{t}_N)
\]</div>
<p>The optimization approach is absolutely dominating the literature on GP regression. The data likelihood is maximized with respect to the hyperparameters and the resulting covariance function is then used for regression. The second approach gives a better quantification of the uncertainties, but is more computationally demanding.</p>
</section>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./LearningFromData-content/MachineLearning/GP"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="RootGP.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">22. </span>Overview of Gaussian process</p>
      </div>
    </a>
    <a class="right-next"
       href="CF/demo-GaussianProcesses.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Demonstration: Gaussian processes</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-using-gaussian-processes">Inference using Gaussian processes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parametric-approach">Parametric approach</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-covariance-matrix-as-the-central-object">The covariance matrix as the central object</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-parametric-approach-mean-and-covariance-functions">Non-parametric approach: Mean and covariance functions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stationary-kernels">Stationary kernels</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gp-models-for-regression">GP models for regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elegant-linear-algebra-tricks-to-obtain-boldsymbol-c-n-1-1">Elegant linear algebra tricks to obtain <span class="math notranslate nohighlight">\(\boldsymbol{C}_{N+1}^{-1}\)</span></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-the-gp-model-hyperparameters">Choosing the GP model hyperparameters</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christian Forssén, Dick Furnstahl, and Daniel Phillips
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>