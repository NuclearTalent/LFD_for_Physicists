
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>21.5. Machine Learning: First Examples &#8212; Combined Learning from Data materials</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/exercise.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/coloredpages.css?v=0a037ad7" />
    <link rel="stylesheet" type="text/css" href="../../../_static/myadmonitions.css?v=89ac28d1" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"loader": {"load": ["[tex]/textmacros"]}, "chtml": {"mtextInheritFont": true}, "tex": {"packages": {"[+]": ["textmacros"]}, "macros": {"data": "\\mathcal{D}", "pars": "\\boldsymbol{\\theta}", "para": "\\theta", "optpars": "\\pars^*", "optpara": "\\para^*", "prob": "\\mathbb{P}", "cprob": ["\\prob\\left( #1 \\, \\left\\vert \\, #2 \\right. \\right)", 2], "cprobsub": ["\\prob_{#1}\\left( #2 \\, \\left\\vert \\, #3 \\right. \\right)", 3], "pdf": ["p \\left( #1 \\, \\left\\vert \\, #2 \\right. \\right)", 2], "pdfsub": ["p_{#1} \\left( #2 \\, \\left\\vert \\, #3 \\right. \\right)", 3], "p": ["p \\left( #1 \\right)", 1], "psub": ["p_{#1} \\left( #2 \\right)", 2], "futuredata": "\\mathcal{F}", "expect": ["\\mathbb{E} \\left[ #1 \\right]", 1], "var": ["\\text{Var} \\left( #1 \\right)", 1], "std": ["\\text{Std} \\left( #1 \\right)", 1], "cov": ["\\text{Cov} \\left( #1, #2 \\right)", 2], "dmat": "\\boldsymbol{X}", "models": ["\\boldsymbol{M}\\left( #1 \\, ; \\, #2 \\right)", 2], "model": ["M\\left( #1 \\, ; \\, #2 \\right)", 2], "modeloutputs": "\\boldsymbol{M}", "modeloutput": "M", "MLmodel": ["\\boldsymbol{\\hat{y}}\\left( #1 \\right)", 1], "MLoutputs": "\\boldsymbol{\\hat{y}}", "MLoutput": "\\hat{y}", "outputs": "\\boldsymbol{y}", "inputs": "\\boldsymbol{x}", "targets": "\\boldsymbol{t}", "weights": "\\boldsymbol{w}", "testoutputs": "\\boldsymbol{y}^\\odot", "testinputs": "\\boldsymbol{x}^\\odot", "output": "y", "inputt": "x", "target": "t", "weight": "w", "testoutput": "y^\\odot", "MLtestoutput": "\\hat{y}^\\odot", "testinput": "x^\\odot", "trainingdata": "\\mathcal{T}", "LaTeX": "\\text{LaTeX}", "residual": "\\epsilon", "residuals": "\\boldsymbol{\\epsilon}", "zeros": "\\boldsymbol{0}", "covres": "\\boldsymbol{\\Sigma_{\\epsilon}}", "covpars": "\\boldsymbol{\\Sigma_{\\pars}}", "tildecovpars": "\\boldsymbol{\\widetilde{\\Sigma}_{\\pars}}", "sigmas": "\\boldsymbol{\\sigma}", "sigmai": "\\sigma_i", "sigmares": "\\sigma_{\\epsilon}", "cbar": "\\bar c", "Lra": "\\Longrightarrow", "yth": "y_{\\text{th}}", "yexp": "y_{\\text{exp}}", "ym": "y_{\\text{m}}", "thetavec": "\\boldsymbol{\\theta}", "parsLR": "\\boldsymbol{\\beta}", "paraLR": "\\beta", "covparsLR": "\\boldsymbol{\\Sigma_{\\parsLR}}", "optparsLR": "\\parsLR^*", "optparaLR": "\\paraLR^*", "tildecovparsLR": "\\boldsymbol{\\widetilde{\\Sigma}_{\\parsLR}}", "alphavec": "\\boldsymbol{\\alpha}", "muvec": "\\boldsymbol{\\mu}", "phivec": "\\boldsymbol{\\phi}", "betavec": "\\boldsymbol{\\beta}", "sigmavec": "\\boldsymbol{\\sigma}", "Sigmavec": "\\boldsymbol{\\Sigma}", "thetavechat": "\\widehat\\thetavec", "avec": "\\boldsymbol{a}", "Bvec": "\\boldsymbol{B}", "fvec": "\\boldsymbol{f}", "mvec": "\\boldsymbol{m}", "qvec": "\\boldsymbol{q}", "rvec": "\\boldsymbol{r}", "uvec": "\\boldsymbol{u}", "wvec": "\\boldsymbol{w}", "xvec": "\\boldsymbol{x}", "yvec": "\\boldsymbol{y}", "wt": "\\widetilde", "nb": "n_b", "mel": ["\\langle #1 | #2 | #3 \\rangle", 3], "qoi": "\\mathbf{Q}"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'LearningFromData-content/MachineLearning/ANN/MachineLearningExamples';</script>
    <script src="../../../_static/custom.js?v=33f35b7a"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="21.6. Exercise: Logistic Regression and neural networks" href="NeuralNet/exercises_LogReg_NeuralNet.html" />
    <link rel="prev" title="21. Logistic Regression" href="../LogReg/LogReg.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../Intro/About.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/logo-copilot.png" class="logo__image only-light" alt="Combined Learning from Data materials - Home"/>
    <script>document.write(`<img src="../../../_static/logo-copilot.png" class="logo__image only-dark" alt="Combined Learning from Data materials - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../Intro/About.html">
                    About this Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Intro/Overview.html">1. Invitation to inductive inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Intro/Introduction.html">2. Introduction</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I: Bayesian methods for scientific modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/RootBayesianBasics.html">3. Overview of Part I</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Inferenceandpdfs.html">4. Inference and PDFs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/exercise_sum_product_rule.html">4.5. Exercise: Checking the sum and product rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/exercise_medical_example_by_Bayes.html">4.6. Exercise: Standard medical example using Bayes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/MoreBayesTheorem.html">4.7. More on Bayesâ€™ theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Bayesian_epistemology.html">4.8. *Aside: Bayesian epistemology</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/DataModelsPredictions.html">4.9. Data, models, and predictions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../BayesianStatistics/BayesianParameterEstimation/parameter_estimation_Gaussian_noise.html">Parameter estimation I: Gaussian mean and variance</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Posteriors.html">5. Bayesian posteriors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Exploring_pdfs.html">5.1. Exploring PDFs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Exploring_pdfs_followups.html">Follow-ups to Exploring PDFs</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Gaussians.html">5.2. Gaussians: A couple of frequentist connections</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/visualization_of_CLT.html">Visualization of the Central Limit Theorem</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianParameterEstimation/Interpreting2Dposteriors.html">5.3. Interpreting 2D posteriors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/chi_squared_tests.html">5.4. Demonstration: Sum of normal variables squared</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/CoinTossing.html">6. Updating via Bayes' rule</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/CoinTossing/sec-01-coin-tossing-frequentists-and-bayesaians.html">6.1. Coin tossing: Frequentists and Bayesaians</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/CoinTossing/sec-02-when-do-priors-matter-when-don-t-they-matter.html">6.2. When do priors matter? When donâ€™t they matter?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/CoinTossing/sec-03-computing-the-posterior-analytically.html">6.3. Computing the posterior analytically</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/CoinTossing/sec-04-degree-of-belief-credibility-intervals-vs-frequentist-1-sigm.html">6.4. Degree of belief/credibility intervals vs frequentist 1-sigma intervals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/CoinTossing/sec-05-take-aways-and-follow-up-questions-from-coin-flipping.html">6.5. Take-aways and follow-up questions from coin flipping:</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/demo-BayesianBasics.html">6.6. ðŸš€ Demonstration:  Bayesian Coin Tossing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/Bayesian_updating_coinflip_interactive.html">6.7. Widgetized coin tossing</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/UsingBayes.html">7. Bayes in practice</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianBasics/BayesianAdvantages.html">7.1. Advantages of the Bayesian approach</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianWorkflow/BayesianWorkflow.html">7.2. Bayesian research workflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianLinearRegression/BayesianLinearRegression_rjf.html">7.3. Bayesian Linear Regression (BLR)</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II: Advanced Bayesian methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../BayesianStatistics/RootAdvancedMethods.html">8. Overview of Part II</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../BayesianStatistics/BayesianParameterEstimation/ParameterEstimation.html">9. More Bayesian parameter estimation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianParameterEstimation/radioactive_lighthouse_exercise.html">9.1. Radioactive lighthouse problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianParameterEstimation/amplitude_in_presence_of_background.html">9.2. Amplitude of a signal in the presence of background</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianParameterEstimation/parameter_estimation_fitting_straight_line_I.html">9.3. Parameter estimation example: fitting a straight line</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianParameterEstimation/parameter_estimation_fitting_straight_line_II.html">9.4. Parameter estimation example: fitting a straight line II</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/BayesianParameterEstimation/dealing_with_outliers.html">9.5. Dealing with outliers</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../BayesianStatistics/AssigningProbabilities/Assigning.html">10. Assigning probabilities</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../BayesianStatistics/AssigningProbabilities/IgnorancePDF.html">10.1. Assigning probabilities (I): Indifferences and translation groups</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../BayesianStatistics/AssigningProbabilities/demo-straight_lines.html">Alternative notebook with MCMC sampling</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/AssigningProbabilities/MaxEnt2.html">10.2. Assigning probabilities (II): The principle of maximum entropy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/AssigningProbabilities/MaxEnt_Function_Reconstruction.html">10.3. Maximum Entropy for reconstructing a function from its moments</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../BayesianStatistics/ComputationalBayes/BayesLinear.html">11. Bayes goes linear: History matching</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../BayesianStatistics/Multimodel_inference.html">12. Multi-model inference with Bayes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../BayesianStatistics/ModelSelection/ModelSelection.html">12.1. Model Selection</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../BayesianStatistics/ModelSelection/BUQ/Evidence_for_model_EFT_coefficients.html">Evidence calculation for EFT expansions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../BayesianStatistics/ModelSelection/BUQ/two_model_evidence.html">Follow-up to EFT evidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../BayesianStatistics/ModelSelection/BUQ/computing_evidence.html">Computing the evidence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../BayesianStatistics/ModelSelection/BUQ/MCMC-parallel-tempering_ptemcee_vs_zeus.html">Demo: Multimodal distributions with two samplers</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/ModelMixing/model_mixing.html">12.2. Model averaging and mixing </a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/DiscrepancyModels.html">13. Discrepancy Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/MD_balldrop_v1.html">13.4. Ball-drop experiment</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III: MCMC sampling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../StochasticProcesses/RootMCMC.html">14. Overview of Part III</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/StochasticProcesses.html">15. Stochastic processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/Metropolis_Poisson_example.html">15.7. Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/demo-MCMC.html">15.8. Demonstration: Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/Recap_BUQ.html">15.9. Recap of Poisson and more about MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/parameter_estimation_Gaussian_noise-2.html">15.10. Parameter estimation example: Gaussian noise and averages II</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/MCMC-random-walk-and-sampling.html">15.11. Exercise: Random walk</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/MCMC_overview.html">16. Overview of Markov Chain Monte Carlo</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/MarkovChains.html">16.1. Markov chains</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/MCMC.html">16.2. Markov chain Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/MCMC_intro_BUQ.html">16.3. Alternative MCMC introduction (Gregory)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/Assignment_extending_radioactive_lighthouse.html">16.4. Assignment: 2D radioactive lighthouse location using MCMC</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/Advanced_MCMC.html">17. Advanced MCMC</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/ComputationalBayes/AdvancedMCMC.html">17.1. Advanced Markov chain Monte Carlo sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/MCMC-diagnostics.html">17.2. Overview: MCMC Diagnostics</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ/intuition_sampling.html">17.4. Intuition on sampling and best practices</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../StochasticProcesses/Other_samplers.html">18. HMC and other samplers</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../StochasticProcesses/BUQ2/HMC_intro_BUQ.html">18.1. Hamiltonian Monte Carlo (HMC) overview and visualization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../StochasticProcesses/BUQ2/Liouville_theorem_visualization.html">Liouville Theorem Visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../StochasticProcesses/BUQ2/Orbital_eqs_with_different_algorithms.html">Solving orbital equations with different algorithms</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/zeus.html">18.2. The Zeus Ensemble Slice Sampler</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../StochasticProcesses/BUQ2/PyMC_intro_updated.html">18.3. PyMC Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../StochasticProcesses/OverviewIntroPyMC.html">Overview of Intro to PyMC notebook</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../StochasticProcesses/BUQ2/parameter_estimation_Gaussian_noise_compare_samplers.html">18.4. Comparing samplers for a simple problem</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV: Machine learning: A Bayesian perspective</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../RootML.html">19. Overview of Part IV</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../GP/RootGP.html">20. Overview of Gaussian processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../GP/GaussianProcesses.html">20.4. Introduction to Gaussian processes</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../GP/CF/demo-GaussianProcesses.html">demo-GaussianProcesses notebook</a></li>
<li class="toctree-l3"><a class="reference internal" href="../GP/BUQ/lecture_20.html">GP recap; GP applications; (old lecture 20)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../GP/Sklearn_demos.html">20.5. scikit-learn demo notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../GP/BUQ/plot_gpr_noisy_targets.html">One-dimension regression example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../GP/BUQ/plot_gpr_prior_posterior.html">Prior and posterior with different kernels</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../GP/GPy_demos.html">20.6. GPy demo notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../GP/BUQ/demo-GaussianProcesses.html">Gaussian processes demonstration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../GP/CF/exercise_GP_GPy.html">Exercise: Gaussian processes using <code class="docutils literal notranslate"><span class="pre">GPy</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../GP/BUQ/Gaussian_processes_exercises.html">Exercise: Gaussian Process models with GPy</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../LogReg/LogReg.html">21. Logistic Regression</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">21.5. Machine Learning: First Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="NeuralNet/exercises_LogReg_NeuralNet.html">21.6. Exercise: Logistic Regression and neural networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="MachineLearning.html">22. Machine learning: Overview and notation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="NeuralNet.html">22.5. Artifical neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="NeuralNet/demo-NeuralNet.html">22.6. Demonstration: Neural network classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="Neural_Network_for_simple_function_in_PyTorch.html">22.7. ANN from ChatGPT using PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="ModelValidation.html">22.8. Model validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="DataBiasFairness.html">22.9. Data bias and fairness in machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="NeuralNet/NeuralNetBackProp.html">22.10. *Neural networks: Backpropagation</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/ANNFT.html">23. ANNs in the large-width limit (ANNFT)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/random_initialized_ANN_vs_width.html">23.3. Distributions of Randomly-Initialized ANNs</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../BNN/bnn.html">24. Bayesian neural nets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../BNN/demo-bnn.html">24.4. Demonstration: Variational Inference and Bayesian Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../BNN/exercises_BNN.html">24.5. Exercise: Bayesian neural networks</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../CNN/cnn.html">25. *Convolutional neural nets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../CNN/demo-cnn.html">25.6. Demonstration: Image recognition with Convolutional Neural Networks</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V: Other topics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../OtherTopics/RootOtherTopics.html">26. Overview of Part V </a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/Emulators.html">27. Emulators</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/ComputationalBayes/BayesFast.html">27.1. Bayes goes fast: Emulators (from CF)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../BayesianStatistics/ComputationalBayes/extra_RBM_emulators.html">27.2. RBM emulators (BUQ)</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../OtherTopics/Student_t_distribution_from_Gaussians.html">28. Student t distribution from Gaussians</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../OtherTopics/SVD.html">29. PCA, SVD, and all that</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../OtherTopics/linear_algebra_games_including_SVD.html">29.5. SVD notebook</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../OtherTopics/qbism.html">30. QBism: Bayesian quantum mechanics</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Backmatter</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Backmatter/bibliography.html">31. Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Backmatter/JB_tests.html">32. Guide to Jupyter Book markdown</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix A: Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Reference/Statistics.html">33. Statistics concepts and notation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/GradientDescent.html">34. Gradient-descent optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix B: Scientific modeling</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/RootScientificModeling.html">35. Overview of scientific modeling material</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/OverviewModeling.html">36. Overview of modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/LinearModels.html">37. Linear models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ModelingOptimization/MathematicalOptimization.html">38. Mathematical optimization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendix C: Getting started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Setup/RootGettingStarted.html">39. Overview of Getting started material</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Setup/exercise_Intro_01_Jupyter_Python.html">40. Jupyter notebooks and Python</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Setup/more_python_and_jupyter.html">41. More about Python and Jupyter notebooks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/exercise_Intro_02_Jupyter_Python.html">41.4. Python lists and iterations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/exercise_Intro_03_Numpy.html">41.5. Linear algebra operations with NumPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/demo-Intro.html">41.6. Reading data and fitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/Simple_widgets_v1.html">41.7. Making a simple widget-based UI</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Setup/setting_up.html">42. Setting up for using this Jupyter Book</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/installing_anaconda.html">42.1. Using Anaconda</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Setup/using_github.html">42.2. Using GitHub</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">TALENT mini-projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/RootMiniProjects.html">Overview of mini-projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/mini-project_I_toy_model_of_EFT.html">MP I: Parameter estimation for a toy model of an EFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/model-selection_mini-project-IIa.html">MP IIa: Model selection basics</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Mini-projects/model-selection_mini-project-IIb_How_many_lines_ptemcee.html">MP IIb: How many lines?</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Mini-projects/Mini-project_IIb_overview.html">Overview of Mini-project IIb: How many lines?</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/mini-project_IIIa_bayesian_optimization.html">MP IIIa: Bayesian optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../Mini-projects/mini-project_IIIb_Bayesian_neural_networks_from_demo.html">MP IIIb: Bayesian Neural Networks</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/NuclearTalent/LFD_for_Physicists/master?urlpath=tree/./LearningFromData-content/MachineLearning/ANN/MachineLearningExamples.md" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="../../../_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/NuclearTalent/LFD_for_Physicists" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/NuclearTalent/LFD_for_Physicists/issues/new?title=Issue%20on%20page%20%2FLearningFromData-content/MachineLearning/ANN/MachineLearningExamples.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/LearningFromData-content/MachineLearning/ANN/MachineLearningExamples.ipynb" target="_blank"
   class="btn btn-sm btn-download-notebook-button dropdown-item"
   title="Download notebook file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li><a href="../../../_sources/LearningFromData-content/MachineLearning/ANN/MachineLearningExamples.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Machine Learning: First Examples</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-scores">Training scores</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-classification">Linear classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-perceptron-classifier">The perceptron classifier</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-soft-classifier">The soft classifier</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors-classification"><span class="math notranslate nohighlight">\(k\)</span> nearest neighbors classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-examples-binary-classifiers">Code examples: binary classifiers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-normalization">Data normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-classifier-s">Linear classifier(s)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#knn-classifier"><span class="math notranslate nohighlight">\(k\)</span>NN classifier</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions">Solutions</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="machine-learning-first-examples">
<span id="sec-linearregressionml"></span><h1><span class="section-number">21.5. </span>Machine Learning: First Examples<a class="headerlink" href="#machine-learning-first-examples" title="Link to this heading">#</a></h1>
<p>In this section we consider two simple, but powerful, machine learning approaches: the <span class="math notranslate nohighlight">\(k\)</span>-nearest neighbor algorithm and the linear model (already encountered in the first part on scientific inference).</p>
<p>We will first consider the regression problem using the linear model, which by now should be a rather familiar task. However, we will dress it in the machine learning language.</p>
<p>Then we will consider a simple binary classification problem with two predictor variables. The linear model makes strong assumptions concerning structure and thereby yields stable but possibly inaccurate predictions. The method of <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors makes very mild structural assumptions which yields predictions that are often accurate but the results are unstable and strongly dependent on the training data.</p>
<section id="linear-regression">
<h2>Linear regression<a class="headerlink" href="#linear-regression" title="Link to this heading">#</a></h2>
<p>Regression modeling deals with the description of a <strong>response</strong> variable(s) <span class="math notranslate nohighlight">\(\outputs\)</span> and how it varies as function of some <strong>predictor</strong> variable(s) <span class="math notranslate nohighlight">\(\inputs\)</span>. In machine learning the predictor variables are often known as <em>features</em>.</p>
<p>Here we will consider the special case in which there is a single response variable but several features.</p>
<p>When performing a regression analysis we will have access to a set of training data <span class="math notranslate nohighlight">\(\trainingdata\)</span> that consists of:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(N\)</span> cases, indexed by <span class="math notranslate nohighlight">\(i = 1, 2, \dots, N\)</span></p></li>
</ul>
<p>For each case there is a</p>
<ul class="simple">
<li><p>response variable <span class="math notranslate nohighlight">\(\output_i\)</span> (observation);</p></li>
<li><p>vector of features <span class="math notranslate nohighlight">\(\inputs_i\)</span> (input).</p></li>
</ul>
<dl class="simple myst">
<dt>Features</dt><dd><p>The key to a successful linear regression analysis is to identify the most relevant features. In physics, these would correspond to a set of basis functions. It is common to include a bias term as a feature, that is we might start the feature indexing at zero and assign <span class="math notranslate nohighlight">\(x_0=1\)</span>.</p>
</dd>
</dl>
<p>Assume that there are <span class="math notranslate nohighlight">\(p\)</span> features, including the bias. We will use the (possibly confusing) notation</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\inputs_i=[x_{0,i}, x_{1,i}, \dots, x_{p-1,i}]\)</span>, with <span class="math notranslate nohighlight">\(x_{0,i} = 1 \, \forall i\)</span>.</p></li>
</ul>
<p>Following <a class="reference internal" href="../../ModelingOptimization/LinearModels.html#sec-linearmodels"><span class="std std-ref">Linear models</span></a> we will apply the linear machine learning model</p>
<div class="amsmath math notranslate nohighlight" id="equation-14fb2b3b-a80a-4c29-9817-54c07d1387e8">
<span class="eqno">(21.16)<a class="headerlink" href="#equation-14fb2b3b-a80a-4c29-9817-54c07d1387e8" title="Permalink to this equation">#</a></span>\[\begin{equation}
\MLmodel{\weights \, ; \, \inputs_i} = \inputs_i \cdot \weights,
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\weights\)</span> is a <span class="math notranslate nohighlight">\(p \times 1\)</span> column vector of model parameters. These are known as weights in this context. We might also simplify the notation and write <span class="math notranslate nohighlight">\(\MLoutput_i = \MLmodel{\inputs_i}\)</span>.</p>
<p>A machine learning regression analysis aims at finding the model parameters <span class="math notranslate nohighlight">\(\weights\)</span> such that a selected cost function is minimized. This optimization step  is the learning part of the model. The cost function is supposed to measure the performance of the model</p>
<p>The <span class="math notranslate nohighlight">\(p\)</span> features for the <span class="math notranslate nohighlight">\(N\)</span> cases in the training data set are collected in the <span class="math notranslate nohighlight">\(N \times p\)</span> design matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> such that we get the simple matrix form</p>
<div class="amsmath math notranslate nohighlight" id="equation-f76259f1-4240-41ae-8406-4734bab5c9b8">
<span class="eqno">(21.17)<a class="headerlink" href="#equation-f76259f1-4240-41ae-8406-4734bab5c9b8" title="Permalink to this equation">#</a></span>\[\begin{equation}
\MLoutputs = \mathbf{X} \weights
\end{equation}\]</div>
<p>The most common metric to quantify the performance of the model is the mean-squared error (MSE)</p>
<div class="math notranslate nohighlight" id="equation-eq-mlexamples-mse">
<span class="eqno">(21.18)<a class="headerlink" href="#equation-eq-mlexamples-mse" title="Link to this equation">#</a></span>\[
\mathrm{MSE}(\weights) = \frac{1}{N}\sum_{i=1}^{N}\left(\output_i-\MLoutput_i\right)^2.
\]</div>
<p>Using this metric as a cost function, <span class="math notranslate nohighlight">\(C(\weights)\)</span>, and using the more compact matrix-vector notation we have</p>
<div class="math notranslate nohighlight" id="equation-eq-mlexamples-mse-cost">
<span class="eqno">(21.19)<a class="headerlink" href="#equation-eq-mlexamples-mse-cost" title="Link to this equation">#</a></span>\[
C(\weights)=\frac{1}{N} \left(\outputs-\boldsymbol{X}\weights\right)^T\left(\outputs-\boldsymbol{X}\weights\right).
\]</div>
<p>In general, the minimization of the cost function is an optimization problem that must be approached with numerical techniques such as <em>gradient descent</em>. For linear regression, however, the learning can be achieved analytically. The minimum of Eq. <a class="reference internal" href="#equation-eq-mlexamples-mse-cost">(21.19)</a> is found by solving the normal equation, and if <span class="math notranslate nohighlight">\(\boldsymbol{X}^T\boldsymbol{X}\)</span> is invertible we have the optimal model parameters</p>
<div class="amsmath math notranslate nohighlight" id="equation-1c19bb5f-5f87-4643-8a12-d98127b19f2c">
<span class="eqno">(21.20)<a class="headerlink" href="#equation-1c19bb5f-5f87-4643-8a12-d98127b19f2c" title="Permalink to this equation">#</a></span>\[\begin{equation}
\weights^* = \left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}\boldsymbol{X}^T\outputs.
\end{equation}\]</div>
<section id="training-scores">
<h3>Training scores<a class="headerlink" href="#training-scores" title="Link to this heading">#</a></h3>
<p>The mean-squared error <a class="reference internal" href="#equation-eq-mlexamples-mse">(21.18)</a> is just one possible choice for the cost function. In general, such metrics are also known as <strong>training scores</strong>.</p>
<p>Other choices are the <strong>mean absolute error</strong> (MAE), defined as</p>
<div class="amsmath math notranslate nohighlight" id="equation-22dbd9b0-5a96-457c-8b2b-61a8a75a6ef4">
<span class="eqno">(21.21)<a class="headerlink" href="#equation-22dbd9b0-5a96-457c-8b2b-61a8a75a6ef4" title="Permalink to this equation">#</a></span>\[\begin{equation}
\mathrm{MAE}(\weights) = \frac{1}{N} \sum_{i=1}^N \left| \output_{i} - \MLoutput_i \right|,
\end{equation}\]</div>
<p>and the <span class="math notranslate nohighlight">\(R2\)</span> score, also known as <em>coefficient of determination</em></p>
<div class="amsmath math notranslate nohighlight" id="equation-a1c7d0b0-897f-4c8a-8cb1-41b1aeed2910">
<span class="eqno">(21.22)<a class="headerlink" href="#equation-a1c7d0b0-897f-4c8a-8cb1-41b1aeed2910" title="Permalink to this equation">#</a></span>\[\begin{equation}
\mathrm{R2}(\weights) = 1 - \frac{\sum_{i=1}^N \left( \output_{i} - \MLoutput_{i} \right)^2}{\sum_{i=1}^N \left( \output_{i} - \bar{y} \right)^2},
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{y} = \frac{1}{N} \sum_{i=1}^N \MLoutput_{i}\)</span> is the mean of the model predictions.</p>
<p>For classification tasks one might also use the misclassification cost function (see <a class="reference internal" href="#exercise:MLexamples:misclassification-cost-function"><span class="std std-numref">Exercise 21.2</span></a>) or cross entropy. Misclassification is the relative number of misclassified instances in a data set and is therefore the same as <span class="math notranslate nohighlight">\(1-\alpha\)</span>, where <span class="math notranslate nohighlight">\(\alpha\)</span> is the accuracy.</p>
<div class="proof example admonition" id="example:MLexamples:binary-classification">
<p class="admonition-title"><span class="caption-number">Example 21.1 </span> (Binary classification)</p>
<section class="example-content" id="proof-content">
<p>Figure <a class="reference internal" href="#fig-example-mlexamples-binary-classification-data"><span class="std std-numref">Fig. 21.2</span></a> shows a scatterplot of training data that is a function of two predictor variables <span class="math notranslate nohighlight">\(\inputs_i = (x_1, x_2)_i\)</span>. The output <span class="math notranslate nohighlight">\(y\)</span> is a class that can be either Blue or Red.</p>
<p>The procedure and code that generate this data is included in the hidden code cell below. However, here we assume that we donâ€™t know the underlying data generating process. Our aim is to construct machine learning algorithms that can be trained on the collected data and make predictions for future data. The task can then be expressed as: Develop a computer program that can learn from labeled data <span class="math notranslate nohighlight">\(\trainingdata = \{ \inputs_i, \output_i\}_{i=1}^N\)</span> and make a prediction of the class <span class="math notranslate nohighlight">\(\testoutput\)</span> that a new input <span class="math notranslate nohighlight">\(\testinputs\)</span> belongs to.</p>
</section>
</div><figure class="align-default" id="fig-example-mlexamples-binary-classification-data">
<img alt="../../../_images/afd851e11eb36b408b1ad1dac8144c33725a00d2ee16889a0affa413d74dbc5e.png" src="../../../_images/afd851e11eb36b408b1ad1dac8144c33725a00d2ee16889a0affa413d74dbc5e.png" />
<figcaption>
<p><span class="caption-number">Fig. 21.2 </span><span class="caption-text">Labeled training data for a binary classification problem.</span><a class="headerlink" href="#fig-example-mlexamples-binary-classification-data" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="cell tag_hide-cell docutils container">
<details class="admonition hide above-input">
<summary aria-label="Toggle hidden content">
<p class="collapsed admonition-title">Show code cell content</p>
<p class="expanded admonition-title">Hide code cell content</p>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">stats</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">myst_nb</span><span class="w"> </span><span class="kn">import</span> <span class="n">glue</span>

<span class="c1"># Generate training data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">2022</span><span class="p">)</span>
<span class="c1"># Step 1</span>
<span class="c1"># - Blue data: generate 10 means mb_k from N([1,0],I)</span>
<span class="c1"># - Red data:  generate 10 means mr_k from N([0,1],I)</span>
<span class="c1"># Step 2</span>
<span class="c1"># - for each class:</span>
<span class="c1">#   - for observation in range(100):</span>
<span class="c1">#     - pick a mean, m_k, from the above list randomly (p=1/10)</span>
<span class="c1">#     - generate a data point from N(m_k,I/5)</span>
<span class="n">num_means</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span><span class="s1">&#39;red&#39;</span><span class="p">]</span> <span class="c1"># blue label is 0, red label is 1</span>
<span class="n">root_means</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">num_data_per_class</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">width</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="c1"># Step 1</span>
<span class="n">class_means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">iclass</span><span class="p">,</span> <span class="n">root_mean</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">root_means</span><span class="p">):</span>
    <span class="n">class_mean_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">iclass_mean</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_means</span><span class="p">):</span>
        <span class="n">mean_k</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">root_mean</span><span class="p">,</span><span class="n">cov</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>
        <span class="n">class_mean_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_k</span><span class="p">)</span>
    <span class="n">class_means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">class_mean_list</span><span class="p">)</span>
<span class="c1"># Step 2</span>
<span class="k">def</span><span class="w"> </span><span class="nf">generate_binaryclass_data</span><span class="p">(</span><span class="n">num_data_per_class</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">num_data_per_class</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">iclass</span><span class="p">,</span> <span class="n">class_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">classes</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">idata</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_data_per_class</span><span class="p">):</span>
            <span class="c1"># label the data</span>
            <span class="n">data</span><span class="p">[</span><span class="n">iclass</span><span class="o">*</span><span class="n">num_data_per_class</span><span class="o">+</span><span class="n">idata</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">iclass</span>
            <span class="c1"># populate the input</span>
            <span class="n">mean_k</span> <span class="o">=</span> <span class="n">class_means</span><span class="p">[</span><span class="n">iclass</span><span class="p">][</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">))]</span>
            <span class="n">xdata</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">mean_k</span><span class="p">,</span><span class="n">cov</span><span class="o">=</span><span class="n">width</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>
            <span class="n">data</span><span class="p">[</span><span class="n">iclass</span><span class="o">*</span><span class="n">num_data_per_class</span><span class="o">+</span><span class="n">idata</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">xdata</span>
    <span class="k">return</span> <span class="n">data</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">generate_binaryclass_data</span><span class="p">(</span><span class="n">num_data_per_class</span><span class="p">)</span>
<span class="c1"># Shuffle the data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>


<span class="n">fig_train_data</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">iclass</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">classes</span><span class="p">):</span>
    <span class="n">x_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="n">train_data</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span><span class="o">==</span><span class="n">iclass</span><span class="p">,:</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">x_data</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">,</span> <span class="s1">&#39;box&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;binary_classification_data_fig&quot;</span><span class="p">,</span> <span class="n">fig_train_data</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/afd851e11eb36b408b1ad1dac8144c33725a00d2ee16889a0affa413d74dbc5e.png" src="../../../_images/afd851e11eb36b408b1ad1dac8144c33725a00d2ee16889a0affa413d74dbc5e.png" />
</div>
</details>
</div>
</section>
</section>
<section id="linear-classification">
<h2>Linear classification<a class="headerlink" href="#linear-classification" title="Link to this heading">#</a></h2>
<p>Letâ€™s consider an application of the linear model for the binary classification task of <a class="reference internal" href="#example:MLexamples:binary-classification">Example 21.1</a>. We label these classes as 0 for Blue and 1 for Red such that <span class="math notranslate nohighlight">\(y_i \in \{0,1\}\)</span>. Let us first introduce a linear model whose output we label as <span class="math notranslate nohighlight">\(z\)</span></p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{z} = \boldsymbol{X} \weights.
\]</div>
<p>We again include a bias feature in the design matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> such that <span class="math notranslate nohighlight">\(\weights = [\weight_0, \weight_1, \weight_2]^T\)</span>. We note that each model output <span class="math notranslate nohighlight">\(z\)</span> is a continuous variable. How could this be used for the task of a binary classifier?</p>
<figure class="align-default" id="fig-mlexamples-fig-linear-classifier-plane">
<img alt="../../../_images/1f7bd8cfdf4f69b21adff612cc6a2bc8bee43d9e3d11bc13db13a6caa2fa969f.png" src="../../../_images/1f7bd8cfdf4f69b21adff612cc6a2bc8bee43d9e3d11bc13db13a6caa2fa969f.png" />
<figcaption>
<p><span class="caption-number">Fig. 21.3 </span><span class="caption-text">This plane is the linear regression (ordinary least squares) fit of a model <span class="math notranslate nohighlight">\(z = \weight_0 + \weight_1 \inputt_1 + \weight_2 \inputt_2\)</span> to the training data of <a class="reference internal" href="#fig-example-mlexamples-binary-classification-data"><span class="std std-numref">Fig. 21.2</span></a>. For visualization we indicate the line where it crosses the <span class="math notranslate nohighlight">\(z=0\)</span> plane and use red (blue) color to emphasize regions with positive (negative) <span class="math notranslate nohighlight">\(z\)</span>-value.</span><a class="headerlink" href="#fig-mlexamples-fig-linear-classifier-plane" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="the-perceptron-classifier">
<h3>The perceptron classifier<a class="headerlink" href="#the-perceptron-classifier" title="Link to this heading">#</a></h3>
<p>We can achieve a mapping from the continuous output <span class="math notranslate nohighlight">\(z\)</span> of the linear regressor to a discrete one <span class="math notranslate nohighlight">\(\MLoutput \in \{ 0, 1 \}\)</span> using the <em>perceptron</em> model which is usually formulated using the sign function as</p>
<div class="amsmath math notranslate nohighlight" id="equation-272fd9d2-edd4-47d1-b92e-acbda8dca3a2">
<span class="eqno">(21.23)<a class="headerlink" href="#equation-272fd9d2-edd4-47d1-b92e-acbda8dca3a2" title="Permalink to this equation">#</a></span>\[\begin{equation}
\MLoutput = \frac{\mathrm{sign}(z)+1}{2}.
\end{equation}\]</div>
<p>A binary classifier, providing a discrete output, can then be obtained accoring to the simple rule</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\MLoutput = \MLoutput(z) = \left\{ 
\begin{array}{ll}
\mathrm{Red} &amp; \text{if } z &gt; 0 \\
\mathrm{Blue} &amp; \text{if } z &lt; 0. \\
\end{array}
\right.
\end{split}\]</div>
<p>The two predicted classes will be separated by a <em>decision boundary</em> <span class="math notranslate nohighlight">\(\left\{ \inputs \, : \, \inputs \weights^* = 0 \right\}\)</span>. This boundary becomes a straight line in this case.</p>
</section>
<section id="the-soft-classifier">
<h3>The soft classifier<a class="headerlink" href="#the-soft-classifier" title="Link to this heading">#</a></h3>
<p>In many cases, it is more favorable to have a â€œsoftâ€ classifier that outputs a probability for an output to belong to a given category rather than providing a hard decision. Logistic regression is the most common example of such a soft classifier. In logistic
regression, the probability that input <span class="math notranslate nohighlight">\(\inputs\)</span> belongs to the class <span class="math notranslate nohighlight">\(c=1\)</span> is given by the so-called <em>logit</em> function (an example of a S-shape or <em>Sigmoid</em> function) ,</p>
<div class="math notranslate nohighlight" id="equation-eq-mlexamples-sigmoid">
<span class="eqno">(21.24)<a class="headerlink" href="#equation-eq-mlexamples-sigmoid" title="Link to this equation">#</a></span>\[
\MLoutput(z) = \prob(\output(\inputs)=1) = \frac{1}{1+e^{-z}} = \frac{e^z}{1+e^z},
\]</div>
<p>where the so called <em>activation</em> <span class="math notranslate nohighlight">\(z = \inputs \cdot \weights\)</span>.</p>
<ul class="simple">
<li><p>Note that <span class="math notranslate nohighlight">\(1-\MLoutput(z)= \MLoutput(-z)\)</span>.</p></li>
<li><p>The sigmoid function can be motivated in several different ways. E.g. in information theory this function represents the probability of a signal <span class="math notranslate nohighlight">\(s=1\)</span> rather than <span class="math notranslate nohighlight">\(s=0\)</span> when transmission occurs over a noisy channel.</p></li>
</ul>
<figure class="align-default" id="fig-mlexamples-fig-linear-classifier">
<img alt="../../../_images/b537a4cd114f3637812f8eb52625cb55c639b032722a7c8538a26daf7ed78d8b.png" src="../../../_images/b537a4cd114f3637812f8eb52625cb55c639b032722a7c8538a26daf7ed78d8b.png" />
<figcaption>
<p><span class="caption-number">Fig. 21.4 </span><span class="caption-text">Results of the hard (perceptron) and soft (sigmoid) linear classifier on the data from <a class="reference internal" href="#example:MLexamples:binary-classification">Example 21.1</a>. The model predictions for the hard classifier are represented by the red and blue regions on either side of the sharp decision boundary (left panel). For the soft classifier (right panel) the colors represent the model prediction for the probability of belonging to the red class. The black line marks the decision boundary where the model gives no preference for either class <span class="math notranslate nohighlight">\(\prob(\MLtestoutput=\text{Red}) = 0.5\)</span>. See also <a class="reference internal" href="#fig-mlexamples-fig-linear-classifier-plane"><span class="std std-numref">Fig. 21.3</span></a> for the corresponding activation <span class="math notranslate nohighlight">\(z\)</span> from the linear regression model.</span><a class="headerlink" href="#fig-mlexamples-fig-linear-classifier" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="k-nearest-neighbors-classification">
<h2><span class="math notranslate nohighlight">\(k\)</span> nearest neighbors classification<a class="headerlink" href="#k-nearest-neighbors-classification" title="Link to this heading">#</a></h2>
<p>Nearest-neighbor methods use the closest observations in the training data <span class="math notranslate nohighlight">\(\trainingdata\)</span> to make a prediction. Usually, the definition of â€œclosestâ€ is usually the Euclidean distance in the input space, i.e., the length <span class="math notranslate nohighlight">\(\left| \testinputs - \inputs_i \right|\)</span>. Specifically, the <span class="math notranslate nohighlight">\(k\)</span> nearest neighbor prediction for a target would be given by the average</p>
<div class="amsmath math notranslate nohighlight" id="equation-0d28dce1-f3dc-4adf-83f3-a009578b2404">
<span class="eqno">(21.25)<a class="headerlink" href="#equation-0d28dce1-f3dc-4adf-83f3-a009578b2404" title="Permalink to this equation">#</a></span>\[\begin{equation}
\tilde\output(\testinputs) = \frac{1}{k} \sum_{\inputs_i \in N_k(\testinputs)} \output_i,
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\( N_k(\testinputs)\)</span> is the neighborhood of <span class="math notranslate nohighlight">\(\testinputs\)</span> defined by the <span class="math notranslate nohighlight">\(k\)</span> nearest points <span class="math notranslate nohighlight">\(\testinputs_i\)</span> in the training data set. Forced to make a hard decision we would use the majority vote and assign <span class="math notranslate nohighlight">\(\MLtestoutput = 1\)</span> if <span class="math notranslate nohighlight">\(\tilde\output(\testinputs) &gt; 0.5\)</span>, and <span class="math notranslate nohighlight">\(\MLtestoutput = 0\)</span> if <span class="math notranslate nohighlight">\(\tilde\output(\testinputs) \leq 0.5\)</span>.</p>
<p>As shown in Fig. <a class="reference internal" href="#fig-mlexamples-fig-knn-classifier"><span class="std std-numref">Fig. 21.5</span></a> there are much fewer misclassifications in the training data set when using the <span class="math notranslate nohighlight">\(k\)</span> nearest neighbor method, than when using the linear classifier. However, one should also note that none(!) of the training data will be misclassified when using <span class="math notranslate nohighlight">\(k=1\)</span>, while the number will be an increasing function of <span class="math notranslate nohighlight">\(k\)</span>. Is it always best to use <span class="math notranslate nohighlight">\(k=1\)</span> nearest neighbors? No, that cannot be. One realizes that the decision boundaries will vary greatly when using a different training set, giving very different predictions for a validation set of data. This is an example of overfitting, but this time in the context of a classification problem.</p>
<figure class="align-default" id="fig-mlexamples-fig-knn-classifier">
<img alt="../../../_images/9fa90350844a40db0edfd0bc5dc0a6314b7e941a4fc460023d3d77fbd749a703.png" src="../../../_images/9fa90350844a40db0edfd0bc5dc0a6314b7e941a4fc460023d3d77fbd749a703.png" />
<figcaption>
<p><span class="caption-number">Fig. 21.5 </span><span class="caption-text">Results of <span class="math notranslate nohighlight">\(k\)</span>NN classifier, with <span class="math notranslate nohighlight">\(k=1,5,15\)</span>, on the data from <a class="reference internal" href="#example:MLexamples:binary-classification">Example 21.1</a>. With <span class="math notranslate nohighlight">\(k=1\)</span> we will trivially reproduce all training data, but the model will be heavily overfitted to the actual set of training data. The <span class="math notranslate nohighlight">\(k=15\)</span> model will misclassify some of the training data but will (probably) generalize better when classifying new data.</span><a class="headerlink" href="#fig-mlexamples-fig-knn-classifier" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition-data-normalization admonition">
<p class="admonition-title">Data normalization</p>
<p>The <span class="math notranslate nohighlight">\(k\)</span>-nearest neighbor classifier is an example of a method for which it becomes very important to normalize the input data. Imagine that one of the two inputs, <span class="math notranslate nohighlight">\(x_1\)</span>, varies in a range <span class="math notranslate nohighlight">\([100,1000]\)</span> whereas the other one, <span class="math notranslate nohighlight">\(x_2\)</span>, is limited to <span class="math notranslate nohighlight">\([0,1]\)</span>. It is then obvious that the Euclidean distance will be dominated by the distance in <span class="math notranslate nohighlight">\(x_1\)</span> and our model will be almost insensitive to <span class="math notranslate nohighlight">\(x_2\)</span>.</p>
<p>We can solve this by normalizing the input parameters by some transformation and scaling. For example, we could consider instead <span class="math notranslate nohighlight">\((\tilde{x}_1, x_2)\)</span>, where <span class="math notranslate nohighlight">\(\tilde{x}_1 \equiv \frac{x_1 - 100}{1000}\)</span>. Now both parameters will be in the <span class="math notranslate nohighlight">\([0,1]\)</span> range and the method will work much better.</p>
<p>Another common normalization scheme is known as standardization</p>
<div class="amsmath math notranslate nohighlight" id="equation-fceec2b1-a89b-48ba-a7ca-15084277f2c5">
<span class="eqno">(21.26)<a class="headerlink" href="#equation-fceec2b1-a89b-48ba-a7ca-15084277f2c5" title="Permalink to this equation">#</a></span>\[\begin{equation}
\tilde{x}_i = \frac{x_i - \bar{x}_i}{\sigma_i},
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{x}_i\)</span> and <span class="math notranslate nohighlight">\(\sigma_i\)</span> is the mean and the variance, respectively, for each input variable evaluated over the training datat set.</p>
<p>Data normalization is a common procedure before any machine learning task. In fact, it is common to also normalize the output. However, remember that:</p>
<ul class="simple">
<li><p>Use only the training data for normalization. You will build a bias into the training procedure if you also include validation or test data into the computation of, e.g., the data mean and standard deviation.</p></li>
<li><p>Apply the same normalization on any new data (including validation data).</p></li>
<li><p>Donâ€™t forget to transform back before presenting results.</p></li>
</ul>
</div>
</section>
<section id="code-examples-binary-classifiers">
<h2>Code examples: binary classifiers<a class="headerlink" href="#code-examples-binary-classifiers" title="Link to this heading">#</a></h2>
<section id="data-normalization">
<h3>Data normalization<a class="headerlink" href="#data-normalization" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">standardize_data</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="s2">&quot;Standardize data to mean zero and std=1.&quot;</span>
    <span class="n">sample_mean</span><span class="o">=</span><span class="n">train_data</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">sample_std</span><span class="o">=</span><span class="n">train_data</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;mean (before standardization) = &#39;</span><span class="p">,</span><span class="n">sample_mean</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39; std (before standardization) = &#39;</span><span class="p">,</span><span class="n">sample_std</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">train_data</span> <span class="o">-</span> <span class="n">sample_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">sample_std</span><span class="p">,</span> <span class="n">sample_mean</span><span class="p">,</span> <span class="n">sample_std</span>
    
<span class="c1"># Apply to our training data set</span>
<span class="n">std_train_data</span><span class="p">,</span> <span class="n">sample_mean</span><span class="p">,</span> <span class="n">sample_std</span> <span class="o">=</span> <span class="n">standardize_data</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mean (before standardization) =  [0.56877654 0.49681303 0.5       ]
 std (before standardization) =  [1.21440984 1.39206493 0.5       ]
</pre></div>
</div>
</div>
</div>
<p>Note in particular that we normalize both input and output. This implies that the transformed data <span class="math notranslate nohighlight">\((\tilde{\inputs},\tilde{\outputs})\)</span> has mean zero and variance one, and consequently that <span class="math notranslate nohighlight">\(\tilde{\output}_i \in \{-1,1\}\)</span>.</p>
</section>
<section id="linear-classifier-s">
<h3>Linear classifier(s)<a class="headerlink" href="#linear-classifier-s" title="Link to this heading">#</a></h3>
<p>Create and fit the linear model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set up design matrix</span>
<span class="c1"># correct shape, column 0 will be the bias feature</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">std_train_data</span><span class="p">),</span><span class="mi">3</span><span class="p">))</span>
<span class="c1"># column 1 corresponds to x1</span>
<span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">std_train_data</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># column 2 corresponds to x2</span>
<span class="n">X</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">std_train_data</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Targets (responses)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">std_train_data</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>

<span class="c1">#ols estimator for model parameter theta</span>
<span class="n">ols_cov</span>   <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">X</span><span class="p">))</span>
<span class="n">ols_xTd</span>   <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">ols_theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">ols_cov</span><span class="p">,</span><span class="n">ols_xTd</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;theta_ols </span><span class="se">\t</span><span class="si">{</span><span class="n">ols_theta</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>theta_ols 	[-5.33776818e-18 -5.06798079e-01  2.67244070e-01]
</pre></div>
</div>
</div>
</div>
<p>Make predictions using both hard and soft classifiers on a grid. Note how we use the standardized data for model training (above) and predictions (below), while we transform the input coordinates back to the original scale for the plot.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Grid for predictions</span>
<span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:</span><span class="mi">3</span><span class="p">:</span><span class="mf">.05</span><span class="p">,</span> <span class="o">-</span><span class="mi">3</span><span class="p">:</span><span class="mi">3</span><span class="p">:</span><span class="mf">.05</span><span class="p">]</span>
<span class="n">x1x2_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dstack</span><span class="p">((</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">))</span>

<span class="c1"># predictions of the linear classifier on the grid</span>
<span class="c1"># liner model output</span>
<span class="n">z_grid</span> <span class="o">=</span> <span class="n">ols_theta</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">ols_theta</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x1x2_grid</span><span class="p">[:,:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">ols_theta</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">x1x2_grid</span><span class="p">[:,:,</span><span class="mi">1</span><span class="p">]</span>
<span class="c1"># perceptron (hard classifier) prediction</span>
<span class="n">yhat_hard_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">z_grid</span><span class="p">)</span>
<span class="c1"># sigmoid (soft classifier) prediction</span>
<span class="n">yhat_soft_grid</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z_grid</span><span class="p">))</span>

<span class="c1"># Final results are transformed back to original coordinates</span>
<span class="n">x1_orig</span> <span class="o">=</span> <span class="n">sample_std</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">x1</span> <span class="o">+</span> <span class="n">sample_mean</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">x2_orig</span> <span class="o">=</span> <span class="n">sample_std</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x2</span> <span class="o">+</span> <span class="n">sample_mean</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="n">fig_linear_classifier</span><span class="p">,</span><span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x1_orig</span><span class="p">,</span> <span class="n">x2_orig</span><span class="p">,</span> <span class="n">yhat_hard_grid</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">RdYlBu_r</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">,</span> <span class="s1">&#39;box&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;hard linear classifier&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x1_orig</span><span class="p">,</span> <span class="n">x2_orig</span><span class="p">,</span> <span class="n">yhat_soft_grid</span><span class="p">,</span><span class="n">levels</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">RdBu_r</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">extend</span><span class="o">=</span><span class="s1">&#39;both&#39;</span><span class="p">);</span>
<span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x1_orig</span><span class="p">,</span> <span class="n">x2_orig</span><span class="p">,</span> <span class="n">yhat_soft_grid</span><span class="p">,</span><span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="mf">0.5</span><span class="p">])</span>

<span class="k">for</span> <span class="n">iclass</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">classes</span><span class="p">):</span>
    <span class="n">x_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="n">train_data</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span><span class="o">==</span><span class="n">iclass</span><span class="p">,:</span><span class="mi">2</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axs</span><span class="p">[:</span><span class="mi">2</span><span class="p">]:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">x_data</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">,</span> <span class="s1">&#39;box&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;soft linear classifier&#39;</span><span class="p">);</span>

<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;fig_linear_classifier&quot;</span><span class="p">,</span> <span class="n">fig_linear_classifier</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/b537a4cd114f3637812f8eb52625cb55c639b032722a7c8538a26daf7ed78d8b.png" src="../../../_images/b537a4cd114f3637812f8eb52625cb55c639b032722a7c8538a26daf7ed78d8b.png" />
</div>
</div>
<p>For visualization we also make a 3D plot of the surface <span class="math notranslate nohighlight">\(z=z(x_1,x_2) = w_0 + w_1 x_1 + w_2 x_2\)</span> that results from the linear regression. This is the plane that minimizes the sum of squared distances to the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">mpl_toolkits.mplot3d</span><span class="w"> </span><span class="kn">import</span> <span class="n">axes3d</span>

<span class="n">fig_linear_classifier_plane</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig_linear_classifier_plane</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>

<span class="c1"># Plot the 3D surface</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">x1_orig</span><span class="p">,</span> <span class="n">x2_orig</span><span class="p">,</span> <span class="n">z_grid</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;royalblue&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">)</span>

<span class="c1"># Plot the x1x2 surface</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">x1_orig</span><span class="p">,</span> <span class="n">x2_orig</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x1_orig</span><span class="p">),</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;grey&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">rstride</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">cstride</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>

<span class="k">for</span> <span class="n">iclass</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">classes</span><span class="p">):</span>
    <span class="n">x_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="n">train_data</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span><span class="o">==</span><span class="n">iclass</span><span class="p">,:</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">x_data</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">zs</span><span class="o">=</span><span class="n">iclass</span><span class="p">,</span> <span class="n">zdir</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">x1_orig</span><span class="p">,</span> <span class="n">x2_orig</span><span class="p">,</span> <span class="n">z_grid</span><span class="p">,</span><span class="n">levels</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">],)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;model: $z$, data: $y$&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">1.75</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mf">15.</span><span class="p">,</span> <span class="n">azim</span><span class="o">=-</span><span class="mi">75</span><span class="p">,</span> <span class="n">roll</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">fig_linear_classifier_plane</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;fig_linear_classifier_plane&quot;</span><span class="p">,</span> <span class="n">fig_linear_classifier_plane</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/1f7bd8cfdf4f69b21adff612cc6a2bc8bee43d9e3d11bc13db13a6caa2fa969f.png" src="../../../_images/1f7bd8cfdf4f69b21adff612cc6a2bc8bee43d9e3d11bc13db13a6caa2fa969f.png" />
</div>
</div>
</section>
<section id="knn-classifier">
<h3><span class="math notranslate nohighlight">\(k\)</span>NN classifier<a class="headerlink" href="#knn-classifier" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># calculate the Euclidean distance between two vectors</span>
<span class="k">def</span><span class="w"> </span><span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">pos1</span><span class="p">,</span> <span class="n">pos2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">pos1</span><span class="o">-</span><span class="n">pos2</span><span class="p">)</span>
    
<span class="k">def</span><span class="w"> </span><span class="nf">knn_classifier</span><span class="p">(</span><span class="n">xstar</span><span class="p">,</span><span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">std_train_data</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>
    <span class="c1"># compute distances to all training data</span>
    <span class="n">distances</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x_train</span> <span class="ow">in</span> <span class="n">std_train_data</span><span class="p">[:,:</span><span class="mi">2</span><span class="p">]:</span>
        <span class="n">distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">euclidean_distance</span><span class="p">(</span><span class="n">xstar</span><span class="p">,</span><span class="n">x_train</span><span class="p">))</span>
    <span class="c1"># sort targets by closest distance</span>
    <span class="n">y_sorted</span> <span class="o">=</span> <span class="p">[</span><span class="n">y</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))]</span>
    <span class="c1"># majority vote (targets are either +1 or -1)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_sorted</span><span class="p">[:</span><span class="n">k</span><span class="p">]))</span>
    <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
        <span class="c1"># allow k to be list-like</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_sorted</span><span class="p">[:</span><span class="n">ki</span><span class="p">]))</span> <span class="k">for</span> <span class="n">ki</span> <span class="ow">in</span> <span class="n">k</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>We will explore three different settings with <span class="math notranslate nohighlight">\(k=1,5,15\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">k_list</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">15</span><span class="p">]</span>
<span class="n">yhat_knn_grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">k_list</span><span class="p">)))</span>
<span class="k">for</span> <span class="n">ix1</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x1x2_grid</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">ix2</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x1x2_grid</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">yhat_knn_grid</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span><span class="n">ix2</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">knn_classifier</span><span class="p">(</span><span class="n">x1x2_grid</span><span class="p">[</span><span class="n">ix1</span><span class="p">,</span><span class="n">ix2</span><span class="p">,:],</span><span class="n">k</span><span class="o">=</span><span class="n">k_list</span><span class="p">))</span>

<span class="n">fig_kNN_classifier</span><span class="p">,</span><span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">k_list</span><span class="p">),</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span><span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_2$&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ik</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">k_list</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">ik</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x_1$&#39;</span><span class="p">)</span>
    <span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x1_orig</span><span class="p">,</span> <span class="n">x2_orig</span><span class="p">,</span>\
    	<span class="n">yhat_knn_grid</span><span class="p">[:,:,</span><span class="n">ik</span><span class="p">],</span><span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">RdYlBu_r</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
    <span class="k">for</span> <span class="n">iclass</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">classes</span><span class="p">):</span>
        <span class="n">x_data</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="n">train_data</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span><span class="o">==</span><span class="n">iclass</span><span class="p">,:</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_data</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">x_data</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">,</span> <span class="s1">&#39;box&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">rf</span><span class="s1">&#39;$k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">$ NN&#39;</span><span class="p">)</span>

<span class="n">glue</span><span class="p">(</span><span class="s2">&quot;fig_kNN_classifier&quot;</span><span class="p">,</span> <span class="n">fig_kNN_classifier</span><span class="p">,</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../_images/9fa90350844a40db0edfd0bc5dc0a6314b7e941a4fc460023d3d77fbd749a703.png" src="../../../_images/9fa90350844a40db0edfd0bc5dc0a6314b7e941a4fc460023d3d77fbd749a703.png" />
</div>
</div>
</section>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<div class="exercise admonition" id="exercise:MLexamples:sigmoid-decision-boundary">

<p class="admonition-title"><span class="caption-number">Exercise 21.1 </span> (Sigmoid decision boundary)</p>
<section id="exercise-content">
<p>Consider the sigmoid classifier <a class="reference internal" href="#equation-eq-mlexamples-sigmoid">(21.24)</a> with the activation determmined by the scalar product of predictor variables and model parameters and the output providing the probability <span class="math notranslate nohighlight">\(\prob(\output(\inputs)=1)\)</span>.</p>
<ul class="simple">
<li><p>Explain why the sigmoid classifier can be considered a non-linear model.</p></li>
<li><p>Explain why the decision boundary of a sigmoid classifier is still a straight line despite the non-linear dependence on the model parameters.</p></li>
</ul>
</section>
</div>
<div class="exercise admonition" id="exercise:MLexamples:misclassification-cost-function">

<p class="admonition-title"><span class="caption-number">Exercise 21.2 </span> (Misclassification cost function)</p>
<section id="exercise-content">
<p>The perfomance of a hard classifier can be evluated using a misclassification cost function</p>
<div class="math notranslate nohighlight" id="equation-eq-mlexamples-misclassification">
<span class="eqno">(21.27)<a class="headerlink" href="#equation-eq-mlexamples-misclassification" title="Link to this equation">#</a></span>\[
C(\weights) = \frac{1}{N} \sum_{i=1}^N ( 1 - \delta_{\MLoutput_i,\output_i}),
\]</div>
<p>where each term evaluates to zero if the classification is correct.</p>
<ul class="simple">
<li><p>What would be different in our training when using such a cost function?</p></li>
<li><p>Do you expect the parameters to be A) the same, B) similar, C) very different compared to the model where the mean-squared error is used to define the cost function?</p></li>
</ul>
</section>
</div>
<div class="exercise admonition" id="exercise:MLexamples:validation-errors">

<p class="admonition-title"><span class="caption-number">Exercise 21.3 </span> (Validation errors)</p>
<section id="exercise-content">
<p>Generate 2000 validation data with the code used in the binary classification <a class="reference internal" href="#example:MLexamples:binary-classification">Example 21.1</a>. Evaluate the misclassification error <a class="reference internal" href="#equation-eq-mlexamples-misclassification">(21.27)</a> for both the training data and this new validation data using:</p>
<ul class="simple">
<li><p>The hard linear classifier</p></li>
<li><p>The <span class="math notranslate nohighlight">\(k\)</span>NN classifier with different <span class="math notranslate nohighlight">\(k \in [1,20]\)</span>.</p></li>
</ul>
<p>Comment on the results.</p>
</section>
</div>
<div class="exercise admonition" id="exercise:MLexamples:kNN-regression">

<p class="admonition-title"><span class="caption-number">Exercise 21.4 </span> (<span class="math notranslate nohighlight">\(k\)</span>NN for regression)</p>
<section id="exercise-content">
<p>How could you use the <span class="math notranslate nohighlight">\(k\)</span>NN model for a regression problem? In particular, how would you decide the prediction <span class="math notranslate nohighlight">\(\MLtestoutput\)</span> for a specific <span class="math notranslate nohighlight">\(\testinputs\)</span>?</p>
</section>
</div>
<div class="exercise admonition" id="exercise:MLexamples:R2-score">

<p class="admonition-title"><span class="caption-number">Exercise 21.5 </span> (<span class="math notranslate nohighlight">\(R2\)</span> score)</p>
<section id="exercise-content">
<p>The <span class="math notranslate nohighlight">\(R2\)</span> score can be said to describe the fraction of variations in the data that is predictable from the independent variables using the model.</p>
<ul class="simple">
<li><p>Try to understand this statement.</p></li>
<li><p>What is the value of the <span class="math notranslate nohighlight">\(R2\)</span> score for a perfect model?</p></li>
<li><p>What does it mean when <span class="math notranslate nohighlight">\(R2 \to 0\)</span>?</p></li>
</ul>
</section>
</div>
</section>
<section id="solutions">
<h2>Solutions<a class="headerlink" href="#solutions" title="Link to this heading">#</a></h2>
<div class="solution dropdown admonition" id="solution:MLexamples:sigmoid-decision-boundary">

<p class="admonition-title">Solution to<a class="reference internal" href="#exercise:MLexamples:sigmoid-decision-boundary"> Exercise 21.1 (Sigmoid decision boundary)</a></p>
<section id="solution-content">
<p>The mathematical model for the sigmoid outputs is not linear in the parameters. The decision boundary corresponds to <span class="math notranslate nohighlight">\(\MLoutput = 0.5\)</span>, which implies <span class="math notranslate nohighlight">\(e^{-z} = 1\)</span>, or <span class="math notranslate nohighlight">\(\inputs \cdot \weights = 0\)</span>. This is a straight line.</p>
</section>
</div>
<div class="solution dropdown admonition" id="solution:MLexamples:kNN-regression">

<p class="admonition-title">Solution to<a class="reference internal" href="#exercise:MLexamples:kNN-regression"> Exercise 21.4 (<span class="math notranslate nohighlight">\(k\)</span>NN for regression)</a></p>
<section id="solution-content">
<p>You could use the same algorithm but your training data would have continuous responses <span class="math notranslate nohighlight">\(\output_i\)</span> and the model must give a continuos variable <span class="math notranslate nohighlight">\(\MLtestoutput\)</span> as the prediction. One way would be to use the average</p>
<div class="math notranslate nohighlight">
\[
\MLoutput(\testinputs) = \frac{1}{k} \sum_{\inputs_i \in N_k(\testinputs)} \output_i,
\]</div>
</section>
</div>
<div class="solution dropdown admonition" id="solution:MLexamples:R2-score">

<p class="admonition-title">Solution to<a class="reference internal" href="#exercise:MLexamples:R2-score"> Exercise 21.5 (<span class="math notranslate nohighlight">\(R2\)</span> score)</a></p>
<section id="solution-content">
<p>For a perfect model we would have <span class="math notranslate nohighlight">\(\output_i = \MLoutput_i\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> and therefore <span class="math notranslate nohighlight">\(R2=1\)</span>.</p>
<p>A model that simply predicts the mean of the training data, <span class="math notranslate nohighlight">\(\MLtestoutput = \bar\output\)</span>, would be one that is completely independent of the independent variables. It would also give <span class="math notranslate nohighlight">\(R2=0\)</span> when evaluated over to the training data. Negative values for <span class="math notranslate nohighlight">\(R2\)</span> would mean that your model is even worse than that simple mean-value model.</p>
</section>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./LearningFromData-content/MachineLearning/ANN"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../LogReg/LogReg.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">21. </span>Logistic Regression</p>
      </div>
    </a>
    <a class="right-next"
       href="NeuralNet/exercises_LogReg_NeuralNet.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">21.6. </span>Exercise: Logistic Regression and neural networks</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-scores">Training scores</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-classification">Linear classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-perceptron-classifier">The perceptron classifier</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-soft-classifier">The soft classifier</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors-classification"><span class="math notranslate nohighlight">\(k\)</span> nearest neighbors classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-examples-binary-classifiers">Code examples: binary classifiers</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-normalization">Data normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-classifier-s">Linear classifier(s)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#knn-classifier"><span class="math notranslate nohighlight">\(k\)</span>NN classifier</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solutions">Solutions</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christian ForssÃ©n, Dick Furnstahl, and Daniel Phillips
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>