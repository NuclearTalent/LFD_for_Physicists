{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b711a5",
   "metadata": {},
   "source": [
    "# ðŸ“¥ Demonstration: Sum of normal variables squared\n",
    "\n",
    "The $\\chi^2$ distribution is supposed to result from the sum of the squares of Gaussian random variables. Here we test it numerically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5531841-fec3-4d56-82d5-338f813c5d15",
   "metadata": {},
   "source": [
    "## Note on $\\chi^2$/dof for model assessment and comparison  $\\newcommand{\\pars}{\\boldsymbol{\\theta}}$\n",
    "\n",
    "Many physicists learn to judge whether a fit of a model to data is good or to pick out the best fitting model among several by evaluating the $\\chi^2$/dof (dof = \"degrees of freedom\") for a given model and comparing the result to one. Here $\\chi^2$ is the sum of the squares of the residuals (data minus model predictions) divided by variance of the error at each point:\n",
    "\n",
    "$$\n",
    "  \\chi^2 \\equiv \\sum_{i=1}^{N_{\\text{data}}} \\frac{\\bigl(y_i - f(x_i;\\hat\\pars)\\bigr)^2}{\\sigma_i^2} ,\n",
    "$$\n",
    "\n",
    "where $y_i$ is the $i^{\\text th}$ data point, $f(x_i;\\hat\\pars)$ is the prediction of the model for that point using the best fit for the parameters, $\\hat\\pars$, and $\\sigma_i$ is the error bar for that data point. The degrees-of-freedom (dof), often denoted by $\\nu$, is the number of data points minus number of fitted parameters:\n",
    "\n",
    "$$\n",
    "  \\nu = N_{\\text{data}} - N_{\\text{fit parameters}} .\n",
    "$$ \n",
    "\n",
    "The rule of thumb is generally that $\\chi^2 \\gg 1$ means a poor fit and $\\chi^2 < 1$ indicates overfitting.\n",
    "*Where does this come from and under what conditions is it a statistically valid thing to analyze fit models this way?*\n",
    "(Note that in contrast to the Bayesian evidence, we are not assessing the model in general, but a particular fit to the model.)\n",
    "\n",
    "Underlying this use of $\\chi^2$/dof is a particular, familiar statistical model\n",
    "\n",
    "$$\n",
    "    y_{\\text expt} = y_{\\text th} + \\delta y_{\\text expt} + \\delta y_{\\text th}\n",
    "$$\n",
    "\n",
    "in which the theory is $y_{{\\text th},i} = f(x_i;\\hat\\pars)$, the experimental error is  *independent* Gaussian distributed noise with mean zero and standard deviation $\\sigma_i$, that is $\\delta y_{\\text expt} \\sim \\mathcal{N}(0,\\Sigma)$ with $\\Sigma_{ij} = \\sigma_i^2 \\delta_{ij}$, and $\\delta y_{\\text th}$ is neglected (i.e., no model discrepancy is included). The prior is (usually implicitly) taken to be uniform, so\n",
    "\n",
    "$$\n",
    "     y_{\\text expt} \\sim \\mathcal{N}\\bigl(f(x_i;\\hat\\pars), \\Sigma\\bigr) .\n",
    "$$ \n",
    "\n",
    "The likelihood (and the posterior, with a uniform prior) is then proportional to $e^{-\\chi^2(\\hat\\pars)/2}$.\n",
    "\n",
    "According to this model, each squared term in $\\chi^2$ is drawn from a *standard* normal distribution. In this context, \"standard\" means that the distribution has mean zero and variance 1. This is exactly what happens when we take as the random variables $\\bigl(y_i - f(x_i;\\hat\\pars)\\bigr)/\\sigma_i$.\n",
    "But the sum of the squares of $k$ *independent* standard normal random variables has a known distribution, called the $\\chi^2$ distribution with $k$ degrees of freedom. \n",
    "So the sum of the normalized residuals squared should be distributed (if you generated many sets of them) as a $\\chi^2$ distribution. How many degrees of freedom? This should be the number of independent pieces of information. But we have found the fitted parameters $\\hat\\pars$ by minimizing $\\chi^2$, i.e., by setting $\\partial \\chi^2(\\pars)/\\partial \\theta_j$ for $j = 1,\\ldots,N_{\\text{fit parameters}}$, which means $N_{\\text{fit parameters}}$ constraints. Therefore the number of dofs is given by $\\nu = N_{\\text{data}} - N_{\\text{fit parameters}}$.\n",
    "\n",
    "Now what do we do with this information? We only have one draw from the (supposed) $\\chi^2$ distribution. But if that distribution is narrow, we should be close to the mean. The mean of a $\\chi^2$ distribution with $k = \\nu$ dofs is $\\nu$, with variance $2\\nu$. \n",
    "So if we've got a good fit (and our statistical model is valid), then $\\chi^2/\\nu$ should be close to one. If it is much larger, than the conditions are not satisfied, so the model doesn't work. If it is smaller, than the failure implies that the residuals are too small, meaning overfitting.\n",
    "\n",
    "But we should expect fluctuations, i.e., we shouldn't always get the mean (or the mode, which is $\\nu - 2$ for $\\nu\\geq 0$). If $\\nu$ is large enough, then the distribution is approximately Gaussian and we can use the standard deviation / dof or $\\sqrt{2\\nu}/\\nu = \\sqrt{2/\\nu}$ as an expected width around one.\n",
    "One might use two or three times $\\sigma$ as a range to consider.\n",
    "If there are 1000 data points, then $\\sigma \\approx 0.045$, so $0.91 \\leq \\chi^2/{\\rm dof} \\leq 1.09$ would be an acceptable range at the 95% confidence level ($2\\sigma$). With fewer data points this range grows significantly.\n",
    "**So in making the comparison of $\\chi^2/\\nu$ to one, be sure to take into account the width of the distribution, i.e., consider $\\pm \\sqrt{2/\\nu}$ roughly.** For a small number of data points, do a better analysis!\n",
    "\n",
    "What can go wrong? Lots! See\n",
    "[\"Do's and Don'ts of reduced chi-squared\"](https://arxiv.org/pdf/1012.3754.pdf) for a thorough discussion. But we've assumed that the data is Gaussian and independent, that data is dominated by experimental and not theoretical errors, and that the constraints from fitting are linearly independent. We've also assumed a lot of data and \n",
    "we've ignored informative (or, more precisely, non-uniform priors) priors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c51cc73-fa7b-4439-923a-800fa0e68d2e",
   "metadata": {},
   "source": [
    "## Test the sum of normal variables squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35a789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d508856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample tot_vals normal variables\n",
    "tot_vals = 1000\n",
    "x_norm_vals = stats.norm.rvs(size=tot_vals, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b79875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of the normal variables and then their squares\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "ax[0].hist(x_norm_vals, bins=20, density=True, alpha=0.5, label='normal')\n",
    "ax[0].legend(loc='best', frameon=False)\n",
    "\n",
    "ax[1].hist(x_norm_vals**2, bins=20, density=True, alpha=0.5, label=r'${\\rm normal}^2$')\n",
    "ax[1].legend(loc='best', frameon=False)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e611e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_norm_squares(num):\n",
    "    \"\"\"\n",
    "    Return the sum of num random variables, with each the square of a random draw\n",
    "    from a normal distribution\n",
    "    \"\"\"\n",
    "    return np.sum(stats.norm.rvs(size=num, random_state=None)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb90c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 5   # number of squared normal-distributed random variables to sum\n",
    "tot_vals = 1000  # no. of trials\n",
    "sum_xsq_vals = np.array([sum_norm_squares(num) for i in range(tot_vals)])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "\n",
    "ax.hist(sum_xsq_vals, bins=20, density=True, alpha=0.5, label='sum squares')\n",
    "\n",
    "x = np.linspace(0,100,1000)\n",
    "dofs = num\n",
    "ax.plot(x, stats.chi2.pdf(x, dofs),\n",
    "       'r-', lw=1, alpha=1, label=r'$\\chi^2$ pdf')\n",
    "\n",
    "ax.set_xlim((0, max(20, 2*dofs)))\n",
    "ax.legend(loc='best', frameon=False)\n",
    "\n",
    "ax.set_title(fr'$\\chi^2$ with dof = {dofs}')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb7310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat but per dof\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "\n",
    "scaled_sum = sum_xsq_vals/num\n",
    "ax.hist(scaled_sum, bins=20, density=True, alpha=0.5, \n",
    "        label='sum squares / dof')\n",
    "\n",
    "dofs = num\n",
    "x = np.linspace(0,5*dofs,1000)\n",
    "ax.plot(x/dofs, dofs*stats.chi2.pdf(x, dofs),\n",
    "       'r-', lw=1, alpha=1, label=r'$\\chi^2/dof$ pdf')\n",
    "\n",
    "ax.set_xlim((0, 5))\n",
    "ax.legend(loc='best', frameon=False)\n",
    "\n",
    "ax.set_title(fr'$\\chi^2/dof$ with dof = {dofs}')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3135dc92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d1bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025-book-env",
   "language": "python",
   "name": "2025-book-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
