{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c01f11a",
   "metadata": {},
   "source": [
    "(sec:BayesianAdvantages)=\n",
    "# Advantages of the Bayesian approach\n",
    "\n",
    "```{epigraph}\n",
    "> “Bayesian inference probabilities are a measure of our state of knowledge about nature, not a measure of nature itself.\"\n",
    "\n",
    "-- Devinderjit Sivia \n",
    "```\n",
    "\n",
    "The Bayesian approach offers a number of distinct advantages in scientific applications. Some of them are listed below. In this chapter we introduce in particular the important tasks of inference with parametric models and the propagation of errors. \n",
    "\n",
    "```{admonition} How the Bayesian approach helps in science\n",
    "1. Provides an elegantly simple and rational approach for answering any scientific question for a given state of information. The procedure is well-defined:\n",
    "   * Clearly state your question and prior information.\n",
    "   * Apply the sum and product rules. The starting point is always Bayes’ theorem.\n",
    "2. Provides a way of eliminating nuisance parameters through marginalization. \n",
    "   * For some problems, the marginalization can be performed analytically, permitting certain calculations to become computationally tractable.\n",
    "   * For other problems, sampling is a straightforward way to include many nuisance parameters.\n",
    "3. Provides a well-defined procedure for propagating errors,\n",
    "   * E.g., incorporating the effects of systematic errors arising from both the measurement operation and theoretical model predictions.\n",
    "4. Incorporates relevant prior (e.g., known signal model or known theory model expansion) information through Bayes’ theorem. \n",
    "   * This is one of the great strengths of Bayesian analysis.\n",
    "   * Enforces explicit assumptions.\n",
    "   * For data with a small signal-to-noise ratio, a Bayesian analysis can frequently yield many orders of magnitude improvement in model parameter estimation, through the incorporation of relevant prior information about the signal model.\n",
    "5. For some problems, a Bayesian analysis may simply lead to a familiar (frequentist) statistic. Even in this situation it often provides a powerful new insight concerning the interpretation of the statistic.\n",
    "6. Provides a more powerful way of assessing competing theories at the forefront of science by automatically quantifying Occam’s razor. \n",
    "   * The evidence for two hypotheses or models, $M_i$ and $M_j$, can be compared in light of data $\\data$ by evaluating the ratio $p(M_i|\\data, I) / (M_j|\\data, I)$.\n",
    "   * The Bayesian quantitative Occam’s razor can also save a lot of time that might otherwise be spent chasing noise artifacts that masquerade as possible detections of real phenomena.\n",
    "```\n",
    "\n",
    "\n",
    "````{admonition} Occam's razor\n",
    ":class: tip\n",
    "Occam’s razor is a principle attributed to the medieval philosopher William of Occam (or Ockham). The principle states that one should not make more assumptions than the minimum needed. It underlies all scientific modeling and theory building. It cautions us to choose from a set of otherwise equivalent models of a given phenomenon the simplest one. In any given model, Occam’s razor helps us to \"shave off\" those variables that are not really needed to explain the phenomenon. It was previously thought to be only a qualitative principle.\n",
    "\n",
    "```{figure} ./figs/Leprechaun_or_Clurichaun.png\n",
    ":height: 250px\n",
    ":name: fig-Leprechaun\n",
    "\n",
    "Did the Leprechaun drink your wine, or is there a simpler explanation?\n",
    "```\n",
    "````\n",
    "\n",
    "## Inference with parametric models\n",
    "\n",
    "Inductive inference with parametric models is a very important tool in the natural sciences.\n",
    "* Consider $N$ different models $M_i$ ($i = 1, \\ldots, N$), each with a parameter vector $\\pars_i$. The number of parameters (length of $\\pars_i$) might be different for different models. Each of them implies a sampling distribution for possible data\n",
    "\n",
    "* Consider a scientific model $M$ with a parameter vector $\\pars$. Together with error models this implies a statistical model that gives a sampling distribution for possible data\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\data|{\\pars}, M)\n",
    "\\end{equation}\n",
    "\n",
    "* The likelihood function is the pdf of the actual, observed data ($\\data_\\mathrm{obs}$) given a set of parameters $\\boldsymbol{\\theta}$:\n",
    "\n",
    "\\begin{equation}\n",
    "{\\mathcal{L}}_i (\\pars) \\equiv p(\\data_\\mathrm{obs}|\\pars, M)\n",
    "\\end{equation}\n",
    "\n",
    "With these ingredients, several types of scientific inquiry can be addressed using a Bayesian framework.\n",
    "\n",
    "### Some key types of Bayesian inference with parametric models\n",
    "\n",
    "````{Admonition} Parameter estimation:\n",
    "  :class: tip\n",
    "  Premise: We have chosen a model (say $M_1$) and have access to a set of observed data ($\\data_\\mathrm{obs}$)\n",
    "  \n",
    "  $\\Rightarrow$ What can we infer about the model parameters $\\boldsymbol{\\theta}_1$?\n",
    "\n",
    "```{figure} ./figs/m1m2.png\n",
    ":name: fig-m1m2\n",
    ":width: 400px\n",
    ":align: center\n",
    "\n",
    "Joint pdf for the masses of two black holes merging obtained from the data analysis of a gravitational wave signal. This representation of a joint pdf is known as a corner plot. \n",
    "```\n",
    "````\n",
    "\n",
    "```{Admonition} Calibrated model predictions:\n",
    "  :class: tip\n",
    "  Premise: We have a calibrated model (say $M_1$ and a posterior for parameters $\\boldsymbol{\\theta}_1$ given data).\n",
    "  \n",
    "  $\\Rightarrow$ What can we predict for new (not measured) data (posterior predictive distribution)?\n",
    "```\n",
    "\n",
    "```{Admonition} Model comparison:\n",
    "  :class: tip\n",
    "  Premise: We have a set of different models, $\\{M_i\\}_{i=1}^N$, each with a parameter vector $\\pars_i$. The number of parameters (length of $\\pars_i$) might be different for different models. \n",
    "  \n",
    "  $\\Rightarrow$ How do they compare with each other? Do we have evidence to say that, e.g. $M_1$, is better than $M_2$? Note: *better* must be defined.\n",
    "```\n",
    "\n",
    "```{Admonition} Combining models:\n",
    "  :class: tip\n",
    "  Premise: We have models $M_1$, $M_2$, $M_3$.\n",
    "  \n",
    "  $\\Rightarrow$ How can we combine them to make better inferences than any single model?\n",
    "```\n",
    "\n",
    "```{Admonition} Experimental design:\n",
    "  :class: tip\n",
    "  Premise: Given a statistical model for experimental predictions.\n",
    "  \n",
    "  $\\Rightarrow$ How should we design an experiment to provide the most information for addressing a specific scientific question? \n",
    "```\n",
    "  \n",
    "Further discussion on Bayesian approaches to all of these will appear in subsequent chapters."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}